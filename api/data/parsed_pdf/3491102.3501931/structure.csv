,index,page,type,intervals,text,x1,y1,x2,y2,block_type,block_id
0,1,0,Title,"[(0, 7)]",Stylete: Styling the Web with Natural Language,0.1918529411764706,0.10411012500000007,0.8081473833333332,0.12584674116161615,,
1,2,0,Author,"[(7, 10), (10, 14), (14, 18), (18, 19), (19, 21), (21, 25), (25, 29), (29, 30), (30, 32), (32, 36), (36, 40), (40, 41), (41, 43), (43, 47), (47, 51), (51, 52)]","Tae Soo Kim School of Computing, KAIST Daejeon, Republic of Korea taesoo.kim@kaist.ac.kr Yoonseo Choi School of Computing, KAIST Daejeon, Republic of Korea yoonseo.choi@kaist.ac.kr DaEun Choi School of Computing, KAIST Daejeon, Republic of Korea daeun.choi@kaist.ac.kr Juho Kim School of Computing, KAIST Daejeon, Republic of Korea juhokim@kaist.ac.kr",0.20267810457516341,0.13923277979797974,0.7986242392156864,0.27173930025252524,,
2,3,0,Paragraph,"[(52, 57), (57, 59), (59, 61), (61, 62), (62, 63), (63, 64), (64, 65)]",“make this stand out more” CSS Properties Value Suggestions STT GPT-Neo VAE Dataset,0.23055662713398697,0.2991165639949495,0.787283045620915,0.49754069439898996,,
3,4,0,Caption,"[(65, 86), (86, 103), (103, 122), (122, 142), (142, 150)]","Figure 1: Stylete enables end-users to change the style of websites they visit by clicking on components and saying a desired change in natural language. A computational pipeline (1) transcribes the request and predicts plausible CSS properties with a large language model, and (2) encodes the clicked component using a convolutional neural network to identify and extract styling values from similar components in our large-scale dataset. These outputs are then presented in a palete that the user can use to iteratively change the component’s style.",0.08790522875816985,0.5210263111111111,0.9124038013071893,0.587685608080808,Figure,1.0
4,5,0,Abstract,"[(150, 151), (151, 160), (160, 167), (167, 176), (176, 187), (187, 198), (198, 209), (209, 218), (218, 229), (229, 242), (242, 251), (251, 260)]","ABSTRACT End-users can potentially style and customize websites by editing them through in-browser developer tools. Unfortunately, end-users lack the knowledge needed to translate high-level styling goals into low-level code edits. We present Stylette , a browser extension that enables users to change the style of websites by expressing goals in natural language. By interpreting the user’s goal with a large language model and extracting suggestions from our dataset of 1.7 million web components, Stylette generates a palette of CSS properties and values that the user can apply to reach their goal. A comparative study (N=40) showed that Stylette lowered the learning curve, helping participants perform styling changes 35% faster than",0.08790522875816989,0.5960617450757576,0.4809794199346405,0.7649559494949495,,
5,6,0,Paragraph,"[(260, 291), (291, 321)]",Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than the,0.08790522875816993,0.7837160295454545,0.4806825929738563,0.8227119891414141,,
6,7,0,Footnote,"[(321, 348), (348, 365), (365, 380)]","author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9157-3/22/04...$15.00",0.08711896699346405,0.8239698174242425,0.48067646993464064,0.8851023762626262,,
7,8,0,Paragraph,"[(380, 381)]",https://doi.org/10.1145/3491102.3501931,0.08790522875816993,0.8863615345959596,0.27277929934640527,0.8951668376262627,,
8,9,0,Abstract,"[(381, 390), (390, 399), (399, 408), (408, 418), (418, 420)]","those using developer tools. By presenting various alternatives for a single goal, the tool helped participants familiarize themselves with CSS through experimentation. Beyond CSS, our work can be expanded to help novices quickly grasp complex software or programming languages.",0.518996331372549,0.5979706707070708,0.9123780799019607,0.6646403676767677,,
9,10,0,Section,"(420, 422)",CCS CONCEPTS,0.5195343137254902,0.6867549268939394,0.6519230158496733,0.7005290430555556,,
10,11,0,List,"[(422, 429), (429, 440), (440, 444)]",• Human-centered computing → Natural language inter- faces ; Web-based interaction ; Interactive systems and tools ; Em- pirical studies in HCI.,0.5195343137254902,0.7050422888888889,0.9147298588235294,0.7449547616161616,,
11,12,0,Keywords,"[(444, 445), (445, 452), (452, 454)]",KEYWORDS Web Design; Natural Language Interface; End-User Programming; Machine Learning,0.5188316993464052,0.7670693208333333,0.9131337630718954,0.811433296969697,,
12,13,0,Paragraph,"[(454, 469)]","ACM Reference Format: Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim. 2022. Stylette:",0.5190522875816994,0.8350605973484849,0.9136237933006536,0.8577235268939395,,
13,13,0,Paragraph,"[(469, 494), (494, 502)]","Styling the Web with Natural Language. In CHI Conference on Human Factors in Computing Systems (CHI ’22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/3491102.3501931",0.5190748607843138,0.8602385521464647,0.9135152107843137,0.8954599536616162,,
14,14,1,Header,"[(0, 10), (10, 20)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.08790522875816993,0.0783156446969697,0.9120898797385621,0.0871209477272727,,
15,15,1,Section,"(20, 22)",1 INTRODUCTION,0.08790522875816993,0.10765139154040401,0.2557663312091503,0.12142550770202028,,
16,16,1,Paragraph,"[(22, 32), (32, 39), (39, 50), (50, 61), (61, 71), (71, 80), (80, 90), (90, 102), (102, 113), (113, 122), (122, 132), (132, 142), (142, 150), (150, 162), (162, 174), (174, 186), (186, 195), (195, 205), (205, 209)]","The web is inherently malleable. Websites are rendered out of documents—HTML, CSS, and JavaScript code—which are trans- mitted to the user’s browser and, thus, can be readily accessed and modifed on the user side. This malleability allows users to improve their experiences on the web by personalizing pages [46], self-repairing existing issues [47], or even enhancing pages with additional features [28, 57, 63]. In addition, by sculpting others’ creations, users can create their own new web pages [9, 39, 51]. The appeal of this malleability has led to the Greasemonkey [25] and Tampermonkey [5] plugins, which manage user scripts for these types of modifcations, to collectively amass more than 10 million users. However, although such plugins allow users to install modifcations designed by others, designing their own personal modifcations may be out of reach for general end-users. To edit a web page’s visual design or style, for example, users must be able to edit the underlying HTML and CSS fles, but this requires an understanding of the code’s language and structure. Thus, without the necessary expertise, most users are unable to mold websites into their own design.",0.08736437908496732,0.1268557717171717,0.4829275475490196,0.38724390303030304,,
17,16,1,Paragraph,"[(209, 219), (219, 230), (230, 241), (241, 249), (249, 259), (259, 268), (268, 280), (280, 290), (290, 299), (299, 307), (307, 319), (319, 330)]","To make the web more malleable for everyone, various end-user programming tools [34, 47, 58] have been designed to allow users with no expertise to directly manipulate a web page’s visual design— abstracting away the underlying code. While these approaches allow users to focus on the visual representation, they require the user to manually perform several low-level operations (e.g., scrubbing on a color picker, typing in values) which can be tedious and efortful. Additionally, users must be able to decompose their high-level goals into the low-level operations supported by these tools—a task that inexperienced users frequently struggle with in other design-related tasks [1, 36]. Thus, to be able to easily transform a web page’s design according to their goals, users require",0.08736437908496732,0.38975981212121213,0.4820946947712419,0.5532880949494949,,
18,16,1,Paragraph,"[(330, 334)]",another level of abstraction.,0.08790522875816993,0.5558027414141413,0.2550729150326798,0.5671239535353535,,
19,16,1,Paragraph,"[(334, 342), (342, 351), (351, 364), (364, 371), (371, 379), (379, 388), (388, 399), (399, 409), (409, 421), (421, 430)]","Natural language interfaces allow users to perform complex, compound operations by simply saying or writing their intentions. The promise of this form of interaction has led to the development of various general-purpose voice assistants—e.g., Apple’s Siri, Google Assistant, or Amazon’s Alexa. In addition, task-specifc natural language interfaces have also been designed to help inexperienced users perform complex tasks such as photo editing [36] or data visualization [24]. Similarly, if users could simply say what change they want to see, they could easily manipulate a web page without thinking about the underlying code or the low-level operations.",0.0873921568627451,0.5696398626262625,0.4827265325163398,0.7054951656565657,,
20,16,1,Paragraph,"[(430, 439), (439, 452), (452, 461), (461, 472), (472, 483), (483, 492), (492, 502), (502, 512), (512, 523), (523, 534), (534, 543), (543, 555)]","To investigate what language users would use when changing the style of a web page and how they would expect such changes to be presented, we conducted novice-expert sessions (N=8). In these sessions, novices used their voice to request changes on a web page’s visual design and the expert, a developer, would then directly perform the changes using an in-browser developer tool. Our fndings revealed that novices were frequently vague in their requests: omitting specifc details (e.g., what color for the back- ground), or using abstract terms that could not be clearly mapped to specifc changes (e.g., “modern” or “vivid”). In addition to being vague due to inexperience, novices were also purposefully am- biguous as they wanted to explore the design space by seeing the",0.08736437908496732,0.7080098121212121,0.48294130114379086,0.871538094949495,,
21,16,1,Paragraph,"[(555, 565), (565, 576), (576, 581)]","expert’s changes. Thus, novices expected the expert to make as- sumptions and provide a set of alternative changes that they could test and further iterate on.",0.5195343137254902,0.10956031717171716,0.9145628003267972,0.14855577171717174,,
22,16,1,Paragraph,"[(581, 591), (591, 601), (601, 612), (612, 625), (625, 636), (636, 647), (647, 660), (660, 672), (672, 684), (684, 694), (694, 703), (703, 713), (713, 721), (721, 730), (730, 739), (739, 749), (749, 754)]","Based on these fndings, we designed Stylette, a natural language- based interface that assumes the user’s intentions to provide a palette of web design properties and values. Stylette allows the user to modify a web component by clicking on it, and then saying or typing their desired change (e.g., “increase the size” or “make this cleaner”). Based on the user’s input, the system provides a toolbox that contains (1) a set of CSS properties that could be changed to satisfy the request, and, (2) for each property, a set of alternative values to explore and sample. The user can then simply change the component by applying the diferent property values found in the toolbox. To generate these toolboxes, we designed a computational pipeline that processes and combines the two input modalities, nat- ural language and clicks. Specifcally, a GPT-Neo-based architecture predicts suitable CSS properties from the natural language request, and a variational autoencoder (VAE) model encodes the clicked-on component to extract the values of similar components from our dataset of 1.7 million components.",0.5191683006535948,0.15107168080808073,0.9145779379084968,0.3837843070707071,,
23,16,1,Paragraph,"[(754, 762), (762, 771), (771, 782), (782, 791), (791, 800), (800, 810), (810, 821), (821, 829), (829, 839), (839, 848), (848, 856), (856, 867), (867, 877), (877, 885), (885, 895), (895, 898)]","To evaluate Stylette, we conducted a between-subjects study (N=40) in which participants performed a design recreation task and an open-ended design task with either our system or Dev- Tools, the Chrome Browser’s developer tool. Our study revealed that Stylette helped participants perform styling changes 35% faster and with a higher success rate—80% of Stylette participants suc- cessfully recreated a design within the allowed time while only 35% succeeded with DevTools. Additionally, our system led participants to experiment with and familiarize themselves with a more diverse set of properties. As participants acquired more knowledge with Stylette, however, natural language interaction limited their pro- ductivity as they could not apply this knowledge to directly make changes themselves. These insights suggest a need for a hybrid approach: natural language interaction to initially support quick familiarization with a tool, and then gradually phasing in more direct interaction methods.",0.5190914406862744,0.38630021616161614,0.9145628003267974,0.6051757212121212,,
24,16,1,Paragraph,"[(898, 904)]",This paper presents the following contributions:,0.5358115529411764,0.6076890303030302,0.8244065647058825,0.6190102424242424,,
25,17,1,List,"[(904, 915), (915, 925), (925, 935), (935, 938), (938, 948), (948, 958), (958, 968), (968, 969), (969, 978), (978, 986), (986, 993)]","(1) Stylette: A novel system that allows users to change the design of websites by using natural language to express their goal, and then iterating with the set of alternatives presented by the system. (2) A computational pipeline that combines NLP and CV tech- niques to process a natural language request and a web component into a set of plausible CSS property and value changes. (3) Findings from a between-subjects study that reveals how natural language support can help novices familiarize with and perform a previously unknown design/coding task.",0.5373539241830065,0.6418873373737374,0.9145742746732024,0.7915797616161616,,
26,18,1,Section,"(993, 996)",2 RELATED WORK,0.5195343137254902,0.823715785479798,0.689124472875817,0.8374899016414141,,
27,19,1,Paragraph,"[(996, 1008), (1008, 1020), (1020, 1031), (1031, 1036)]","We aim to enable end-users to modify the design of any website by simply saying what change they need. To this end, we review related work in (1) web manipulation tools, and (2) designing and (3) coding with natural language.",0.5188316993464052,0.8429214282828282,0.9126347816993463,0.8957527414141414,,
28,20,2,Header,"[(0, 7), (7, 17)]","Stylete: Styling the Web with Natural Language CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
29,21,2,Section,"(17, 23)",2.1 Web Design and Manipulation Tools,0.08790522875816993,0.10765139154040401,0.4211140625816993,0.12142550770202028,,
30,22,2,Paragraph,"[(23, 33), (33, 44), (44, 54), (54, 62), (62, 72), (72, 84), (84, 94), (94, 103), (103, 112), (112, 120), (120, 128), (128, 140), (140, 150)]","As web interfaces are visual representations of HTML and CSS code, various tools have been designed to facilitate the process of modifying the code to produce desired visual changes. For exam- ple, openHTML [49] provides an educational environment which shows HTML code, CSS code, and a website preview side-by-side. Other systems [9, 39, 51] allow users to inspect the code behind pages to understand the connection between code and visuals. Be- yond inspection, Chickenfoot [7] allows end-users to write simple scripts to modify components, and, more recently, Spacewalker [67] leverages genetic algorithms and crowdsourcing to generate design alternatives. These tools, however, were designed with developers or learners in mind, and require the user to understand and inter- act with the code—a task impractical for end-users with limited",0.0873921568627451,0.1268557717171717,0.4829471459150327,0.30422117575757585,,
31,22,2,Paragraph,"[(150, 151)]",knowledge.,0.08790522875816993,0.30673708484848483,0.15645716601307189,0.31805829696969695,,
32,22,2,Paragraph,"[(151, 161), (161, 172), (172, 182), (182, 193), (193, 200), (200, 210), (210, 218), (218, 226), (226, 238), (238, 246), (246, 257), (257, 269), (269, 279), (279, 290), (290, 297), (297, 299)]","To make manipulation more practical, a separate line of re- search allows users to modify a website’s visuals by directly in- teracting with the visuals. CrowdAdapt [ 47], CrowdUI [48], and XDBrowser [46] allow users to modify the positioning of web com- ponents through direct manipulation (e.g., drag-and-drop). Aimed at designers who have limited coding knowledge, Poirot [58] and CoCapture [13] provide designer-specifc widgets to support design editing and animation authoring, respectively, directly on websites. These tools, however, still require the user to expend time and efort deciding between and performing various possible editing opera- tions. Example-based systems [16, 33, 37] aim to reduce this mental and manual efort by allowing users to copy the styles of other websites. Our work aims to simplify the process further: modifying a website’s design by simply describing a change and selecting from suggested alternatives—without deciding on operations or looking for examples.",0.08736437908496732,0.3205742060606061,0.48294744477124185,0.5394509737373737,,
33,23,2,Section,"(299, 304)",2.2 Designing with Natural Language,0.08790522875816993,0.5556236137626263,0.3991532599673204,0.5693977299242424,,
34,24,2,Paragraph,"[(304, 314), (314, 323), (323, 336), (336, 348), (348, 356), (356, 364), (364, 372), (372, 383), (383, 394), (394, 403), (403, 413), (413, 423), (423, 433), (433, 443), (443, 450), (450, 461), (461, 469), (469, 479), (479, 483)]","Novices struggle to translate high-level design goals into tool op- erations due to the vocabulary problem [22]—the language used by the user and the tool do not match. Thus, empowering users to be able to design by simply stating their high-level goals has been a long-standing goal for HCI researchers. Query-Feature Graphs (QF-Graphs) [18] and CommandSpace [1] jointly modeled natural language descriptions with feature names in design applications (e.g., GIMP and Photoshop) to help users identify features based on their needs. Other systems support the use of natural language con- cepts to search for design references or components—images [17], graphic designs [29], or 3D models [10]. Beyond searching, several systems generate design artifacts (e.g., images [35] or icons [65]) based on the semantic meaning of words. Leveraging whole expres- sions instead of only words, Crosspower [60] and PixelTone [36] decompose expressions into operations for animation authoring and image editing, respectively. Our work expands this line of re- search by interpreting vague natural language expressions, which cannot be clearly decomposed, to support design editing in the context of web pages.",0.0874656862745098,0.5748292565656565,0.48294370915032675,0.8352161252525253,,
35,25,2,Section,"(483, 488)",2.3 Coding with Natural Language,0.08790522875816993,0.8513900279040404,0.37626554035947724,0.8651641440656566,,
36,26,2,Paragraph,"[(488, 497), (497, 508)]","A crucial step in programming is coding—writing instructions in the form of machine-readable syntax. To lower the barrier to coding,",0.0873921568627451,0.8705956707070707,0.48207558218954244,0.8957527414141414,,
37,26,2,Paragraph,"[(508, 518), (518, 525), (525, 537), (537, 547), (547, 559), (559, 568), (568, 580), (580, 590), (590, 601), (601, 610), (610, 620), (620, 629), (629, 639), (639, 650), (650, 662), (662, 673), (673, 682), (682, 684)]","substantial efort has been dedicated to bridge natural language and complex programming languages [45]. For instance, researchers have used semantic parsers [50] and bimodal models [2] to map nat- ural language to code. Such techniques enabled systems that allow novice coders to quickly search for code snippets [52, 53], and non- coders to code small programs by demonstrating and describing tasks [38, 44]. Beyond mapping, a line of work has also developed techniques that take natural language as input and generate code— e.g., Python [41, 61], Bash commands [40], SQL queries [68], or API calls [59]. Recent advancements in natural language processing (NLP), and especially in large language models (e.g., GPT-3 [8]), have led to performance boosts in natural language-based code generation [27]. OpenAI’s Codex [12], a GPT-3-based model, is able to generate basic games from a few natural language sentences [62]. In this same line of research, our work leverages a large language model to facilitate editing of CSS code and provides insight into how natural language interaction can scafold novices’ learning of coding languages.",0.5190212418300654,0.1095624868686868,0.9145726465686275,0.3561100646464647,,
38,27,2,Section,"(684, 687)",3 FORMATIVE STUDY,0.5195343137254902,0.3698016440656565,0.7144007862745098,0.3835757602272727,,
39,28,2,Paragraph,"[(687, 697), (697, 708), (708, 717), (717, 727), (727, 736), (736, 745)]","We conducted a formative study to investigate how novices would change the design of websites and how they would naturally request such changes. In this study, participants freely browsed through a website and requested styling changes by speaking aloud. One of the researchers, with several years of development experience, acted as an expert and made these changes on-the-go.",0.5188316993464052,0.38900602424242425,0.9137097990196076,0.4695128424242424,,
40,29,2,Section,"(745, 747)",3.1 Participants,0.5195343137254902,0.4832031592171717,0.6579658016339869,0.4969772753787879,,
41,30,2,Paragraph,"[(747, 760), (760, 768), (768, 778), (778, 790), (790, 800), (800, 810), (810, 819), (819, 824)]","We invited 8 participants (5 female, 3 male), all of whom had no background in web development. Each participant sat alongside the expert or, if participating remotely, shared their screen through a video conferencing tool 1 . To reduce the time participants spent familiarizing themselves with a website and to prompt more realistic requests, participants chose a website they frequently visit for the study. Most participants chose either our university’s web portal or its learning management system.",0.5188316993464052,0.502408802020202,0.9120995784313727,0.6105886,,
42,31,2,Section,"(824, 827)",3.2 Study Procedure,0.5195343137254902,0.6242801794191919,0.6926717129084967,0.6380542955808081,,
43,32,2,Paragraph,"[(827, 837), (837, 848), (848, 859), (859, 870), (870, 882), (882, 893), (893, 905), (905, 915), (915, 923)]","During the study, participants were asked to examine the website, and request any styling changes that they want or could improve their future experiences on the site. On their own computer, the expert used the Chrome Browser’s DevTools 2 to directly edit the CSS code. The expert would then share the edits and, if participants were not satisfed, they could ask for further edits. After around 30 minutes of editing, the participants were then asked a couple of questions about their experience. Sessions lasted a maximum of 40 minutes and participants made 8.38 requests on average.",0.5189918300653594,0.6434845595959596,0.9137038225490197,0.7655027414141414,,
44,33,2,Section,"(923, 929)",3.3 Requests were Vague and Abstract,0.5195343137254902,0.7791930582070706,0.8363794975490196,0.7929671743686869,,
45,34,2,Paragraph,"[(929, 939), (939, 949), (949, 958), (958, 968), (968, 978)]","Despite being able to concretely specify which web component they wanted to edit, participants struggled to concretely explain how it should be changed. Participants generally relied on vague phrases (e.g., “more readable” or “emphasize this”) or abstract terms (e.g., “modern”, “vivid” or “dull”) that did not immediately reveal what",0.5178790849673203,0.798398701010101,0.9137048271241831,0.8650671353535353,,
46,35,2,Footnote,"[(978, 982)]",1 https://zoom.us 2 https://developer.chrome.com/docs/devtools/,0.519328431372549,0.8738550801767677,0.733634277124183,0.8951665345959596,,
47,36,3,Header,"[(0, 10), (10, 20)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.08790522875816993,0.0783156446969697,0.9120898797385621,0.0871209477272727,,
48,37,3,Paragraph,"[(20, 32), (32, 42), (42, 56), (56, 64), (64, 68)]","visual aspect of the component should be changed or how. Even if they were specifc about which aspect to change, participants would also tend to be vague about the value to set for that aspect. For instance, a participant said “more transparent” without specifying how much more transparent.",0.08753921568627451,0.10956031717171716,0.4807369540849673,0.17622875151515158,,
49,37,3,Paragraph,"[(68, 78), (78, 87), (87, 97), (97, 108), (108, 118), (118, 130), (130, 141), (141, 150), (150, 161), (161, 175), (175, 178)]","We observed that the behavior of our participants was beyond not knowing the names of CSS properties—like the vocabulary problem observed in other tasks [22]. Participants also struggled to specify the visual aspects of the web components even without us- ing the actual property names. For example, a participant requested a text component to be highlighted but, when asked if the text should be bolder or colored diferently, they were unable to provide a defnite answer. Participants explained that their hesitation was either because (1) they were unsure about which aspect to change, or (2) they could decide on an aspect but were not confdent that it would “look good”.",0.08736437908496732,0.17874466060606056,0.48294691307189536,0.3284358222222223,,
50,38,3,Section,"(178, 182)",3.4 Assumptions Over Questions,0.08790522875816993,0.3477271491161616,0.3625578640522876,0.36150126527777776,,
51,39,3,Paragraph,"[(182, 191), (191, 201), (201, 212), (212, 221), (221, 230), (230, 240), (240, 252), (252, 263), (263, 275), (275, 287), (287, 296), (296, 305), (305, 316)]","To concretize the participants’ vague requests, the expert asked questions to prompt further details. For example, when a participant asked to make a component “less tacky”, the expert asked about what made it appear “tacky”. While participants recognized how these questions helped them iteratively decompose their goals, they found this back-and-forth to be tedious. As participants were unsure about the details, they did not want to dedicate the mental efort to ponder about the details and, instead, expected the expert to assume the details for them. They mentioned that it would be easier to distinguish what they liked or disliked if the expert made these assumptions and presented a visual result. Additionally, instead of one outcome for each request, participants wanted various options for the same request in order to explore the design space.",0.08736724640522876,0.3669327919191919,0.48100594526143786,0.5442981959595959,,
52,40,3,Section,"(316, 323)",3.5 Natural Language is Not a Panacea,0.5195343137254902,0.10765139154040401,0.8388572179738563,0.12142550770202028,,
53,41,3,Paragraph,"[(323, 334), (334, 344), (344, 354), (354, 364), (364, 375), (375, 384), (384, 395), (395, 405), (405, 415), (415, 425)]","For most participants, the use of voice or natural language was a major positive aspect about interacting with the expert. Partici- pants mentioned how it was “comfortable” to use natural language to simply explain what they wanted to change. However, while they felt that natural language helped to get the editing process started, participants desired more direct control when iterating on edits. Specifcally, when deciding on a value for a property, they felt frustrated about having to test diferent values by turn-taking with the expert. Instead, participants wanted to be presented with widgets that allowed them to test alternative values by themselves.",0.5189918300653594,0.1268557717171717,0.9145706062091505,0.2627110747474747,,
54,41,3,Paragraph,"[(425, 436)]","Based on the study insights, we derive the following design goals:",0.5358137254901961,0.26522698383838383,0.9138313215686276,0.276548195959596,,
55,42,3,List,"[(436, 445), (445, 454), (454, 458), (458, 470), (470, 471)]",• DG1: Interpret vague requests to present plausible changes. • DG2: Provide multiple alternative properties and values that could satisfy one request. • DG3: Allow users to directly iterate on the details for a change.,0.545617077124183,0.2805675414141414,0.9142521797385621,0.3481542565656566,,
56,43,3,Section,"(471, 473)",4 STYLETTE,0.5195343137254902,0.3615642703282828,0.63489982875817,0.375338386489899,,
57,44,3,Paragraph,"[(473, 486), (486, 497), (497, 508), (508, 519), (519, 530), (530, 540), (540, 551), (551, 560), (560, 570), (570, 580), (580, 593)]","Based on our design goals, we present Stylette (Fig. 2), a system that enables end-users to change the visual design of any website by simply clicking on a component and saying what change they want to see. The system interprets the user’s request through an NLP pipeline trained on vague language (DG1) to present a palette that consists of multiple CSS properties (DG2) that could be changed to satisfy the request. To iteratively edit each property, the user can directly adjust values and experiment with various sugges- tions extracted from a large-scale dataset (DG2, DG3). Stylette is implemented as a Chrome Extension and, using a method similar to Tanner et al.’s [58], it saves the user’s changes in the browser’s",0.51953431372549,0.38076991313131314,0.9145628003267972,0.5304610747474747,,
58,44,3,Paragraph,"[(593, 605)]",memory so that they persist when the user returns to the page.,0.5195343137254902,0.5329769838383839,0.8968849647058823,0.5442981959595959,,
59,45,3,Figure,"[(605, 606), (606, 607), (607, 608)]",a b c,0.25702607849509806,0.6032027481767677,0.7576201833333333,0.674372552885101,,
60,46,3,Caption,"[(608, 630), (630, 650), (650, 659)]","Figure 2: Stylete is shown overlaid on a website. When activated, the system shows a blue border (a) over components the user has hovered-on or clicked. After the user selects a component and records a request, Stylette transcribes the request (b) and displays a palete that contains CSS properties and values.",0.08790522875816965,0.8525843919191919,0.9124038013071893,0.8916086101010101,Figure,4.0
61,47,4,Header,"[(0, 7), (7, 17)]","Stylete: Styling the Web with Natural Language CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
62,48,4,Section,"(17, 20)",4.1 User Scenario,0.08790522875816993,0.10765139154040401,0.23888574673202617,0.12142550770202028,,
63,49,4,Paragraph,"[(20, 32), (32, 43), (43, 52), (52, 65), (65, 77), (77, 88), (88, 93)]","To illustrate how Stylette can be used, we follow Sofa, a sociol- ogist preparing for a paper submission to CHI 2022. While Sofa frequently visits the conference’s website to check for submission details or recent news, she feels that the design can make it chal- lenging to look for and read the contained information. As she has no web development experience, she decides to use Stylette to make styling changes to the website.",0.0874656862745098,0.1268557717171717,0.4829424915032679,0.2211997111111111,,
64,49,4,Paragraph,"[(93, 104), (104, 114), (114, 126), (126, 138), (138, 151), (151, 155)]","4.1.1 Selecting a Component. In the frontpage of the CHI website, Sofa feels that the header text is overemphasized and prevents several news posts from being seen in one glance. To start editing, she clicks on the Stylette icon on her extension toolbar. Now, she can select components to edit so she clicks on the frst header in the page (Fig. 2a).",0.08790522875816993,0.2327610747474748,0.4820812611111111,0.31326789292929297,,
65,49,4,Paragraph,"[(155, 164), (164, 174), (174, 189), (189, 202), (202, 212), (212, 224), (224, 236), (236, 247), (247, 256), (256, 269), (269, 283), (283, 285)]","4.1.2 Making a Verbal Request. With the component selected, Stylette overlays a transcript box on the website, prompting Sofa to say her request. To do so, she holds down the Ctrl key and says: “tone down the text” (Fig. 2b). After releasing the Ctrl key and a short processing period, the transcript box now displays a tran- script of what Sofa said. In case the transcription is wrong, Sofa can correct it by typing directly on the box and pressing Enter to process the corrected transcript. In addition, a palette is now presented, showing three diferent CSS properties that Sofa can edit to satisfy her needs (Fig. 2c): she can make the text smaller with font-size , change it to a slimmer font-family , or apply a lighter color .",0.08570751633986928,0.32482925656565664,0.48293277843137244,0.48836886060606055,,
66,49,4,Paragraph,"[(285, 295), (295, 308), (308, 321), (321, 333), (333, 344), (344, 353), (353, 365), (365, 377), (377, 391), (391, 402), (402, 413), (413, 424), (424, 434), (434, 449), (449, 459), (459, 461)]","4.1.3 Iterating with the palete. Under each property, the palette presents a list of values: the current value for the property (Fig. 3a), the default or original value (Fig. 3b), and a set of value suggestions (Fig. 3c). For properties with numerical values, like font-size , the sys- tem also interprets whether the user wants to increase or decrease the current value (Fig. 3d) and provides suggestions accordingly. As Sofa hovers over the suggested values for font-size , Sofa can see how the header would look with that font-size . After fnding one she feels satisfed with, she clicks on it to apply that change. If Sofa actually wanted to increase the font-size and the system gave an incorrect prediction, she could click on “Decrease” next to the property name to switch it to “Increase” and the suggestions would change accordingly. If she wanted to change another property sim- ilar to font-size , she could also click on the property name to see a drop-down of other properties with similar names (e.g., font-style , font-weight ).",0.5190262192810456,0.10956031717171716,0.9145753362745097,0.32844714343434345,,
67,49,4,Paragraph,"[(461, 472), (472, 487), (487, 497), (497, 509), (509, 523), (523, 532), (532, 547), (547, 557), (557, 571), (571, 582)]","After setting the font-size , Sofa also notices the color property. As she feels that this could also be toned down a bit, she clicks on the lighter black color (“#242424f”) in the suggestions. After seeing this change, she feels that the header’s color should be even lighter, so she clicks on the arrows next to that suggestion (Fig. 3e) to see other similar suggestions. Going through the carousel, she fnds a color that she likes so she clicks on it. If she is unsatisfed with the suggestions, she can see other diferent suggestions by clicking on the “+” at the bottom (Fig. 3f), or manually set her own value by clicking on the current value to expose manual change widgets",0.5190212418300654,0.33095173131313127,0.9143646245098042,0.4668057717171717,,
68,49,4,Paragraph,"[(582, 584)]",(Fig. 3g).,0.51909477124183,0.4693216808080808,0.5704757594771241,0.4806428929292929,,
69,49,4,Paragraph,"[(584, 585), (585, 586), (586, 587), (587, 588), (588, 589), (589, 590), (590, 591)]",c a b d f g e,0.2499934333071895,0.5154199563232323,0.7681688280784315,0.7702336144646466,,
70,50,4,Caption,"[(591, 613), (613, 634), (634, 656), (656, 681), (681, 701)]","Figure 3: For each property, the palete presents the current value (a), the default or original value before any changes (b), and a list of suggested values (c). For numerical values, the palette presents suggested values that are either larger or smaller than the current value based on the system’s prediction (d). To see other similar suggestions, the user can click on the arrows next to a suggested value (e). To see diferent suggestions, the user can click on the “+” button (f). The user can also click on the current value to reveal widgets to manually set values (g): input box for numerical properties (e.g., font-size ), drop-down menu",0.08790522875816985,0.8110730282828282,0.9120961307189546,0.8777662888888889,Figure,5.0
71,51,4,Footnote,"[(701, 712)]","for nominal properties (e.g., font-family ), or color picker for colors.",0.08790522875816985,0.8802456343434343,0.5349505934640522,0.8916008101010101,,
72,52,5,Header,"[(0, 10), (10, 20)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.08790522875816993,0.0783156446969697,0.9120898797385621,0.0871209477272727,,
73,53,5,Section,"(20, 22)",4.2 Pipeline,0.08790522875816993,0.10765139154040401,0.19551672663398695,0.12142550770202028,,
74,54,5,Paragraph,"[(22, 32), (32, 41), (41, 54), (54, 63), (63, 73), (73, 82), (82, 83)]","To support the interaction presented in the scenario, we present a computational pipeline that processes the two input modalities, voice and click, to generate the palettes (Fig. 4). For voice, the audio is recorded and automatically transcribed. For clicks, a screenshot of the component selected by the user is automatically captured. These inputs are then processed separately by the computational pipeline.",0.0874656862745098,0.1268557717171717,0.4827245,0.2211997111111111,,
75,54,5,Paragraph,"[(83, 91), (91, 100), (100, 111), (111, 123), (123, 131), (131, 142), (142, 153), (153, 160), (160, 169), (169, 178)]","4.2.1 Processing Natural Language. Our pipeline’s NLP module takes the transcribed request, and predicts relevant CSS properties and the direction of the change (e.g., increase, decrease, or neither). For this purpose, we employ the 2.7 billion parameter version of the GPT-Neo model [6], an open-source implementation of OpenAI’s GPT-3 model [8]. With a well-crafted prompt and a small num- ber of examples, these models have been shown to achieve high performance on previously unseen tasks. However, hand-crafting prompts can be a time-consuming and very imprecise process— small alterations can lead to signifcant diferences in performance.",0.08790522875816993,0.24922698383838382,0.4829381349673202,0.38508102424242424,,
76,54,5,Paragraph,"[(178, 185), (185, 195), (195, 204), (204, 213), (213, 223), (223, 233), (233, 252)]","Architecture: Instead, we implement the P-tuning tech- nique [42] that automatically searches for a prompt with high performance. In this technique, prompts are composed by con- catenating pseudo-tokens to the natural language input and train- ing the embeddings for these pseudo-tokens. In our pipeline, we use 12 pseudo-tokens. For training, we template the prompt as [ P 1:4 ,R, P 5:8 ,C, P 9:12 ,D], where p 1:12 are the pseudo-tokens, R is the",0.08789910669934646,0.3875742909090909,0.48295038578431376,0.48313956957070703,,
77,54,5,Paragraph,"[(252, 263), (263, 274), (274, 284), (284, 298), (298, 311), (311, 320), (320, 332), (332, 340), (340, 354), (354, 358)]","natural language request, C is the CSS properties separated by com- mas, and D is the change direction (i.e., “increase”, “decrease”, or “none”). During inference, we template the prompt as shown in Figure 4 (“Concatenate”): [ P 1:4 ,R, P 5:8 ]. This templated prompt is passed as input to the model and the model’s output is controlled to generate at least three CSS properties—to provide multiple alterna- tives to users (“Input” and “Generate” in Fig. 4). Then, the generated CSS properties and the remaining pseudo-tokens are concatenated to the initial prompt, and this result is passed to the model again to generate the change direction.",0.5178727388888889,0.10956031717171716,0.9145770614379084,0.24541435757575758,,
78,54,5,Paragraph,"[(358, 369), (369, 381), (381, 391), (391, 402), (402, 414), (414, 424), (424, 436), (436, 447), (447, 458), (458, 469), (469, 480), (480, 491), (491, 503), (503, 513), (513, 522), (522, 531), (531, 539)]","Dataset: Another merit of the P-tuning technique is that it only requires a small amount of data for training. To train our pseudo- tokens, we created a small-scale dataset consisting of 300 triplets of (1) vague natural language requests, (2) CSS property sets, and (3) change directions. As a frst step in creating this dataset, we requested 29 web developers to each write three hypothetical vague requests that a user could ask when wanting to change a website’s design. Then, each developer looked at the requests written by an- other person and wrote CSS properties to change and the direction for the change that could satisfy each request. We removed requests that were too specifc (e.g., included property names), and added re- quests from our formative study and system’s pilot studies. The CSS properties in this initial data are the ones supported in our system (Table 2). We then expanded the dataset by automatically augment- ing the initial data with synonym/antonym replacement [43, 64], and/or backtranslation [54]—one of the authors checked and cor- rected the augmentations. Finally, as performance can deteriorate",0.51909477124183,0.24790502424242417,0.9145781710784314,0.4806428929292929,,
79,54,5,Paragraph,"[(539, 569), (569, 572), (572, 593), (593, 596), (596, 597), (597, 599), (599, 600), (600, 601)]","P 1 P 2 P 3 P 4 make this stand out more P 5 P 6 P 7 P 8 font-size,height P 9 P 10 P 11 P 12 Google Cloud Speech-to-Text P 1 P 2 P 3 P 4 make this stand out more P 5 P 6 P 7 P 8 P seudo-tokens Record Concatenate Input Transcribe Extract Capture",0.10788831641993464,0.5058618686805556,0.7937932622875817,0.7415507520214646,,
80,55,5,Figure,"[(601, 602), (602, 605), (605, 606), (606, 608), (608, 610), (610, 614), (614, 617), (617, 619), (619, 623), (623, 626), (626, 628), (628, 632), (632, 635), (635, 637), (637, 638), (638, 640), (640, 643), (643, 647), (647, 652), (652, 654), (654, 655), (655, 656), (656, 674), (674, 695), (695, 698), (698, 700), (700, 732)]","Input Web Component Dataset Embedding Cosine Similarity Property Values font-size: 18px height: 120px color: #ffffff … width: 80px font-size: 18px height: 120px color: #ffffff … width: 80px font-size: 18px height: 120px color: #ffffff … width: 80px … 256 components Group and sample GPT-Neo with Trained P-tuning “make this stand out more” P seudo-tokens Concatenate 512 128x 128x 32 64x 64x 64 32x 32x 128 16x 16x 256 8x 8x 512 4x 4x 1024 8x 8x 512 16x 16x 256 32x 32x 128 64x 64x 256 128x 128x 512 256x 256x 3 256x 256x 3 4x 4x 1024 Variational Autoencoder P 1 P 2 P 3 P 4 make this stand out more P 5 P 6 P 7 P 8 font-size,height P 9 P 10 P 11 P 12 increase Generate",0.16537879409640524,0.51738793822601,0.8939638984640521,0.829257414294192,,
81,56,5,Caption,"[(732, 749), (749, 766), (766, 780)]","Figure 4: Our computational pipeline integrates a natural language processing (NLP) module (top, orange) and a computer vision (CV) module (bottom, blue). The diagram illustrates the pipeline at inference time—processing user’s input of natural language and clicks to generate a set of CSS property alternatives and value suggestions.",0.08752430326797385,0.8525843919191919,0.9124038013071895,0.8915746464646465,Figure,5.0
82,57,6,Header,"[(0, 7)]",Stylete: Styling the Web with Natural Language,0.08790522875816993,0.0783156446969697,0.3156362630718955,0.0871209477272727,,
83,58,6,Table,"[(7, 8), (8, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29)]",Model CSS Property Prediction Acc. Pre. Rec. F1 Direction Acc. P-tuning Hand-crafted 0.557 0.509 0.670 0.623 0.761 0.648 0.653 0.585 0.819 0.413,0.10687624052287573,0.11074608282828277,0.4615032209150326,0.16929939191919186,,
84,59,6,Caption,"[(29, 38), (38, 45), (45, 53), (53, 56)]","Table 1: With trained P-tuning, the GPT-Neo model achieved higher performance when predicting CSS properties and change directions, when compared to using a hand-crafted prompt as input.",0.0874197385620915,0.175933622222222,0.48046158954248386,0.22875839797979775,Table,10.0
85,60,6,Paragraph,"[(56, 67), (67, 77), (77, 90), (90, 100)]","signifcantly due to class imbalance [66], we ensured that each CSS property appeared in at least 10% of the requests—the represen- tation of CSS properties in the dataset is shown in Table 2. After augmentation and balancing, we fnalized our dataset of 300 triplets.",0.08790522875816989,0.2685805191919191,0.48293371535947704,0.3214118323232324,,
86,60,6,Paragraph,"[(100, 111), (111, 121), (121, 131), (131, 141), (141, 153), (153, 166), (166, 177), (177, 186), (186, 194), (194, 203), (203, 214), (214, 226)]","Training: In the training process, we used 200 triplets for train- ing and validation (80%-20% split), and reserved 100 for testing. The pseudo-tokens were trained on the generative loss from the GPT-Neo model with the Adam optimizer, until early stopping on the validation loss. We used an initial learning rate of 0.0001, batch size of 8, weight decay value of 3e-7, and gradient clipping value of 5. When compared to the model with our best hand-crafted prompt, GPT-Neo with trained P-tuning achieved a higher F1-score when predicting CSS properties and higher accuracy when predicting change direction (Table 1). Additionally, the recall with P-tuning exceeds 75% which suggests that, for the average request, the model will likely return most of the properties that the user might need.",0.08736437908496732,0.32390509898989894,0.48294401127450975,0.48745602424242424,,
87,60,6,Paragraph,"[(226, 235), (235, 247), (247, 258), (258, 268), (268, 280), (280, 290), (290, 301), (301, 312), (312, 322), (322, 335), (335, 343), (343, 356), (356, 365), (365, 368)]","4.2.2 Processing Web Components. As our formative study re- vealed, users can struggle when deciding on a value for a change (e.g., what color for the background) and may beneft from seeing various alternatives. The components in other websites can be a rich source for these alternatives. However, as the style of a compo- nent depends on what that component represents (e.g., the font-size for a header vs that for a paragraph), selecting random components would not lead to sensible and useful alternatives. Thus, it would be more benefcial to identify components in other websites that are similar to the one the user wants to change. Similarity could be measured by calculating property diferences and aggregating these into one measure, but, as properties difer in the scale and type of values, this requires the diference and aggregation calculations to be carefully formulated.",0.08736437908496732,0.4969959232323232,0.48294865931372544,0.6881984484848485,,
88,60,6,Paragraph,"[(368, 377), (377, 387), (387, 396), (396, 408), (408, 418), (418, 429), (429, 439), (439, 449), (449, 460), (460, 468), (468, 480), (480, 493), (493, 502), (502, 513), (513, 524)]","Architecture: As an alternative, we leverage a variational au- toencoder (VAE) model [30] (“Variational Autoencoder” in Fig. 4) to automatically learn a concise representation of the visual features of components. In our pipeline, we use this VAE model to encode the screenshot image of a component into a 512-dimensional vector. Through cosine similarity, this vector is then compared to the vec- tor representations of all the components in our large-scale dataset to identify 256 similar components and retrieve their property val- ues (“Cosine Similarity” in Fig. 4). To provide coarse diversity but also more fne-grained alternatives, the palette presents suggested values in two levels: (1) diferent values as separate rows, and (2) similar values as a carousel in the same row. To support this, the pipeline groups the retrieved values according to specifc rules (“Grouping Method” in Table 2) and each group represents a sug- gestion row. Then, a maximum of 10 values are randomly sampled",0.0874656862745098,0.6906891151515151,0.4829490861111111,0.8957527414141414,,
89,61,6,Header,"[(524, 534)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636882352941177,0.0783156446969697,0.9120899774509803,0.0871209477272727,,
90,62,6,Table,"[(534, 536), (536, 538), (538, 539), (539, 540), (540, 543), (543, 544), (544, 545), (545, 548), (548, 549), (549, 550), (550, 553), (553, 554), (554, 555), (555, 558), (558, 559), (559, 560), (560, 563), (563, 564), (564, 568), (568, 569), (569, 570), (570, 573), (573, 574), (574, 575), (575, 578), (578, 579), (579, 580), (580, 584), (584, 585), (585, 586), (586, 588), (588, 589), (589, 590), (590, 593), (593, 594), (594, 595), (595, 597), (597, 598), (598, 599), (599, 601), (601, 602), (602, 603), (603, 606), (606, 607), (607, 608), (608, 611), (611, 612), (612, 613), (613, 616), (616, 617)]",CSS Property Grouping Method Percent height Interval binning (N=20) 11.7% width Interval binning (N=20) 11.7% margin Interval binning (N=10) 11.3% padding Interval binning (N=10) 12.9% color K-means clustering (N=6) 12.9% background-color K-means clustering (N=6) 12.5% opacity Interval binning (N=2) 10.4% font-size Interval binning (N=10) 13.3% font-family Google Fonts categories (N=5) 13.8% font-style Nominal value 11.3% font-weight Interval binning (N=10) 11.7% text-align Nominal value 11.3% text-decoration Nominal value 11.3% border-width Interval binning (N=10) 11.3% border-color K-means clustering (N=6) 11.3% border-radius Interval binning (N=10) 11.3%,0.5308382352941174,0.11074348282828282,0.9007901411764706,0.3491455676767674,,
91,63,6,Caption,"[(617, 627), (627, 637), (637, 647), (647, 657), (657, 666), (666, 674), (674, 684), (684, 688)]","Table 2: The CSS properties supported by Stylette. The ta- ble presents the representation of each property in the nat- ural language request dataset as a percentage. Each row also shows how values for a property are grouped when sug- gested to the user: interval binning into N equally-spaced intervals, K-means clustering with the elbow method, based on the categories from Google Fonts, and no grouping for properties with nominal values.",0.5190441960784311,0.3557797979797977,0.9147085725490194,0.4639426585858582,Table,6.0
92,64,6,Paragraph,"[(688, 698), (698, 706), (706, 718), (718, 727), (727, 734)]","for each group and these alternatives are presented through the carousel. For color-related properties and the font-family property, users in the pilot studies wanted more diverse values so, for these properties, we populate other suggestion groups by retrieving the values from random components in the dataset.",0.5191683006535948,0.5108330444444444,0.9136930130718954,0.5775027414141415,,
93,64,6,Paragraph,"[(734, 743), (743, 756), (756, 766), (766, 778), (778, 789), (789, 801), (801, 814), (814, 827), (827, 838), (838, 846), (846, 859), (859, 861)]","Dataset: Although there are datasets for mobile UI compo- nents [11] or for whole web pages [32], there are none for individual web components. Thus, to train the VAE model, we constructed our own dataset. We frst compiled a list of websites from various sources: the S&P500, the Webby Awards [4], and the Open PageR- ank dataset [15]. We removed any websites that (1) could not be accessed, (2) had very similar URLs, or (3) had less than 16 compo- nents. This led to a fnal list of 7,565 websites. For each website’s main page, we used a crawler to capture each component’s CSS properties and screenshot image. After removing components that were less than 10 pixels wide or tall, the fnal dataset consisted of 1,761,161 components.",0.5189918300653594,0.5799934080808081,0.9145747950980393,0.7435469333333333,,
94,64,6,Paragraph,"[(861, 871), (871, 883), (883, 892), (892, 902), (902, 914), (914, 925), (925, 936), (936, 946), (946, 956), (956, 968), (968, 977)]","Training: The VAE model is composed of six convolutional lay- ers for encoding, one linear layer as a bottleneck, and six transposed convolutional layers for decoding. The dimensions of the outputs at each layer are shown in Figure 4 (“Variational Autoencoder”). During training, the image of a component is encoded into a vec- tor using the encoding and bottleneck layers, and then this vector is passed through the decoding layers to recreate the image. The model is trained to maximize the evidence lower bound (ELBO) value between the original image and the recreated image. We trained our model for 3 epochs with an Adam optimizer, using a learning rate of 0.0001 and batch size of 256.",0.519160714869281,0.7460376,0.9145766066993464,0.8957527414141414,,
95,65,7,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.3363069709150327,0.0871209477272727,,
96,66,7,Section,"(10, 12)",4.3 Implementation,0.08790522875816993,0.10765139154040401,0.26009788562091507,0.12142550770202028,,
97,67,7,Paragraph,"[(12, 23), (23, 35), (35, 44), (44, 56), (56, 66), (66, 69)]","We implemented the interface of Stylette as a Chrome Extension, us- ing JavaScript, HTML, and CSS. For the backend, we used a Node.js server to pre-process requests from the interface and transcribe the audio with the Google Cloud Speech-to-Text API 3 . To serve the computational pipeline, we used a Flask server running with DeepSpeed 4 .",0.08720261437908497,0.1268557717171717,0.48293799052287584,0.20736258989898998,,
98,68,7,Section,"(69, 71)",5 EVALUATION,0.08790522875816993,0.2221539167929293,0.2281370746732026,0.23592803295454556,,
99,69,7,Paragraph,"[(71, 79), (79, 88), (88, 98), (98, 109), (109, 122), (122, 133), (133, 143), (143, 155), (155, 168), (168, 175)]","We conducted a between-subjects study where we compared Stylette against the Chrome Browser’s DevTools, a tool widely available for general end-users to edit websites with. The study was composed of (1) a well-defned task of redesigning a website to look like a given outcome, and (2) an open-ended task of styling a website to follow the design direction of provided references. We designed these two tasks to investigate how Stylette helped partici- pants style components when (1) they have a clear idea about how it should change, or (2) they only have a vague sense of direction. Specifcally, we pose the following research questions:",0.08720261437908497,0.24135955959595956,0.48293706258169933,0.3772136,,
100,70,7,List,"[(175, 187), (187, 194), (194, 204), (204, 213), (213, 224), (224, 231)]",• RQ1. How does Stylette help novice users fnd the CSS prop- erties required to perform desired styling changes? • RQ2. Can Stylette encourage novices to perform a greater number of changes and use more diverse CSS properties? • RQ3. How does novices’ usage of Stylette afect their self- confdence regarding their own web designing abilities?,0.11398799215686274,0.38247796363636355,0.4829488653594772,0.4639004686868687,,
101,71,7,Section,"(231, 235)",5.1 Participants and Apparatus,0.08790522875816993,0.47869179558080804,0.3494740741830065,0.4924659117424242,,
102,72,7,Paragraph,"[(235, 246), (246, 256), (256, 268), (268, 278), (278, 286), (286, 295), (295, 305), (305, 314), (314, 325), (325, 337), (337, 346), (346, 356), (356, 367), (367, 376), (376, 379)]","We recruited 40 participants (11 female, 29 male; age M=21.5 and SD=3.05) who all reported having no previous experience with web design or coding (no knowledge of HTML and CSS). We also verifed that participants were relatively fuent in spoken English to reduce frustration due to the performance of speech-to-text technologies. Participants were divided into two equally-sized groups and each group was assigned to use either Stylette or Chrome DevTools. As six participants mentioned having other prior design experi- ences and this could afect performance (e.g., the term “padding” is used in other design tasks), they were also equally split into each condition. To simulate a realistic setting, participants who used Chrome DevTools were also allowed to freely use search engines to fnd resources and information. The study lasted a maximum of 90 minutes and participants were compensated with 30,000 KRW (approximately 26 USD).",0.08720261437908497,0.4978974383838384,0.48294865931372555,0.7029358222222222,,
103,73,7,Section,"(379, 382)",5.2 Study Procedure,0.08790522875816993,0.7177284117424243,0.26104262794117644,0.7315025279040404,,
104,74,7,Paragraph,"[(382, 391), (391, 401), (401, 409), (409, 419), (419, 429), (429, 438), (438, 449), (449, 463), (463, 470)]","The study took place face-to-face, strictly following the COVID-19 guidelines: participants had to wear masks and plastic gloves, and their temperature was checked before sessions. Each participant was provided with a computer with a Chrome browser installed and their assigned tool, Stylette or DevTools, already opened. After reading and signing the informed consent form, participants were frst provided with a brief walkthrough of their assigned tool and were then allowed to test the tool for a total of 5 minutes. After this, participants completed a short pre-task survey.",0.08736437908496732,0.7369327919191919,0.4807490307189542,0.8589509737373738,,
105,75,7,Footnote,"[(470, 474)]",3 https://cloud.google.com/speech-to-text 4 https://www.deepspeed.ai/,0.08769934640522875,0.8738550801767677,0.27797293039215687,0.8951665345959596,,
106,76,7,Author,"[(474, 484)]","Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.651298361764706,0.07831618308080812,0.9120865754901962,0.08712148611111113,,
107,77,7,Paragraph,"[(484, 485), (485, 488), (488, 490)]",Tag Properties to Change Success Range,0.5285669934640523,0.11074348282828282,0.8392556836601308,0.12206469494949497,,
108,78,7,Table,"[(490, 491), (491, 493), (493, 496), (496, 497), (497, 499), (499, 502), (502, 503), (503, 505), (505, 507), (507, 514), (514, 521), (521, 522), (522, 524), (524, 527), (527, 532), (532, 535), (535, 542), (542, 543), (543, 545), (545, 546), (546, 547), (547, 549), (549, 552), (552, 555), (555, 558), (558, 559), (559, 561), (561, 564), (564, 565), (565, 567), (567, 571), (571, 572), (572, 574), (574, 575), (575, 576), (576, 578), (578, 581), (581, 582), (582, 584), (584, 587), (587, 588), (588, 590), (590, 593)]","h2 font-size (FSz) 80px - 120px p font-weight (FW) 700 - 900 span background-color (BgC) color (C) (0.0, 0.0, 0.6) - (0.2, 0.2, 1.0) (1.0, 1.0, 1.0) - (0.8, 0.8, 0.8) video border-radius (BR) 60px - 100px button border-width (BW) border-color (BC) 6px - 10px (0.8, 0.4, 0.0) - (1.0, 0.8, 0.2) div text-align (TA) “center” h2 font-style (FSt) “italic” or “oblique” button padding (P) 30px - 60px img width (W) 600px - 800px h3 font-family (F) Any in “cursive” category h3 text-decoration (TD) “underline” div margin (M) 80px - 120px img height (H) 350px - 450px img opacity (O) 0.3 - 0.7",0.5285669934640523,0.13030653737373737,0.9030607032679739,0.3491455676767674,,
109,79,7,Caption,"[(593, 603), (603, 615), (615, 625), (625, 636), (636, 646), (646, 656), (656, 665)]","Table 3: List of the components that participants had to change during Task 1, in the order that they had to be changed. For each component, the table shows its tag type and the properties that had to be changed. For the proper- ties, the list also shows their abbreviated names (which are used hereafter), and the range of values that were accepted as successful changes (color values shown as RGB triplets).",0.5190438562091504,0.3557797979797977,0.9147082326797387,0.450108137373737,Table,6.0
110,80,7,Paragraph,"[(665, 674), (674, 684), (684, 698), (698, 709), (709, 718), (718, 727), (727, 737), (737, 747), (747, 758), (758, 769), (769, 781), (781, 791), (791, 803), (803, 813), (813, 824), (824, 834), (834, 842)]","After the survey, participants started Task 1. Participants were tasked with using their assigned tool to redesign our institute’s “About” web page 5 to look as close as possible to a provided fnal design. This fnal design was provided as a before-after image with circling around components to change and labels showing how many properties to change for each component. Natural language explanations of the changes were not provided to prevent bias- ing the language used by Stylette participants. The task involved changing 14 diferent components and all of the 16 CSS proper- ties supported by our system (Table 3). Participants were asked to change the components in the order that they appeared in the web- site, but were allowed to skip challenging components and come back to them later. A researcher verifed that a component had been successfully changed once the values of the correct properties were within the accepted success range (“Success Range” in Table 3). Par- ticipants had 30 minutes to successfully change all the components. After the task, participants completed a short survey.",0.5178790849673203,0.4904769838383838,0.9145661475490195,0.7231908727272728,,
111,80,7,Paragraph,"[(842, 853), (853, 865), (865, 877), (877, 889), (889, 900), (900, 911), (911, 919), (919, 931), (931, 941), (941, 952), (952, 962)]","After Task 1, participants started Task 2 after a 5-minute break. To ensure that all participants started Task 2 with the same amount of knowledge, those that did not complete Task 1 were frst shown how to perform the changes that they did not complete. The aim of Task 2 was to investigate how participants used their assigned tool when only provided with a vague direction for changes and allowed greater fexibility. Participants were tasked with changing a given website such that it followed the design direction of four reference websites (Fig. 5). These references were chosen as they shared a similar modern aesthetic, but also difered in how their content was structured. Participants were given 25 minutes for this",0.51909477124183,0.7257067818181817,0.9143497539215687,0.8753979434343434,,
112,81,7,Footnote,"[(962, 964)]",5 https://www.kaist.ac.kr/en/html/kaist/01.html,0.5195343137254902,0.8844750296717171,0.738066970261438,0.8951665345959596,,
113,82,8,Header,"[(0, 7), (7, 17)]","Stylete: Styling the Web with Natural Language CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
114,83,8,Paragraph,"[(17, 27), (27, 36), (36, 40)]","task. After this task, participants completed a short survey. Finally, a short interview was conducted asking participants about their experiences during both tasks.",0.08790522875816993,0.10956031717171716,0.48206837973856215,0.14855577171717174,,
115,84,8,Section,"(40, 42)",5.3 Measures,0.08790522875816993,0.1626198258838383,0.2042511367647059,0.17639394204545458,,
116,85,8,Paragraph,"[(42, 51), (51, 62), (62, 72), (72, 84), (84, 97), (97, 108), (108, 117), (117, 126), (126, 129)]","We collected responses to the pre-survey and the post-surveys after each task. All of the surveys contained four questions asking participants to rate, on a 7-point Likert scale, their self-confdence with respect to their ability to (1) perform a website design changing task, (2) plan design changes, (3) iterate on changes, and (4) use the given tool. We averaged the responses to these questions to derive one score for self-confdence. The two post-surveys included the six questions from the NASA-TLX questionnaire [26] to measure participants’ perceived workload.",0.08720261437908497,0.18182546868686872,0.48047411111111116,0.30384238787878787,,
117,85,8,Paragraph,"[(129, 137), (137, 148), (148, 159), (159, 168), (168, 179), (179, 188), (188, 197), (197, 210), (210, 223), (223, 234), (234, 242), (242, 254), (254, 264), (264, 274)]","We also quantitatively measured task-related metrics. For Task 1, we measured the time taken to successfully change each compo- nent and to complete the whole task. We hypothesized that Stylette would help participants fnd properties faster, and therefore com- plete changes in less time. Although all participants had no previous web design experiences and those with other design experiences were equally divided into each condition, individual design interest and skill could still afect the quality of the fnal designs in Task 2. Due to this reason, we did not rate these designs and, instead, we measured how many property changes were made in total. We hypothesized that Stylette participants would make more changes as they could explore diverse properties and values. A value close to 0 indicates equal usage, spread across various properties, and one close to 1 indicates unequal usage, few properties used excessively.",0.5189918300653594,0.10956031717171716,0.9145772581699346,0.30076284242424245,,
118,86,8,Caption,"[(274, 296), (296, 315), (315, 327)]","Figure 5: (Top) The website that participants styled in Task 2 mimics the portfolio of a creative director for a museum. The website only has basic styling to encourage participants to be creative and make many changes. (Bottom) The four reference websites that were provided during the task: Suparise (https://suparise.com), MadeByShape (https://madebyshape.co.uk), Land-",0.08733384052287586,0.8387472707070708,0.9147186562091504,0.8777375252525254,Figure,2.0
119,87,8,Footnote,"[(327, 332)]","bot (https://landbot.io), and Rodeo (https://getrodeo.io).",0.08790522875816993,0.8802508343434343,0.4528171973856208,0.8915720464646465,,
120,88,9,Header,"[(0, 10), (10, 20)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.08790522875816993,0.0783156446969697,0.9120898797385621,0.0871209477272727,,
121,89,9,Paragraph,"[(20, 28), (28, 38), (38, 50), (50, 62), (62, 72)]","For qualitative data, we analyzed participants’ responses during the short interviews to understand their perceptions of the given tool and how they leveraged it for their purposes. We also iteratively coded the requests used by Stylette participants in Task 2 to classify them according to the vagueness of their content and language.",0.08790522875816993,0.10956031717171716,0.48084079362745114,0.17622875151515158,,
122,90,9,Section,"(72, 74)",6 RESULTS,0.08790522875816993,0.19174608851010091,0.1914347267973856,0.2055202046717172,,
123,91,9,Paragraph,"[(74, 82), (82, 94), (94, 106), (106, 116), (116, 126), (126, 135), (135, 146), (146, 154), (154, 165), (165, 169)]","Our results demonstrated that Stylette helped participants perform styling changes faster and with greater success in Task 1, but it did not enhance productivity in Task 2. For the statistic analysis of each measure, we frst conducted a Shapiro-Wilk test to determine if the data was parametric (noted with “P”) or non-parametric (noted with “NP”). When comparing between conditions, we used an independent t-test (if parametric) and a Mann-Whitney U test (if non-parametric). When comparing between tasks within the same condition, we used a paired t-test (if parametric) and a Wilcoxon signed-rank test (if non-parametric).",0.0874656862745098,0.21095046868686873,0.48047411111111105,0.34680450909090915,,
124,92,9,Section,"(169, 174)",6.1 Task 1: Well-Defned Task,0.08790522875816993,0.36232184608585855,0.33874322745098046,0.37609596224747477,,
125,93,9,Paragraph,"[(174, 184), (184, 196), (196, 201)]","To answer RQ1, we analyzed participants’ performance in Task 1 (i.e., time taken and success rate to perform the given changes). We additionally measured participants’ perceived workload.",0.0874656862745098,0.3815262262626262,0.4808100473856209,0.42052168080808083,,
126,93,9,Paragraph,"[(201, 208), (208, 218), (218, 229), (229, 238), (238, 247), (247, 257), (257, 265), (265, 275), (275, 282)]","6.1.1 Performance. Overall, Stylette participants signifcantly out- performed DevTools participants in this task (Fig. 6). While only 7 out of 20 DevTools participants completed all changes, 16 out of 20 Stylette participants completed the task. Additionally, when comparing only those who completed the task, Stylette participants (M=971.8s, SD=314.4s) completed the task in 35% less time than those that used DevTools (M=1493.0s, SD=295.9s, t=-3.72, p=0.001, P). Comparing the time taken to successfully change each compo- nent revealed that DevTools participants struggled signifcantly",0.0874656862745098,0.43197193333333334,0.4829432212418301,0.5539888525252525,,
127,93,9,Paragraph,"[(282, 292), (292, 302), (302, 315), (315, 323)]","with specifc properties (e.g., border-radius (BR) and padding (P) in Fig. 6). These struggles generally involved two scenarios: (1) vague search queries led to unhelpful results, or (2) the name of a CSS property did not immediately reveal its visual function.",0.5189918300653594,0.10956031717171716,0.9120970478758168,0.16239289292929288,,
128,93,9,Paragraph,"[(323, 332), (332, 343), (343, 353), (353, 365), (365, 374), (374, 384), (384, 395), (395, 405), (405, 415), (415, 425), (425, 433), (433, 439)]","To illustrate the frst scenario, several participants tried queries like “enlarge the border in CSS” when searching for the padding property, but this only returned results for border-width —the search engine took them “too literally” (D2, D7, D9, D14). In other cases, participants’ vague queries returned search results for more ad- vanced changes beyond their needs. For example, to adjust the height or width , participants searched “resize image in CSS” but this returned results about “responsive images”. In contrast, as our system was trained on vague requests and presents multiple prop- erties for one request, Stylette participants had more success using similarly vague language—requesting “enlarge the border” to the system returned padding among the options.",0.5191607148692808,0.16490753939393943,0.9145667287581699,0.32844714343434345,,
129,93,9,Paragraph,"[(439, 448), (448, 459), (459, 469), (469, 478), (478, 488), (488, 497), (497, 508), (508, 516), (516, 526), (526, 530)]","The second scenario involved properties with names that could be unclear for novices, such as border-radius or text-decoration . In these situations, the properties were frequently found in the search results, but DevTools participants would overlook them as they could not immediately visualize the functions from the names or mismatched with their mental models. However, hovering over the suggested values allowed them to quickly use and test the functions of properties. S5 mentioned: “By applying [the recommendations], I could understand what [visual] concept the [margin and padding] were related to” .",0.5195343137254901,0.33095173131313127,0.9136863433006536,0.46681709292929285,,
130,93,9,Paragraph,"[(530, 540), (540, 550), (550, 559), (559, 568), (568, 577)]","6.1.2 Perceived Workload. In Task 1, responses to the NASA-TLX questions revealed that the efect of Stylette on perceived workload was mixed (Table 4). Stylette participants reported experiencing sig- nifcantly less temporal demand (U=118.0, p=0.0118, NP) and efort (U=133.0, p=0.0336, NP) than those that used DevTools. DevTools",0.5189887094771242,0.48732041818181815,0.9145661475490194,0.5539888525252525,,
131,93,9,Paragraph,"[(577, 582), (582, 583), (583, 584), (584, 585), (585, 586), (586, 587), (587, 588), (588, 589), (589, 590), (590, 591), (591, 600), (600, 609), (609, 611), (611, 618), (618, 632), (632, 641), (641, 645), (645, 654), (654, 657), (657, 666), (666, 670), (670, 678), (678, 679), (679, 689), (689, 695), (695, 700), (700, 708), (708, 710), (710, 718), (718, 725), (725, 731), (731, 737), (737, 744), (744, 745)]",T o t a l 0 350 700 1050 1400 1750 2100 971.8±314.4 1493.0±295.9 f o n t - s i z e f o n t - w e i g h t b a c k g r o un d - c o l o r & c o l o r b o r d e r - r a d i u s b o r d e r - w i d t h & b o r d e r - c o l o r t e x t - a li g n f o n t - s t y l e p a dd i n g w i d t h f o n t - f a m il y t e x t - d e c o r a t i o n m a r g i n h e i g h t o p a c i t y 0,0.11521774661764705,0.5752737967424243,0.8589449562696667,0.8355185885326717,,
132,94,9,Figure,"[(745, 746), (746, 747), (747, 748), (748, 749), (749, 750), (750, 751)]",100 200 300 400 500 600,0.20079209950980392,0.5752737967424243,0.21603770539215686,0.7470854844444444,,
133,95,9,Paragraph,"[(751, 752), (752, 753), (753, 754), (754, 755), (755, 756), (756, 757), (757, 758), (758, 759), (759, 760), (760, 761), (761, 762), (762, 763), (763, 764), (764, 765), (765, 766), (766, 767), (767, 768), (768, 769), (769, 770), (770, 771), (771, 772), (772, 773), (773, 774), (774, 775), (775, 776), (776, 777), (777, 778), (778, 779), (779, 780), (780, 781), (781, 783), (783, 784), (784, 785), (785, 786), (786, 787), (787, 788), (788, 789), (789, 790)]",72.8±42.8 75.5±83.4 69.0±23.6 91.8±81.1 103.9±46.2 83.7±58.1 41.4±24.1 125.9±85.0 46.5±15.2 108.9±63.3 33.1±21.6 113.9±72.1 42.4±23.5 48.6±30.5 93.2±68.3 74.0±36.3 139.2±100.5 187.5±130.7 144.4±132.9 83.2±75.1 51.7±29.9 307.7±187.6 157.8±122.1 142.5±69.2 62.1±55.3 51.0±21.8 134.3±168.2 127.9±100.0 Stylette DevTools ** * ** ** ** * * ** **,0.1615649478251634,0.5873366608459596,0.8759327293137252,0.7907006691553029,,
134,96,9,Caption,"[(790, 808), (808, 827)]","Figure 6: The average time taken for participants to successfully change each component using Stylette or DevTools. Each component is represented with the abbreviated names of the properties changed (Table 3). For each property, the fgure shows",0.08790522875816975,0.8525843919191919,0.9120961307189541,0.8777401252525253,Figure,4.0
135,97,9,Table,"[(827, 846)]","if the diference in time taken for each condition was statistically signifcant (*: p <.05, **: p < .01).",0.08790522875816957,0.8802534343434343,0.7329292915032677,0.8915746464646465,,
136,98,10,Header,"[(0, 7), (7, 17)]","Stylete: Styling the Web with Natural Language CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
137,99,10,Table,"[(17, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 25), (25, 26), (26, 30), (30, 32), (32, 34), (34, 36), (36, 38), (38, 39), (39, 40), (40, 44), (44, 46), (46, 48), (48, 50), (50, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 64), (64, 66), (66, 68), (68, 70), (70, 72), (72, 74), (74, 76), (76, 78), (78, 80), (80, 82), (82, 84), (84, 86), (86, 87), (87, 88), (88, 89), (89, 90), (90, 91), (91, 92), (92, 93)]",Task Condition Mental Physical Temporal Efort Performance Frustration Stylette 3.90 (1.41) 2.55 (1.57) 3.45 (1.73) 3.15 (1.79) 5.45 (1.10) 3.00 (1.52) 1 DevTools 4.35 (1.42) 1.65 (0.93) 4.50 (0.95) 4.00 (1.45) 4.85 (1.53) 2.25 (1.21) p 0.14 0.02 0.01 0.03 0.11 0.05 2 Stylette DevTools 4.75 (1.29) 4.90 (1.21) 2.90 (1.71) 2.00 (1.45) 4.35 (1.66) 4.55 (1.50) 4.25 (1.48) 4.55 (0.83) 4.05 (1.43) 4.45 (1.39) 3.55 (1.47) 2.65 (1.50) p 0.34 0.03 0.69 0.23 0.24 0.03,0.19124019607843137,0.11074348282828282,0.8087643686274509,0.21650624646464633,,
138,100,10,Caption,"[(93, 110), (110, 127), (127, 146), (146, 149)]","Table 4: For Task 1, participants’ average ratings on the perceived workload questions (NASA-TLX) showed that temporal demand and efort were signifcantly lower with Stylette, but physical demand and frustration were signifcantly higher. For Task 2, physical demand and frustration were still rated signifcantly higher with Stylette, but temporal demand and efort no longer difered signifcantly.",0.08745265098039223,0.2231404767676765,0.9124347058823529,0.2759652525252522,Table,5.0
139,101,10,Paragraph,"[(149, 160), (160, 170), (170, 179), (179, 189), (189, 196)]","participants felt signifcant time pressure due to the lengthy and ef- fortful process of thinking about what to search, skimming through search results, and reading resources. In comparison, Stylette par- ticipants could simply say something and look through the three to fve properties presented by the system.",0.08790522875816985,0.315368397979798,0.4829403346405229,0.3820368323232323,,
140,101,10,Paragraph,"[(196, 202), (202, 209), (209, 217), (217, 228), (228, 237), (237, 247), (247, 255), (255, 265), (265, 273), (273, 283), (283, 291), (291, 301), (301, 310), (310, 316)]","However, Stylette participants also experienced signifcantly higher frustration when compared to DevTools participants (U=139.5, p=0.0472, NP). According to participants, this frustration was partially attributed to the fact that the coupled AI algorithms (i.e., speech-to-text and property prediction) could both fail. For example, as they did not notice the transcription errors, several participants were confused when concrete requests (e.g., “under- line text”) did not return the correct properties. Other participants were overly preoccupied with the transcription and immediately corrected any errors—failing to notice that the system had already returned desired properties. When fxing errors, participants also had to alternate between modalities (i.e., voice, text, and clicks) which could explain why Stylette participants reported feeling a higher physical demand (U=127.0, p=0.0187, NP).",0.08736437908496732,0.38455274141414136,0.4829275475490196,0.5757552666666667,,
141,102,10,Section,"(316, 321)",6.2 Task 2: Open-Ended Task,0.5195343137254902,0.3134594723484848,0.7639908454248366,0.3272335885101011,,
142,103,10,Paragraph,"[(321, 331), (331, 341), (341, 353), (353, 364), (364, 375)]","To answer RQ2, we evaluated participants’ productivity in Task 2 (i.e., how many changes were made and whether varied properties were used). As in Task 1, we also analyzed perceived workload. Sam- ples of the participants’ fnal designs (Fig. 7) show their creativity and how they each focused on diferent aspects of the website.",0.5189918300653594,0.3326638525252525,0.9145692681372551,0.3993335494949495,,
143,103,10,Paragraph,"[(375, 383), (383, 393), (393, 401), (401, 411), (411, 422), (422, 430), (430, 440), (440, 448), (448, 459), (459, 469)]","6.2.1 Productivity. While participants in the Stylette condition (M=42.85, SD=12.18) made more property changes than those in the DevTools condition (M=39.30, SD=12.71), this diference was not statistically signifcant (t=0.901, p=0.3729, P). The lack of a statisti- cal diference could be attributed to the benefts and drawbacks of each tool’s interaction method. DevTools participants spent more time searching for information, but, once they had the required knowledge, they could directly make changes. Stylette participants could use natural language to easily fnd properties, but, even if they already knew which property to change, they expended time",0.51909477124183,0.4343620848484848,0.9145722019607843,0.5702161252525253,,
144,103,10,Paragraph,"[(469, 480)]",waiting for the system to process requests and fxing any AI-related,0.5189918300653594,0.5727320343434343,0.912099691013072,0.5840532464646465,,
145,104,10,Figure,"[(480, 481), (481, 483), (483, 484)]",S1 D12 S17 D9,0.2633821199673203,0.6057113522979798,0.8724265961437908,0.8155683424570707,,
146,105,10,Caption,"[(484, 505), (505, 525), (525, 545), (545, 565)]",Figure 7: Sample of designs created by Task 2 participants. S1 used padding to spread content vertically such that each item would appear gradually as the user scrolls down the page. S17 serendipitously found the border-width property and used it to add a “shadow” to the container for the “Creative Projects” subheader. D9 used opacity in several components to lighten the web page’s content. D12 increased the border-width and added border-color to add colored bars on the sides of the page.,0.08733384052287559,0.8387472707070708,0.9120961307189543,0.8916060101010101,Figure,2.0
147,106,11,Header,"[(0, 10), (10, 20)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.08790522875816993,0.0783156446969697,0.9120898797385621,0.0871209477272727,,
148,107,11,Paragraph,"[(20, 31), (31, 43), (43, 50)]","errors. Several participants (S5, S6, S7, S15) noted that, after learn- ing the properties in Task 1, they wanted to directly change the properties in Task 2—without using natural language.",0.08790522875816993,0.10956031717171716,0.48293827777777765,0.14855577171717174,,
149,107,11,Paragraph,"[(50, 60), (60, 68), (68, 78), (78, 88), (88, 98), (98, 105), (105, 116), (116, 123), (123, 132), (132, 141), (141, 150), (150, 163), (163, 174), (174, 177)]","Additionally, as Stylette presents other options in the palette , participants appeared to spend additional time browsing through them. While this exploration could increase efort, it also appeared to encourage familiarization with a wider range of properties. The Gini index for property usage shows that Stylette participants tried various properties (M=0.292, SD=0.045) while DevTools participants mostly stuck with a few properties that they were accustomed to (M=0.325, SD=0.052, t=-2.169, p=0.0364, P). Beyond encouraging experimentation with more properties, in some cases, the system also led participants to serendipitously fnd alternative uses for known properties. S17 mentioned, “Accidentally I just found [border- width] while trying to change the radius [so I changed it] and it shows a shadow efect that looks really, really good.” (design shown in Fig. 7).",0.0874656862745098,0.15107168080808073,0.4820724116013071,0.34227294343434345,,
150,107,11,Paragraph,"[(177, 188), (188, 196), (196, 204), (204, 215), (215, 224), (224, 234), (234, 245), (245, 256), (256, 264), (264, 266)]","6.2.2 Perceived Workload. Similar to the results of Task 1, par- ticipants in the Stylette condition reported experiencing higher physical demand (U=131.5, p=0.0278, NP) and frustration (U=129.0, p=0.0258, NP) than those in the DevTools condition (Table 4). Unlike Task 1, however, Stylette participants no longer reported feeling signifcantly less temporal demand or efort. It is plausible that, due to the open-ended nature of Task 2, Stylette participants now spent more time and efort exploring the design space through the alternatives presented by the system—Gini index results support this explanation.",0.0874656862745098,0.357868397979798,0.48293371535947704,0.49372243838383845,,
151,107,11,Paragraph,"[(266, 278), (278, 288), (288, 298), (298, 308), (308, 314)]","6.2.3 Usage Paterns of Stylete. As Task 2 allowed for more fexible and natural use, we also analyzed participants’ usage of Stylette during this task. Participants issued an average of 36.8 requests (max=58, min=18, SD=11.2) and the requests had an average length of 3.2 words (max=12, min=1, SD=1.2).",0.0874656862745098,0.509317892929293,0.4804730411764707,0.5759875898989899,,
152,107,11,Paragraph,"[(314, 323), (323, 332), (332, 345), (345, 354), (354, 365), (365, 377), (377, 389), (389, 399), (399, 411), (411, 422), (422, 430)]","Our categorization of these requests showed that, unlike our formative study results, the requests were frequently specifc and became more specifc and less vague towards the end of the task (Ta- ble 5). Participants’ interviews revealed that this gradual specifcity was due to various reasons. For one, the tool helped participants learn property names so they could now use them in requests (S5, S8, S13, S19). Others observed that the system was more accurate if they were more specifc, so they adjusted their requests accordingly (S3, S4, S16, S20). A sample of participants’ requests (Table 6) shows that the system was indeed more likely to predict users’ expected properties if the requests included more specifc information.",0.5189918300653594,0.10956031717171716,0.9145661475490194,0.25925147878787885,,
153,107,11,Paragraph,"[(430, 440), (440, 452), (452, 463), (463, 474), (474, 483), (483, 496), (496, 505), (505, 515), (515, 528), (528, 538), (538, 541)]","Like our formative study, however, around half of the requests were vague (“PP”, “PV” and “A” in Table 5). Several vague requests were due to participants not remembering the name of a property, but they were able to quickly remember them by seeing Stylette’s predicted properties. In other cases, vagueness was to deliberately get the system to act in a certain way. Several participants (S5, S6, S7, S14, S18, S19) mentioned using requests as “macros”—being vague (e.g., “change font”) so the system returned several related properties that could be changed in one go. Others (S1, S2, S4, S8, S15) used vague requests to explore what other styling changes they could make.",0.5189918300653594,0.2617673878787879,0.9137028905228759,0.41145854949494953,,
154,107,11,Paragraph,"[(541, 550), (550, 565), (565, 577), (577, 588), (588, 599), (599, 608), (608, 618), (618, 628), (628, 637), (637, 639)]","Regarding the value suggestions, there were three particular uses: (1) as a “starting point”, (2) as a “guideline”, or (3) as a “shortcut”. For the frst type, participants (S4, S11, S14, S17, S20) picked a suggested value and then manually adjusted it more to their preference. Others (S2, S5, S9, S10, S18) used the suggestions as a guideline—hovering through values to mentally map numerical diferences to visual diferences. Finally, as similar values would be suggested for similar components, several participants (S1, S7, S15) looked for the same suggestion when editing multiple similar components—as a sort of “value shortcut”.",0.5178790849673203,0.4139744585858586,0.9138313215686276,0.549828498989899,,
155,108,11,Table,"[(639, 642), (642, 643), (643, 644), (644, 645), (645, 646), (646, 647), (647, 650), (650, 653), (653, 656), (656, 661), (661, 662), (662, 667), (667, 669), (669, 675), (675, 677), (677, 680), (680, 685), (685, 687), (687, 690), (690, 693), (693, 696), (696, 698), (698, 700), (700, 702), (702, 703), (703, 704), (704, 705), (705, 706), (706, 707), (707, 708), (708, 710), (710, 711), (711, 712), (712, 714), (714, 716), (716, 719), (719, 722), (722, 727), (727, 728), (728, 734), (734, 737), (737, 742), (742, 748), (748, 751), (751, 755), (755, 757), (757, 759), (759, 760), (760, 761), (761, 762), (762, 763), (763, 765), (765, 766), (766, 767), (767, 769), (769, 771), (771, 773), (773, 777), (777, 780), (780, 785), (785, 789), (789, 791), (791, 792), (792, 793)]","Type of Request Description Examples Percentage Q1 Q4 Property Specifc (PS) Property Partial (PP) Property Vague (PV) Specifc property name expressed in request. Property name partially expressed in the request. Property name not clearly apparent in the request. ""change background color"" ""align text in the center"" ""add border"" ""change the font"" ""make this bigger"" ""increase the spacing"" 48.1% (352) 35.3% (258) 11.5% (84) 46.6% 35.2% 11.9% 56.0% 34.7% 4.7% Property Total - - 94.9% (694) 93.8% 95.3% Value Specifc (VS) Value Vague (VV) Specifc value expressed in the request. Vague direction given for a value in the request. ""change to dark grey color"" ""increase font size to 14 px"" ""decrease the height"" ""make the edges rounder"" 11.4% (83) 20.0% (146) 14.0% 25.9% 9.8% 15.0% Value Total - - 31.2% (229) 39.9% 24.9% Abstract (A) Request with abstract description of a change. ""make it look more stylish"" ""make it more playful"" 3.3% (24) 3.6% 3.1%",0.09604805751633984,0.5972983414141414,0.9147008888888893,0.8307870202020204,,
156,109,11,Caption,"[(793, 812), (812, 833), (833, 849)]","Table 5: Coding of the participants’ requests during Task 2. Requests can either mention both properties and values, only properties or only values, or be abstract. The percentage of requests for each category are shown. The table also shows the percentage for each category for the frst quartile (Q1) and last quartile (Q4) of participants’ requests.",0.08744793202614418,0.8374099292929296,0.9125325437908501,0.8764001838383841,Table,5.0
157,110,12,Header,"[(0, 7), (7, 17)]","Stylete: Styling the Web with Natural Language CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
158,111,12,Section,"(17, 21)",6.3 Self-Confdence Across Tasks,0.08790522875816993,0.10765139154040401,0.365071235130719,0.12142550770202028,,
159,112,12,Paragraph,"[(21, 30), (30, 38), (38, 45), (45, 54), (54, 64), (64, 76), (76, 88), (88, 97), (97, 104), (104, 111), (111, 121), (121, 131), (131, 142), (142, 146)]","To answer RQ3, we evaluated how self-confdence changed during the study by analyzing intra-condition diferences in participants’ responses (Fig. 8). Stylette participants’ self-confdence increased signifcantly between the pre-survey (M=4.30, SD=1.12) and the end of Task 1 (M=5.13, SD=1.22, z=18.5, p=0.0035, NP). Participants felt satisfed about completing Task 1, and mentioned that it was easy to learn about and make changes using Stylette: “It gave me the feeling of learning and becoming familiarized with web development terms.” (S12). Surprisingly, DevTools participants’ self-confdence also in- creased signifcantly between the pre-survey (M=4.01, SD=1.26) and Task 1 (M=4.81, SD=1.25, z=34.5, p=0.0148, NP). Despite most of these participants not completing Task 1, they were satisfed with what they had accomplished as they expected that CSS code would be exceptionally challenging.",0.08736437908496732,0.1268557717171717,0.48293630588235287,0.31805829696969695,,
160,112,12,Paragraph,"[(146, 153), (153, 163), (163, 171), (171, 182), (182, 195), (195, 204), (204, 213), (213, 222), (222, 228), (228, 239), (239, 248), (248, 261), (261, 272), (272, 283), (283, 293), (293, 300)]","For similar reasons, DevTools participants’ self-confdence in- creased between Task 1 (M=4.81, SD=1.25) and Task 2 (M=4.99, SD=1.32), although this was not statistically signifcant (t=0.540, p=0.595, P). These participants felt proud about their own efort and learning during the study: “This is my frst time handling [CSS] but I did this!” (D14). In contrast, self-confdence for Stylette participants decreased signifcantly between Task 1 (M=5.13, SD=1.22) and Task 2 (M=4.58, SD=1.16, t=-3.204, p=0.0047, P). Unlike DevTools partic- ipants’ self-refective comments, Stylette participants’ comments mostly focused on the tool. Some participants (S9, S16, S17, S20) mentioned how the system presented too many possibilities, mak- ing it difcult to decide on changes: “It was hard [to choose] because the suggestions were all cute.” (S16). On the other hand, several participants (S4, S7, S11, S15) felt limited by the tool’s possibilities— expecting the system to reveal new properties or support more complex changes (e.g., adding a “sparkle” animation).",0.08790522875816989,0.3205742060606061,0.4829490861111111,0.5394509737373737,,
161,113,12,Section,"(300, 302)",7 DISCUSSION,0.08790522875816993,0.5544228561868687,0.21984829771241826,0.5681969723484849,,
162,114,12,Paragraph,"[(302, 314), (314, 325), (325, 334), (334, 345), (345, 354), (354, 364), (364, 376)]","In this paper, we propose Stylette, a system that allows users to easily edit a website’s design through a suggested set of proper- ties and values generated from natural language requests. Stylette can be generalized to a variety of applications: expanded with a community feature for users to share website modifcations, imple- mented as an IDE plugin to support web developers’ help-seeking, or integrated into tools for user feedback. In this section, we further",0.08790522875816993,0.573628498989899,0.48293661111111114,0.6679711757575757,,
163,115,12,Figure,"[(376, 378), (378, 379), (379, 380), (380, 381), (381, 382), (382, 383), (383, 384), (384, 387), (387, 390), (390, 393), (393, 394), (394, 396), (396, 398), (398, 400), (400, 401), (401, 402), (402, 403), (403, 404), (404, 405), (405, 406), (406, 409), (409, 414)]",Stylette 1 2 3 4 5 6 7 4.3 ± 1.1 5.1 ± 1.2 4.6 ± 1.2 Pre-Task Task 1 Task 2 DevTools 1 2 3 4 5 6 7 4.0 ± 1.3 4.8 ± 1.25.0 ± 1.3,0.5330975740196078,0.1141411215909091,0.8760727490196077,0.34411888068181806,,
164,116,12,Caption,"[(414, 422), (422, 428), (428, 436), (436, 444), (444, 452), (452, 462)]","Figure 8: For both conditions, participants’ reported self- confdence increased signifcantly between the pre-survey and the post-Task 1 survey. However, self-confdence de- creased signifcantly for Stylette participants after Task 2, but did not change signifcantly for DevTools participants. elaborate on the potential of Stylette and suggest opportunities for",0.5195343137254901,0.3613924727272727,0.9147152078431375,0.4811959232323232,Figure,4.0
165,117,12,Paragraph,"[(462, 464)]",future work.,0.5195343137254902,0.4837118323232323,0.5941957098039214,0.4950330444444444,,
166,118,12,Section,"(464, 471)",7.1 Stylette as a Web Designing Springboard,0.5195343137254902,0.512911492550505,0.8851139410130718,0.5266856087121212,,
167,119,12,Paragraph,"[(471, 482), (482, 491), (491, 502), (502, 511), (511, 521), (521, 529), (529, 539), (539, 549), (549, 558), (558, 566)]","In our study, Stylette allowed users with no prior knowledge to quickly perform desired styling changes on websites. Unlike search engines that can take the meaning of queries “literally”, our sys- tem interpreted the vagueness behind users’ requests to present more varied and suitable solutions. Stylette also allowed users to “learn-by-doing” by immediately testing the functions of proper- ties by hovering on value suggestions—instead of having to skim through search results. As a side efect of interpreting vagueness, the system appeared to encourage creativity by presenting users with alternatives beyond their initial intentions. Together, these",0.5178790849673203,0.5321171353535353,0.914566350490196,0.6679685757575757,,
168,120,12,Table,"[(566, 567), (567, 568), (568, 569), (569, 570), (570, 577), (577, 580)]",Request Type Expected Predicted “change the font family to Helvetica” (S7) PS & VS,0.20047058823529412,0.6892813616161616,0.7418536156862746,0.7201769494949495,,
169,121,12,Paragraph,"[(580, 581), (581, 582), (582, 583), (583, 584), (584, 585)]",FF FF FSz FSt FW,0.5515373803921569,0.7088217737373738,0.7642403137254903,0.7201656282828283,,
170,122,12,Table,"[(585, 588)]",“increase padding” (S8),0.2004705882352941,0.7226902585858587,0.3347321725490196,0.7340114707070706,,
171,123,12,Paragraph,"[(588, 591), (591, 592), (592, 593), (593, 594), (594, 595), (595, 596)]",PS & VV P BW M P W,0.4586062117647059,0.722656294949495,0.7605189647058824,0.7340001494949495,,
172,124,12,Table,"[(596, 600)]",“change text color” (S6),0.20048523921568628,0.7365247797979798,0.33486403137254905,0.747845991919192,,
173,125,12,Paragraph,"[(600, 601), (601, 602), (602, 603), (603, 604), (604, 605), (605, 606), (606, 607)]",PS C BgC C FSt O TD,0.47716900392156864,0.7364922202020201,0.7995274300653594,0.7478360747474747,,
174,126,12,Table,"[(607, 614)]",“change the picture radius to 24” (S18),0.20046349281045742,0.7503607050505051,0.4211072575163398,0.7616819171717173,,
175,127,12,Paragraph,"[(614, 617), (617, 618), (618, 619), (619, 620), (620, 621), (621, 622), (622, 623)]",PP & VS BR BC BR C FSz W,0.45958073202614363,0.7503267414141415,0.7969928104575162,0.761670595959596,,
176,128,12,Table,"[(623, 627)]",“increase the size” (S16),0.20047814379084944,0.7641952262626263,0.3367762143790847,0.7755164383838384,,
177,129,12,Paragraph,"[(627, 630), (630, 631), (631, 632), (632, 633), (633, 634), (634, 635), (635, 636)]",PP & VV FSz BR BW H P W,0.45818888888888865,0.7641839050505052,0.7969928104575161,0.7755051171717173,,
178,130,12,Table,"[(636, 639)]",“change borders” (S11),0.20047814379084933,0.7780297474747475,0.33030048104575127,0.7893509595959597,,
179,131,12,Paragraph,"[(639, 640), (640, 641), (641, 642), (642, 643), (643, 644), (644, 645), (645, 646)]",PP BW BC BR BW C W,0.47675168104575133,0.7779957838383839,0.7969869500653592,0.7893396383838385,,
180,132,12,Table,"[(646, 653)]",“make it go in the middle” (S15),0.20046349281045725,0.7918642686868687,0.38469957124182985,0.8031854808080808,,
181,133,12,Paragraph,"[(653, 656), (656, 657), (657, 658), (658, 659), (659, 660), (660, 661)]",PV & VS TA H M P W,0.458599116339869,0.7918529474747475,0.7605118692810455,0.8031741595959597,,
182,134,12,Table,"[(661, 668)]",“add some spacing at the bottom” (S2),0.20046349281045725,0.80569878989899,0.42025750065359446,0.8170200020202021,,
183,135,12,Paragraph,"[(668, 671), (671, 672), (672, 673), (673, 674), (674, 675), (675, 676)]",PV & VV P BR H M P,0.4571926222222219,0.8056648262626264,0.7580358535947709,0.8170086808080809,,
184,136,12,Table,"[(676, 680)]",“change the distance” (S5),0.20046349281045725,0.8195333111111113,0.34997674771241805,0.8308545232323233,,
185,137,12,Paragraph,"[(680, 681), (681, 682), (682, 683), (683, 684), (684, 685), (685, 686), (686, 687)]",PV M BW C H P W,0.4757407633986926,0.8195219898989899,0.796978159477124,0.830843202020202,,
186,138,12,Table,"[(687, 691)]",“make this modern” (S19),0.20046349281045733,0.8333678323232324,0.34805746928104564,0.8446890444444446,,
187,139,12,Paragraph,"[(691, 692), (692, 693), (693, 694), (694, 695), (695, 696), (696, 697), (697, 698)]",A FF BW H M P W,0.4795646692810455,0.8333565111111112,0.796964973594771,0.8446777232323233,,
188,140,12,Caption,"[(698, 719), (719, 737)]","Table 6: A sample of participants’ requests in Task 2, ordered from most specifc to most vague/abstract. For each property, the table shows the request type, the property expected by the user, and the properties predicted by the system.",0.08740187712418279,0.8513119535353537,0.9120762614379079,0.8764676868686869,Table,5.0
189,141,13,Header,"[(0, 10), (10, 20)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.08790522875816993,0.0783156446969697,0.9120898797385621,0.0871209477272727,,
190,142,13,Paragraph,"[(20, 30), (30, 36)]",insights suggest that Stylette can support novices to explore and learn about CSS with continued usage.,0.08790522875816993,0.10956031717171716,0.4804679629084967,0.13471865050505044,,
191,142,13,Paragraph,"[(36, 46), (46, 56), (56, 64), (64, 74), (74, 83), (83, 92), (92, 99), (99, 109), (109, 121), (121, 130), (130, 140), (140, 148)]","The back-to-back tasks in our study provided a window into such continued usage of Stylette. We observed that users gradually developed knowledge about concrete CSS property names and values. For these now more knowledgeable users, the system still provided beneft: enabling the request of multiple properties for increased efciency, supporting exploration of the design space, and helping users quickly remember forgotten information. However, we also observed that perceived efort could increase with continued usage and users’ learning. This owed to the fact that, even after acquiring the knowledge to directly make changes by themselves, user still had to interact with the underlying, probabilistic AI— waiting for its processing and correcting any errors.",0.08736437908496732,0.1372345595959596,0.48209685392156865,0.30076284242424245,,
192,142,13,Paragraph,"[(148, 159), (159, 170), (170, 180), (180, 189), (189, 201), (201, 210), (210, 219), (219, 227), (227, 236), (236, 244), (244, 257)]","Thus, while Stylette is well-suited for novices to learn about CSS, its beneft may decrease with users’ increasing knowledge due to the form of interaction. Elaborating on Amershi et al.’s guidelines [3], this suggests how human-AI interaction should be designed for over time use in the context of novice support systems. For future work, we propose an adaptive approach: initial natural language interaction to help users acquire knowledge about properties, and then gradually exposing direct manipulation widgets for properties that users have acquired knowledge about. Knowledge could be modeled by identifying previously used properties, repeated usage of a property, or the use of the property’s name in voice requests.",0.08736437908496732,0.30327748888888884,0.4820770274509804,0.4529699131313132,,
193,142,13,Paragraph,"[(257, 266), (266, 276), (276, 285), (285, 295), (295, 306), (306, 316), (316, 326), (326, 337), (337, 349), (349, 360), (360, 371), (371, 373)]","To further overcome the frustration and physical demand ob- served in the study, future work could also investigate mecha- nisms to support the discoverability of natural language input. Prior work [14, 23, 56] has demonstrated that supporting discov- erability can reduce the amount of “guessing” that users must do. As Stylette’s NLP pipeline appears to provide more accurate pre- dictions for specifc requests, future iterations of the system could guide users to new or desired properties by suggesting more specifc language. For example, if the user makes a vague request but does not use any of the predicted properties, the system could suggest specifc requests related to other properties that the user has not seen before.",0.0873921568627451,0.45548455959595957,0.482943681862745,0.6190128424242424,,
194,143,13,Section,"(373, 382)",7.2 Leveraging Large Language Models to Support Software Use,0.08790522875816993,0.6413193208333333,0.43154187875816996,0.6714433128787879,,
195,144,13,Paragraph,"[(382, 394), (394, 405), (405, 416), (416, 427), (427, 437), (437, 448), (448, 458), (458, 466), (466, 476), (476, 486), (486, 495), (495, 506), (506, 516), (516, 528), (528, 539), (539, 549)]","The grand scale of large language models (e.g., GPT-3 [8] or GPT- Neo [6]), in terms of architecture and datasets, has allowed them to perform previously unseen tasks with only a few data points. We leveraged this quality and the P-tuning technique [42] to allow novices to interact with website designs by constructing only a small dataset of 300 requests. Similar approaches can be taken to enable novices to use natural language to use various complex software—overcoming the vocabulary problem [22]. While a rich body of work has enabled similar natural language interaction to support software usage [1, 18–21], their approaches relied on a wealth of user-generated content. Thus, these approaches are not possible for new applications or features as such content might not exist. Moreover, as shown by the struggles of DevTools participants in our study, the language used in such content may also difer greatly from the vague language used by novices as the content is usually created by intermediate or advanced users. With our",0.08720261437908497,0.6768772363636363,0.48294422058823533,0.8957488040404041,,
196,144,13,Paragraph,"[(549, 558), (558, 570), (570, 579), (579, 588)]","approach, in contrast, natural language interaction can be enabled for new applications with only the efort of creating a small dataset of examples, and, by including representative examples of novices’ language, the support can be designed specifcally for novices.",0.5195343137254902,0.10956031717171716,0.9136587581699347,0.16239289292929288,,
197,145,13,Section,"(588, 596)",7.3 Natural Language Coding as a Learning Tool,0.5195343137254902,0.17623346224747471,0.878090762254902,0.2063574542929294,,
198,146,13,Paragraph,"[(596, 606), (606, 615), (615, 623), (623, 636), (636, 646), (646, 657), (657, 665), (665, 675), (675, 684), (684, 695), (695, 703), (703, 714), (714, 723), (723, 733), (733, 743), (743, 753), (753, 761)]","Our natural language interface helps novices learn about a cod- ing language by demonstrating how the code realizes high-level goals—lowering the selection, coordination, and use barriers identi- fed by Ko et al. [31]. In addition, by exposing novices to multiple alternatives for an intended goal, we observed that our approach allowed users to acquire a greater breadth of knowledge about the code—familiarizing with more properties and learning new uses for properties. However, the study also revealed that DevTools par- ticipants appeared to feel more satisfaction about their learning experience when compared to Stylette. We suspect that this is due to DevTools participants expending more deliberate efort searching for and reading through resources. Based on these insights, we frst suggest that natural language coding tools should provide multiple code alternatives for the same goal. Then, by incorporating interven- tions that prompt users to refect on these alternatives—similar to prompts used in video learning [55]—to gain a wider understanding about the code through a deliberate learning experience.",0.5195343137254902,0.21179264040404033,0.9145661475490198,0.4445052666666667,,
199,147,13,Section,"(761, 764)",7.4 Beyond CSS,0.5195343137254902,0.45834709861111106,0.6558980637254902,0.4721212147727272,,
200,148,13,Paragraph,"[(764, 775), (775, 787), (787, 797), (797, 807), (807, 819), (819, 828), (828, 837), (837, 846), (846, 857), (857, 866), (866, 874), (874, 885), (885, 896), (896, 902)]","Stylette aims to make the web more malleable for general users with no prior knowledge. Our work focuses on CSS code and allows novices to simply describe their high-level goal to start modify- ing it—without requiring the user to decompose the goal them- selves [58] or look for examples [16, 33, 37]. However, websites are also composed of HTML (structure) and JavaScript code (function- alities). As structure-related changes might be more suitable for direct manipulation, Stylette could be combined with systems that already support this [46, 47]. Finally, to allow end-users to program new functionalities, models like OpenAI’s Codex [62], which can generate JavaScript code from natural language descriptions, could be coupled with Stylette. By integrating these three types of sup- port into one coherent system, future work could enable all users to fully access the web’s malleability.",0.5189918300653594,0.4775514787878788,0.9145660960784314,0.6687540040404042,,
201,149,13,Section,"(902, 904)",8 LIMITATIONS,0.5195343137254902,0.6825958359848484,0.6627429892156863,0.6963699521464646,,
202,150,13,Paragraph,"[(904, 915)]",Our work has several limitations which we address in this section.,0.5195343137254902,0.7018014787878787,0.9142518143790851,0.7131226909090909,,
203,151,13,List,"[(915, 924), (924, 936), (936, 945), (945, 953), (953, 963), (963, 970), (970, 978), (978, 986), (986, 995), (995, 1001), (1001, 1011), (1011, 1021), (1021, 1029)]","• Stylette currently supports 16 diferent CSS properties. These were the ones used the most in the creation of our request dataset. While Stylette could be extended to support more properties by expanding the dataset, certain complex prop- erties (e.g., those related to fexbox and grid) also require corresponding modifcations on parent elements. As Stylette only modifes the selected element’s properties, it cannot currently support these properties. To overcome this limi- tation, the system could be enhanced to cascade necessary property modifcations up the HTML tree. • In our evaluation, we compared Stylette against using Dev- Tools and search engines. A possible concern is that DevTools participants could change more properties and might have",0.545617077124183,0.7174715818181818,0.9145763173202615,0.8957527414141414,,
204,152,14,Header,"[(0, 7), (7, 17)]","Stylete: Styling the Web with Natural Language CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
205,153,14,List,"[(17, 25), (25, 34), (34, 43), (43, 48), (48, 61), (61, 68), (68, 75), (75, 84), (84, 92), (92, 101), (101, 109), (109, 110), (110, 122), (122, 132), (132, 141), (141, 149), (149, 150)]","misdirected efort into these. Although the average DevTools participant only tried around two properties that were not supported in Stylette, we acknowledge that this could have afected results in Task 1. • We relied on a dataset of 300 requests to train and evaluate our computational pipeline. While participants were gen- erally satisfed with the pipeline’s predictions, evaluating on a larger dataset would provide a better understanding of its performance. Also, while P-tuning has demonstrated high performance with even smaller datasets (N=32) [42], a larger dataset could increase our pipeline’s performance and robustness. • As we focused on a controlled evaluation of Stylette, it is still unclear how users would modify websites in the real- world. Future work could conduct a deployment study to understand how Stylette integrates into users’ actual web experiences.",0.11398799215686274,0.10956031717171716,0.4829472323529411,0.34227294343434345,,
206,154,14,Section,"(150, 152)",9 CONCLUSION,0.08790522875816993,0.3557738662878787,0.23132780816993465,0.36954798244949494,,
207,155,14,Paragraph,"[(152, 164), (164, 175), (175, 185), (185, 192), (192, 202), (202, 213), (213, 226), (226, 236), (236, 246), (246, 256), (256, 266), (266, 275)]","This paper presents Stylette , a novel system that allows users to describe a styling request in natural language to change the visual design of websites. By combining a GPT-Neo-based model and a convolutional VAE model, our computational pipeline processes the user’s request and the component they clicked. The processed outputs are then combined to generate a palette of CSS properties and values that the user can experiment and iterate on to reach their desired style. A user-study revealed that Stylette could help users familiarize themselves with CSS properties in a shorter amount of time and with greater breadth. Insights from the study regarding the benefts and limitations of natural language support can guide the design of future work on novice support systems.",0.0874656862745098,0.37497950909090905,0.48074028758169934,0.5385065292929293,,
208,156,14,Section,"(275, 276)",ACKNOWLEDGMENTS,0.08790522875816993,0.5520074521464646,0.27999095032679744,0.5657815683080808,,
209,157,14,Paragraph,"[(276, 286), (286, 294), (294, 300), (300, 307), (307, 315), (315, 327), (327, 336), (336, 346), (346, 353)]","This work was supported by Institute of Information & communi- cations Technology Planning & Evaluation(IITP) grant funded by the Korea government (MSIT) (No.2021-0-01347,Video Interaction Technologies Using Object-Oriented Video Modeling). This work was partly supported by KAIST-NAVER Hypercreative AI Center. The authors would like to thank the members of KIXLAB for their thoughtful comments and thank our participants for their positive engagement during the studies. Finally, we thank the reviewers as their feedback helped us improve the paper.",0.08736437908496732,0.571213094949495,0.4829363687908496,0.6932300141414143,,
210,158,14,Section,"(353, 354)",REFERENCES,0.08790522875816993,0.7067309369949495,0.20132778316993463,0.7205050531565655,,
211,159,14,Bibliography,"[(354, 375), (375, 397), (397, 414), (414, 435), (435, 451), (451, 473), (473, 497), (497, 510), (510, 530), (530, 543), (543, 567), (567, 579), (579, 602), (602, 626), (626, 651), (651, 671), (671, 692), (692, 704), (704, 727), (727, 741), (741, 763), (763, 782), (782, 805), (805, 824), (824, 847), (847, 869), (869, 888), (888, 910), (910, 932), (932, 953), (953, 973), (973, 996), (996, 1014), (1014, 1040), (1040, 1060), (1060, 1079), (1079, 1099), (1099, 1122), (1122, 1144), (1144, 1167), (1167, 1189), (1189, 1214), (1214, 1237), (1237, 1266)]","[1] Eytan Adar, Mira Dontcheva, and Gierad Laput. 2014. CommandSpace: Modeling the Relationships between Tasks, Descriptions and Features. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (Honolulu, Hawaii, USA) (UIST ’14) . Association for Computing Machinery, New York, NY, USA, 167–176. https://doi.org/10.1145/2642918.2647395 [2] Miltos Allamanis, Daniel Tarlow, Andrew Gordon, and Yi Wei. 2015. Bimodal Modelling of Source Code and Natural Language. In Proceedings of the 32nd International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 37) , Francis Bach and David Blei (Eds.). PMLR, Lille, France, 2123– 2132. https://proceedings.mlr.press/v37/allamanis15.html [3] Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. 2019. Guidelines for Human- AI Interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3290605.3300233 [4] The Webby Awards. 2021. Top Websites and Mobile Sites | The Webby Awards. Retrieved August 29, 2021 from https://winners.webbyawards.com/winners/ websites-and-mobile-sites [5] Jan Biniok. 2021. Tampermonkey. Retrieved September 5, 2021 from https: //www.tampermonkey.net/ [6] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT- Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorfow . https: //doi.org/10.5281/zenodo.5297715 If you use this software, please cite it using these metadata. [7] Michael Bolin, Matthew Webber, Philip Rha, Tom Wilson, and Robert C. Miller. 2005. Automation and Customization of Rendered Web Pages. In Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology (Seattle, WA, USA) (UIST ’05) . Association for Computing Machinery, New York, NY, USA, 163–172. https://doi.org/10.1145/1095034.1095062 [8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jefrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL] [9] Kerry Shih-Ping Chang and Brad A. Myers. 2012. WebCrystal: Understanding and Reusing Examples in Web Authoring . Association for Computing Machinery, New York, NY, USA, 3205–3214. https://doi.org/10.1145/2207676.2208740 [10] Siddhartha Chaudhuri, Evangelos Kalogerakis, Stephen Giguere, and Thomas Funkhouser. 2013. Attribit: Content Creation with Semantic Attributes. In Pro- ceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (St. Andrews, Scotland, United Kingdom) (UIST ’13) . Association for Computing Machinery, New York, NY, USA, 193–202. https://doi.org/10.1145/ 2501988.2502008 [11] Chunyang Chen, Sidong Feng, Zhenchang Xing, Linda Liu, Shengdong Zhao, and Jinshui Wang. 2019. Gallery D.C.: Design Search and Knowledge Discovery through Auto-Created GUI Component Gallery. Proc. ACM Hum.-Comput. Inter- act. 3, CSCW, Article 180 (Nov. 2019), 22 pages. https://doi.org/10.1145/3359282 [12] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo- tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan- tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs.LG] [13] Yan Chen, Sang Won Lee, and Steve Oney. 2021. CoCapture: Efectively Commu- nicating UI Behaviors on Existing Websites by Demonstrating and Remixing. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI ’21) . Association for Computing Machinery, New York, NY, USA, Article 416, 14 pages. https://doi.org/10.1145/3411764.3445573 [14] Eric Corbett and Astrid Weber. 2016. What Can I Say? Addressing User Experience Challenges of a Mobile Voice User Interface for Accessibility. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services (Florence, Italy) (MobileHCI ’16) . Association for Computing Machinery, New York, NY, USA, 72–82. https://doi.org/10.1145/2935334.2935386 [15] DomCop. 2021. What is Open PageRank? Retrieved August 29, 2021 from https://www.domcop.com/openpagerank/what-is-openpagerank [16] Michael J Fitzgerald et al. 2008. CopyStyler: Web design by example . Ph.D. Dissertation. Massachusetts Institute of Technology. [17] James Fogarty, Desney Tan, Ashish Kapoor, and Simon Winder. 2008. CueFlik: Interactive Concept Learning in Image Search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Florence, Italy) (CHI ’08) . Association for Computing Machinery, New York, NY, USA, 29–38. https://doi. org/10.1145/1357054.1357061 [18] Adam Fourney, Richard Mann, and Michael Terry. 2011. Query-Feature Graphs: Bridging User Vocabulary and System Functionality. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (Santa Barbara, California, USA) (UIST ’11) . Association for Computing Machinery, New York, NY, USA, 207–216. https://doi.org/10.1145/2047196.2047224 [19] C. Ailie Fraser, Mira Dontcheva, Holger Winnemöller, Sheryl Ehrlich, and Scott Klemmer. 2016. DiscoverySpace: Suggesting Actions in Complex Software. In Proceedings of the 2016 ACM Conference on Designing Interactive Systems (Brisbane, QLD, Australia) (DIS ’16) . Association for Computing Machinery, New York, NY, USA, 1221–1232. https://doi.org/10.1145/2901790.2901849 [20] C. Ailie Fraser, Julia M. Markel, N. James Basa, Mira Dontcheva, and Scott Klem- mer. 2020. ReMap: Lowering the Barrier to Help-Seeking with Multimodal Search.",0.09320424836601307,0.11148867222222222,0.9140264529411766,0.9052309285353536,,
212,160,15,Header,"[(0, 10), (10, 20)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Tae Soo Kim, DaEun Choi, Yoonseo Choi, and Juho Kim",0.08790522875816993,0.0783156446969697,0.9120898797385621,0.0871209477272727,,
213,161,15,Bibliography,"[(20, 44), (44, 64), (64, 85), (85, 98), (98, 125), (125, 151), (151, 173), (173, 199), (199, 223), (223, 234), (234, 245), (245, 270), (270, 287), (287, 308), (308, 330), (330, 343), (343, 366), (366, 388), (388, 401), (401, 414), (414, 441), (441, 449), (449, 472), (472, 495), (495, 519), (519, 527), (527, 540), (540, 565), (565, 589), (589, 600), (600, 623), (623, 637), (637, 661), (661, 670), (670, 695), (695, 716), (716, 728), (728, 756), (756, 779), (779, 808), (808, 829), (829, 845), (845, 869), (869, 884), (884, 906), (906, 924), (924, 932), (932, 953), (953, 968), (968, 991), (991, 1013), (1013, 1035), (1035, 1045), (1045, 1066), (1066, 1085), (1085, 1108), (1108, 1123), (1123, 1145), (1145, 1166), (1166, 1177), (1177, 1200), (1200, 1222), (1222, 1248), (1248, 1272), (1272, 1296), (1296, 1319), (1319, 1343), (1343, 1364), (1364, 1390), (1390, 1400), (1400, 1422), (1422, 1445), (1445, 1447)]","In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (Virtual Event, USA) (UIST ’20) . Association for Computing Machin- ery, New York, NY, USA, 979–986. https://doi.org/10.1145/3379337.3415592 [21] C. Ailie Fraser, Tricia J. Ngoon, Mira Dontcheva, and Scott Klemmer. 2019. RePlay: Contextually Presenting Learning Videos Across Software Applica- tions. In Proceedings of the 2019 CHI Conference on Human Factors in Comput- ing Systems . Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3290605.3300527 [22] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. 1987. The Vocabulary Problem in Human-System Communication. Commun. ACM 30, 11 (Nov. 1987), 964–971. https://doi.org/10.1145/32206.32212 [23] Anushay Furqan, Chelsea Myers, and Jichen Zhu. 2017. Learnability through Adaptive Discovery Tools in Voice User Interfaces. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (Denver, Colorado, USA) (CHI EA ’17) . Association for Computing Machinery, New York, NY, USA, 1617–1623. https://doi.org/10.1145/3027063.3053166 [24] Tong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu, and Karrie G. Kara- halios. 2015. DataTone: Managing Ambiguity in Natural Language Interfaces for Data Visualization. In Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology (Charlotte, NC, USA) (UIST ’15) . As- sociation for Computing Machinery, New York, NY, USA, 489–500. https: //doi.org/10.1145/2807442.2807478 [25] Greasemonkey. 2021. Greasespot. Retrieved September 5, 2021 from https: //www.greasespot.net/ [26] Sandra G Hart and Lowell E Staveland. 1988. Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. In Advances in psy- chology , Vol. 52. Elsevier, 139–183. [27] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Ja- cob Steinhardt. 2021. Measuring Coding Challenge Competence With APPS. arXiv:2105.09938 [cs.SE] [28] Jane Im, Sonali Tandon, Eshwar Chandrasekharan, Taylor Denby, and Eric Gilbert. 2020. Synthesized Social Signals: Computationally-Derived Social Signals from Account Histories . Association for Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3313831.3376383 [29] Youwen Kang, Zhida Sun, Sitong Wang, Zeyu Huang, Ziming Wu, and Xiao- juan Ma. 2021. MetaMap: Supporting Visual Metaphor Ideation through Multi- Dimensional Example-Based Exploration. In Proceedings of the 2021 CHI Con- ference on Human Factors in Computing Systems (Yokohama, Japan) (CHI ’21) . Association for Computing Machinery, New York, NY, USA, Article 427, 15 pages. https://doi.org/10.1145/3411764.3445325 [30] Diederik P Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. arXiv:1312.6114 [stat.ML] [31] Amy J. Ko, Brad A. Myers, and Htet Htet Aung. 2004. Six Learning Barriers in End-User Programming Systems. In 2004 IEEE Symposium on Visual Languages - Human Centric Computing . IEEE, 199–206. https://doi.org/10.1109/VLHCC.2004. 47 [32] Ranjitha Kumar, Arvind Satyanarayan, Cesar Torres, Maxine Lim, Salman Ahmad, Scott R. Klemmer, and Jerry O. Talton. 2013. Webzeitgeist: Design Mining the Web. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . Association for Computing Machinery, New York, NY, USA, 3083–3092. https://doi.org/10.1145/2470654.2466420 [33] Ranjitha Kumar, Jerry O. Talton, Salman Ahmad, and Scott R. Klemmer. 2011. Bricolage: Example-Based Retargeting for Web Design . Association for Computing Machinery, New York, NY, USA, 2197–2206. https://doi.org/10.1145/1978942. 1979262 [34] Google Chrome Labs. 2018. VisBug. Retrieved August 26, 2021 from https: //visbug.web.app/ [35] Michelle S. Lam, Grace B. Young, Catherine Y. Xu, Ranjay Krishna, and Michael S. Bernstein. 2019. Eevee: Transforming Images by Bridging High-Level Goals and Low-Level Edit Operations. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI EA ’19) . Association for Computing Machinery, New York, NY, USA, 1–6. https://doi. org/10.1145/3290607.3312929 [36] Gierad P. Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang, Aseem Agar- wala, Jason Linder, and Eytan Adar. 2013. PixelTone: A Multimodal Interface for Image Editing . Association for Computing Machinery, New York, NY, USA, 2185–2194. https://doi.org/10.1145/2470654.2481301 [37] Brian Lee, Savil Srivastava, Ranjitha Kumar, Ronen Brafman, and Scott R. Klem- mer. 2010. Designing with Interactive Example Galleries . Association for Comput- ing Machinery, New York, NY, USA, 2257–2266. https://doi.org/10.1145/1753326. 1753667 [38] Toby Jia-Jun Li, Marissa Radensky, Justin Jia, Kirielle Singarajah, Tom M. Mitchell, and Brad A. Myers. 2019. PUMICE: A Multi-Modal Agent That Learns Concepts and Conditionals from Natural Language and Demonstrations. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (New Orleans, LA, USA) (UIST ’19) . Association for Computing Machinery, New York, NY, USA, 577–589. https://doi.org/10.1145/3332165.3347899 [39] Sarah Lim, Joshua Hibschman, Haoqi Zhang, and Eleanor O’Rourke. 2018. Ply: A Visual Web Inspector for Learning from Professional Webpages. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (Berlin, Germany) (UIST ’18) . Association for Computing Machinery, New York, NY, USA, 991–1002. https://doi.org/10.1145/3242587.3242660 [40] Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D. Ernst. 2018. NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) . European Language Resources Association (ELRA), Miyazaki, Japan. https://aclanthology.org/L18-1491 [41] Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, Fumin Wang, and Andrew Senior. 2016. Latent Predictor Networks for Code Generation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Berlin, Germany, 599–609. https://doi.org/10.18653/v1/P16-1057 [42] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT Understands, Too. arXiv:2103.10385 [cs.CL] [43] Edward Ma. 2019. NLP Augmentation. https://github.com/makcedward/nlpaug. [44] Mehdi Manshadi, Daniel Gildea, and James Allen. 2013. Integrating Programming by Example and Natural Language Programming. https://www.aaai.org/ocs/ index.php/AAAI/AAAI13/paper/view/6477/7230 [45] Rada Mihalcea, Hugo Liu, and Henry Lieberman. 2006. NLP (Natural Language Processing) for NLP (Natural Language Programming). In Computational Lin- guistics and Intelligent Text Processing , Alexander Gelbukh (Ed.). Springer Berlin Heidelberg, Berlin, Heidelberg, 319–330. [46] Michael Nebeling and Anind K. Dey. 2016. XDBrowser: User-Defned Cross-Device Web Page Designs . Association for Computing Machinery, New York, NY, USA, 5494–5505. https://doi.org/10.1145/2858036.2858048 [47] Michael Nebeling, Maximilian Speicher, and Moira C. Norrie. 2013. CrowdAdapt: Enabling Crowdsourced Web Page Adaptation for Individual Viewing Conditions and Preferences. In Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems (London, United Kingdom) (EICS ’13) . Association for Computing Machinery, New York, NY, USA, 23–32. https://doi.org/10.1145/ 2494603.2480304 [48] Jonas Oppenlaender, Thanassis Tiropanis, and Simo Hosio. 2020. CrowdUI: Supporting Web Design with the Crowd. Proc. ACM Hum.-Comput. Interact. 4, EICS, Article 76 (June 2020), 28 pages. https://doi.org/10.1145/3394978 [49] Thomas H. Park, Ankur Saxena, Swathi Jagannath, Susan Wiedenbeck, and Andrea Forte. 2013. OpenHTML: Designing a Transitional Web Editor for Novices. In CHI ’13 Extended Abstracts on Human Factors in Computing Systems (Paris, France) (CHI EA ’13) . Association for Computing Machinery, New York, NY, USA, 1863–1868. https://doi.org/10.1145/2468356.2468690 [50] Chris Quirk, Raymond Mooney, and Michel Galley. 2015. Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) . Association for Computational Linguistics, Beijing, China, 878–888. https://doi.org/10.3115/v1/P15-1085 [51] Daniel Ritchie, Ankita Arvind Kejriwal, and Scott R. Klemmer. 2011. D.Tour: Style-Based Exploration of Design Example Galleries. In Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology (Santa Barbara, California, USA) (UIST ’11) . Association for Computing Machinery, New York, NY, USA, 165–174. https://doi.org/10.1145/2047196.2047216 [52] Xin Rong, Shiyan Yan, Stephen Oney, Mira Dontcheva, and Eytan Adar. 2016. CodeMend: Assisting Interactive Programming with Bimodal Embedding. In Pro- ceedings of the 29th Annual Symposium on User Interface Software and Technology (Tokyo, Japan) (UIST ’16) . Association for Computing Machinery, New York, NY, USA, 247–258. https://doi.org/10.1145/2984511.2984544 [53] Viktor Schlegel, Benedikt Lang, Siegfried Handschuh, and André Freitas. 2019. Vajra: Step-by-Step Programming with Natural Language. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, Cal- ifornia) (IUI ’19) . Association for Computing Machinery, New York, NY, USA, 30–39. https://doi.org/10.1145/3301275.3302267 [54] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving Neu- ral Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, Berlin, Germany, 86–96. https://doi.org/10.18653/v1/P16-1009 [55] Hyungyu Shin, Eun-Young Ko, Joseph Jay Williams, and Juho Kim. 2018. Under- standing the Efect of In-Video Prompting on Learners and Instructors . Association for Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/ 3173574.3173893 [56] Arjun Srinivasan, Mira Dontcheva, Eytan Adar, and Seth Walker. 2019. Discov- ering Natural Language Commands in Multimodal Interfaces. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI ’19) . Association for Computing Machinery, New York, NY, USA, 661–672. https://doi.org/10.1145/3301275.3302292",0.08789870392156865,0.11149001944444449,0.9140204918300655,0.8930895143939394,,
214,162,16,Title,"[(0, 7)]",Stylete: Styling the Web with Natural Language,0.08790522875816993,0.0783156446969697,0.3156362630718955,0.0871209477272727,,
215,163,16,Bibliography,"[(7, 31), (31, 39), (39, 65), (65, 89), (89, 110), (110, 117), (117, 140), (140, 156), (156, 181), (181, 189), (189, 205)]","[57] Amanda Swearngin, Amy J. Ko, and James Fogarty. 2017. Genie: Input Retargeting on the Web through Command Reverse Engineering . Association for Computing Machinery, New York, NY, USA, 4703–4714. https://doi.org/10.1145/3025453. 3025506 [58] Kesler Tanner, Naomi Johnson, and James A. Landay. 2019. Poirot: A Web Inspector for Designers . Association for Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3290605.3300758 [59] Sebastian Weigelt, Vanessa Steurer, Tobias Hey, and Walter F. Tichy. 2020. Pro- gramming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 4280–4295. https://doi.org/10.18653/v1/ 2020.acl-main.395 [60] Haijun Xia. 2020. Crosspower: Bridging Graphics and Linguistics. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (Virtual Event, USA) (UIST ’20) . Association for Computing Machinery, New York, NY, USA, 722–734. https://doi.org/10.1145/3379337.3415845 [61] Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General- Purpose Code Generation. In The 55th Annual Meeting of the Association for Computational Linguistics (ACL) . Vancouver, Canada. https://arxiv.org/abs/1704. 01696 [62] Wojciech Zaremba, Greg Brockman, and OpenAI. 2021. OpenAI Codex. Retrieved August 28, 2021 from https://openai.com/blog/openai-codex/",0.08790522875816993,0.11149001944444449,0.4823918269607843,0.3416867366161616,,
216,164,16,Header,"[(205, 215)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636931434640523,0.07831131767676769,0.9120948856209149,0.0871166207070707,,
217,165,16,Bibliography,"[(215, 240), (240, 259), (259, 282), (282, 306), (306, 322), (322, 345), (345, 370), (370, 394), (394, 413), (413, 424)]","[63] Xiong Zhang and Philip J. Guo. 2018. Fusion: Opportunistic Web Prototyping with UI Mashups. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (Berlin, Germany) (UIST ’18) . Association for Computing Machinery, New York, NY, USA, 951–962. https://doi.org/10.1145/ 3242587.3242632 [64] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level Convolu- tional Networks for Text Classifcation. In NIPS . 649–657. http://papers.nips.cc/ paper/5782-character-level-convolutional-networks-for-text-classifcation [65] Nanxuan Zhao, Nam Wook Kim, Laura Mariah Herman, Hanspeter Pfster, Ryn- son W.H. Lau, Jose Echevarria, and Zoya Bylinskii. 2020. ICONATE: Automatic Compound Icon Generation and Ideation . Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3313831.3376618 [66] Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate Before Use: Improving Few-Shot Performance of Language Models. arXiv:2102.09690 [cs.CL] [67] Mingyuan Zhong, Gang Li, and Yang Li. 2021. Spacewalker: Rapid UI Design Ex- ploration Using Lightweight Markup Enhancement and Crowd Genetic Program- ming. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI ’21) . Association for Computing Machinery, New York, NY, USA, Article 315, 11 pages. https://doi.org/10.1145/3411764.3445326 [68] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generat- ing Structured Queries from Natural Language using Reinforcement Learning. arXiv:1709.00103 [cs.CL]",0.5195337581699346,0.11148969949494951,0.914021977124183,0.3416867366161616,,
