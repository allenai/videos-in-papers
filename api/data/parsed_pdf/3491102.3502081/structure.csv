,index,page,type,intervals,text,x1,y1,x2,y2,block_type,block_id
0,1,0,Title,"[(0, 7), (7, 8)]",Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility,0.09739215686274509,0.10411012500000007,0.9026059232026146,0.150996006060606,,
1,2,0,Author,"[(8, 10), (10, 14), (14, 18), (18, 19), (19, 21), (21, 25), (25, 28), (28, 32), (32, 33), (33, 35), (35, 39), (39, 40), (40, 44), (44, 45), (45, 47), (47, 49), (49, 51), (51, 55), (55, 59), (59, 60), (60, 64), (64, 67), (67, 71), (71, 72), (72, 76), (76, 80), (80, 81)]","Mina Huh School of Computing, KAIST Daejeon, Republic of Korea minarainbow@kaist.ac.kr Yunjung Lee Computer Science and Engineering, Ewha Womans University Seoul, Republic of Korea yunjung_lee20@ewhain.net Dasom Choi Department of Industrial Design, KAIST Daejeon, Republic of Korea dasomchoi@kaist.ac.kr Haesoo Kim Uran Oh Juho Kim School of Computing, KAIST Daejeon, Republic of Korea haesookim@kaist.ac.kr Computer Science and Engineering, Ewha Womans University Seoul, Republic of Korea uran.oh@ewha.ac.kr School of Computing, KAIST Daejeon, Republic of Korea juhokim@kaist.ac.kr",0.13562254901960785,0.16431485050505046,0.8797116810457518,0.32801722954545426,,
2,3,0,Paragraph,"[(81, 83)]",Abridged Description,0.15733228137254904,0.4231521575757576,0.2921052225490196,0.4341748848484849,,
3,4,0,Section,"(83, 85)",Selective Details,0.15733228137254904,0.45009170303030305,0.2621493401960784,0.46111443030303034,,
4,5,0,Figure,"[(85, 97), (97, 101), (101, 111), (111, 121), (121, 135), (135, 145), (145, 154), (154, 160), (160, 177), (177, 186), (186, 187), (187, 189), (189, 190), (190, 191), (191, 192), (192, 193)]","Brenda is playing the lyre. She looks calm and relaxed. Many stars shine in the background. Persephone holds Hades’ hands to figure out how to dance. She blushes as she looks at his hand touches hers. Hades and Persephone continue dancing in the background as the Nymphs are watching them. There are four people, three of them Nymphs and Thanatos. Thanatos is sweating and looking nervous. He’s biting his lips and his eyes are blue. Persephone draws closer to Hades as they dance. She is now floating next to Hades as they are dancing. Behind them, the bonfire is growing bigger. Comments-based Adaptive Description Panel-anchored Comments A B",0.1425717343137255,0.36918645252525256,0.7879761013071895,0.5791796704545454,,
5,6,0,Paragraph,"[(193, 195)]",Key Panels,0.822910049019608,0.37713300606060607,0.8919084313725492,0.38815573333333336,,
6,7,0,Figure,"[(195, 211), (211, 213)]","“Her dress was gorgeous, when it was fl ying as she danced. I guess it’s silk?” Panel-anchored Comments",0.5288463071895425,0.5425652323232324,0.7144539052287583,0.5807270164141415,,
7,8,0,Caption,"[(213, 231), (231, 245), (245, 264)]","Figure 1: Cocomix is an interactive webtoon reader that leverages comments data to provide non-visual access to webtoon contents. Cocomix has two main components: (A) comments-based adaptive descriptions and (B) panel-anchored comments. With (A), readers can listen to more detailed descriptions in key panels which are frequently mentioned in comments. Abridged",0.08715802875816987,0.6074442404040404,0.914601448366013,0.6464684585858584,Figure,1.0
8,9,0,Abstract,"[(264, 282), (282, 297), (297, 298), (298, 309), (309, 320), (320, 329), (329, 339), (339, 347)]","descriptions are provided for remaining panels as default and readers can selectively access more details. With (B), readers can easily access descriptive comments relevant to the current panel. Image from [WEBTOON SERIES] (2022). ABSTRACT Webtoon is a type of digital comics read online where readers can leave comments to share their thoughts on the story. While it has experienced a surge in popularity internationally, people with visual impairments cannot enjoy webtoon with the lack of an accessible format. While traditional image description practices",0.08720261437908497,0.6489478040404041,0.9120961307189541,0.7683537515151515,,
9,10,0,Paragraph,"[(347, 378), (378, 408), (408, 435)]","Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission",0.08790522875816993,0.7837160295454545,0.4806825929738563,0.8428395818181817,,
10,11,0,Footnote,"[(435, 452), (452, 467)]","and/or a fee. Request permissions from permissions@acm.org. CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9157-3/22/04...$15.00",0.08711896699346405,0.844096080050505,0.4731393081699347,0.8851023762626262,,
11,12,0,Footer,"[(467, 468)]",https://doi.org/10.1145/3491102.3502081,0.08790522875816993,0.8863615345959596,0.27277929934640527,0.8951668376262627,,
12,13,0,Abstract,"[(468, 476), (476, 487), (487, 495), (495, 506), (506, 516), (516, 524), (524, 534), (534, 542), (542, 552), (552, 560), (560, 570), (570, 573)]","can be adopted, resulting descriptions cannot preserve webtoons’ unique values such as control over the reading pace and social engagement through comments. To improve the webtoon read- ing experience for BLV users, we propose Cocomix , an interactive webtoon reader that leverages comments into the design of novel webtoon interactions. Since comments can identify story highlights and provide additional context, we designed a system that provides 1) comments-based adaptive descriptions with selective access to details and 2) panel-anchored comments for easy access to rele- vant descriptive comments. Our evaluation (N=12) showed that Cocomix users could adapt the description for various needs and better utilize comments.",0.5189918300653594,0.6843886,0.9145628003267974,0.8479156202020203,,
13,14,1,Header,"[(0, 10), (10, 13)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Huh et al.",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
14,15,1,Paragraph,"[(13, 15)]",CCS CONCEPTS,0.08790522875816993,0.10765139154040401,0.22029393088235297,0.12142550770202028,,
15,16,1,Title,"[(15, 22)]",• Human-centered computing → Accessibility systems and,0.08790522875816993,0.12593875353535355,0.48046359738562094,0.13817698383838384,,
16,17,1,Keywords,"[(22, 29), (29, 30), (30, 37), (37, 38)]","tools ; Empirical studies in accessibility . KEYWORDS Accessibility, Digital comics, Webtoons, Image description, Com- ments",0.0873921568627451,0.1406676505050504,0.4829436818627451,0.21096865050505056,,
17,18,1,Paragraph,"[(38, 53), (53, 72), (72, 90)]","ACM Reference Format: Mina Huh, Yunjung Lee, Dasom Choi, Haesoo Kim, Uran Oh, and Juho Kim. 2022. Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility. In CHI Conference on Human Factors in Computing Systems (CHI ’22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 18 pages. https://doi.org/10.1145/3491102.3502081",0.08711111111111111,0.22011489027777778,0.4818937712418301,0.29309347638888894,,
18,19,1,Section,"(90, 92)",1 INTRODUCTION,0.08790522875816993,0.31039381578282826,0.2557663312091503,0.32416793194444454,,
19,20,1,Paragraph,"[(92, 104), (104, 114), (114, 122), (122, 131), (131, 140), (140, 149), (149, 160), (160, 171), (171, 182), (182, 195), (195, 205), (205, 215), (215, 224), (224, 236), (236, 245)]","Webtoon, a portmanteau of “web” and “cartoons,” is a type of digital comics published and read online. Optimized for web and mobile consumption, webtoons abandoned the traditional layout of panels and adopted the single-strip display where readers navigate panels with scrolling or swiping gestures. Unlike traditional media chan- nels for comics, webtoon platforms facilitate active participation of readers by enabling them to add comments on each episode. Also, the short reading time per episode has enabled readers to enjoy webtoons as a lightweight medium, often referred to as ‘snack cul- ture’ 1 . While webtoons originated in South Korea, there has been a surge in popularity internationally. For instance, Naver Webtoon 2 3 has more than 67 million monthly active users (MAU) worldwide. Together with the commercial success of adaptations into flms or dramas (e.g., Sweet Home, Along with the Gods ), webtoons now became one of the mainstream cultures in many countries.",0.08720261437908497,0.32959945858585854,0.48294427973856213,0.534639105050505,,
20,20,1,Paragraph,"[(245, 255), (255, 265), (265, 273), (273, 282), (282, 292), (292, 302), (302, 313), (313, 322), (322, 328)]","However, blind or low vision (BLV) people cannot enjoy this popular medium because of the accessibility barriers posed by the foundation of comics—combining graphics and text. Incapable of participating in what is considered the cultural mainstream, BLV people feel excluded [74]. The most standard and economical way of providing accessible version of images is via textual description. However, it is yet to be explored how the conventional image description practices can be generalized to webtoons, a unique media as a sequence of images.",0.08790522875816993,0.5371550141414141,0.48272518692810457,0.6591719333333333,,
21,20,1,Paragraph,"[(328, 337), (337, 345), (345, 354), (354, 363), (363, 373), (373, 383), (383, 393), (393, 401), (401, 412), (412, 423), (423, 426)]","To better understand BLV readers’ needs in webtoon descrip- tions, we conducted formative interviews followed by co-reading exercises with 10 BLV participants. From the two-stage formative study, we have identifed the following challenges when adopting traditional description practices in webtoons. First, the fow of the plot is disturbed by the repetitive information in image descrip- tions. Second, BLV readers have difculty in processing the given information when reading long descriptions. Third, BLV readers lack control over the amount of details in the description. Finally, BLV readers have difculty in fnding comments that are useful for understanding the content.",0.08790522875816993,0.6616878424242424,0.48293916094771233,0.8113790040404041,,
22,20,1,Paragraph,"[(426, 436), (436, 445)]","To overcome the challenges, we leverage comments data to im- prove webtoon descriptions. We believe that comments can be",0.08790522875816993,0.8138949131313131,0.4829345424836601,0.8390532464646465,,
23,21,1,Footnote,"[(445, 449), (449, 469)]","1 https://en.wikipedia.org/wiki/Snack_culture 2 https://www.webtoons.com/en/ 3 This work is not endorsed by WEBTOON. Views, opinions, and thoughts in this work are solely of the author.",0.08748366013071895,0.852853817550505,0.4804702799019607,0.8951665345959596,,
24,22,1,Paragraph,"[(469, 478), (478, 489), (489, 499), (499, 508), (508, 518), (518, 529), (529, 539), (539, 549), (549, 560), (560, 570), (570, 577)]","utilized to enrich webtoon descriptions, as they implicitly high- light key moments in the content through the reactions of readers, and provide additional context to the story. While comments data present a unique opportunity in augmenting visual descriptions for BLV people, to our knowledge, researchers have not yet attempted to leverage comments as the meta data for accessibility. To explore the feasibility and limitations of using webtoon comments for elicit- ing useful information for BLV readers, we performed a qualitative content-analysis of a set of 1,000 comments. From the results, we distilled the criteria of descriptive comment , which informed our approach of extraction and usage of comments.",0.5195343137254901,0.10956031717171716,0.9145661475490198,0.25925147878787885,,
25,22,1,Paragraph,"[(577, 586), (586, 595), (595, 602), (602, 610), (610, 623), (623, 633), (633, 642), (642, 651), (651, 662), (662, 673), (673, 680)]","We developed Cocomix (Figure 1), a prototype webtoon reader that provides efcient and interactive webtoon descriptions to BLV users through comments-driven interaction techniques. Our com- putational pipeline automatically calculates the importance of each panel based on the comment data. BLV users can listen to the de- scription with adaptive levels of detail depending on its importance (Figure 1.A). To provide additional context, Cocomix also extracts descriptive comments and links them to corresponding panels (Fig- ure 1.B). While reading, BLV users have active control over the reading pace and the level of detail by requesting additional details or relevant comments per panel on demand.",0.51909477124183,0.2617673878787879,0.9145764196078431,0.41145854949494953,,
26,22,1,Paragraph,"[(680, 690), (690, 699), (699, 707), (707, 716), (716, 726), (726, 737), (737, 739)]","To assess the feasibility of Cocomix for enhancing the webtoon experience for BLV readers, we conducted a within-subjects study with 12 BLV people. Participants found Cocomix’s comments-driven interactions useful for controlling the reading pace and utilizing comments. They also enjoyed having options to adapt the descrip- tion for diferent reading contexts such as light reading and detail- focused reading.",0.5189918300653594,0.4139744585858586,0.9145722019607843,0.5083171353535354,,
27,22,1,Paragraph,"[(739, 746)]",This paper makes the following main contributions:,0.5358115529411764,0.5108304444444444,0.8462511764705883,0.5221516565656565,,
28,23,1,List,"[(746, 754), (754, 764), (764, 773), (773, 777), (777, 787), (787, 791), (791, 800), (800, 803)]","• Identifcation of high-level goals for webtoon accessibility • A design approach that leverages comments data to improve the webtoon reading experience, backed up with the analysis of 1,000 webtoon comments • Cocomix, a prototype webtoon reader that supports an in- teractive webtoon reading experience • An empirical evaluation showing the value of comments- driven interaction techniques",0.545617077124183,0.5327369898989899,0.9145763173202616,0.6418411252525253,,
29,24,1,Section,"(803, 806)",2 RELATED WORK,0.5195343137254902,0.6621678056818182,0.689124472875817,0.6759419218434345,,
30,25,1,Paragraph,"[(806, 815), (815, 823), (823, 832), (832, 842), (842, 849)]","We review previous research in accessible representations of images (Section 2.1), background for understanding comics, webtoons and research around them (Section 2.2), existing approaches for accessi- ble comics (Section 2.3), and prior works on leveraging comments as a supplemental knowledge source (Section 2.4).",0.5188316993464052,0.6813721858585858,0.9145661475490198,0.7480418828282829,,
31,26,1,Section,"(849, 853)",2.1 Accessible Image Representations,0.5195343137254902,0.7683685632575757,0.8330105107843138,0.7821426794191919,,
32,27,1,Paragraph,"[(853, 863), (863, 874), (874, 888), (888, 899), (899, 910), (910, 919), (919, 927), (927, 937)]","Digital imagery is pervasive in the online world, facilitating the sharing of information and ideas. BLV people often use screen read- ers and screen magnifers [47, 74, 78] to access them. In order for a screen reader to render an image, an alternative text description [72] should be provided which can be generated manually by humans or automatically by using object recognition techniques [79]. With the advancement in computer vision techniques, prior research have increased the image accessibility at scale [27, 79]. For interactive",0.5195328537581697,0.7875729434343434,0.9145661475490194,0.8957527414141414,,
33,28,2,Header,"[(0, 8), (8, 18)]","Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
34,29,2,Paragraph,"[(18, 28), (28, 34)]","and personalized alt-text, Morris et al. [48] has proposed alt-text with progressive-detail and question & answer.",0.08736437908496732,0.10956031717171716,0.4804697057189542,0.13471865050505044,,
35,29,2,Paragraph,"[(34, 45), (45, 56), (56, 69), (69, 77), (77, 86), (86, 94), (94, 102)]","A number of studies have been conducted to add image de- scriptions to various classes of images such as STEM images [46], memes [26], and GIFs [25]. For eyes-free art, Rector et al. [60] pro- posed proxemic audio interfaces. For artwork exploration using touch screen, Ahmetovic et al. [1] proposed attribute-based and hierarchical exploration. Also, EyeDescribe [61] utilized eye gaze data and a spoken description for image labelling.",0.08790522875816993,0.1372345595959596,0.4829391609477124,0.23157723636363645,,
36,29,2,Paragraph,"[(102, 110), (110, 123), (123, 132)]","There have been many guideline-level suggestions for represen- tations of various classes of images. In this paper, we focus on a class of images which remains relatively inaccessible — webtoons.",0.08790522875816993,0.23409314545454543,0.48293294379084967,0.2730886,,
37,30,2,Section,"(132, 138)",2.2 Comics as a Unique Medium,0.08790522875816993,0.28706427032828286,0.35865411748366016,0.3008383864898991,,
38,31,2,Paragraph,"[(138, 148), (148, 157), (157, 167), (167, 180), (180, 191), (191, 203), (203, 214), (214, 223), (223, 228)]","Broadly, comics are “Juxtaposed pictorial and other images in delib- erate sequence, intended to convey information and/or to produce an aesthetic response” [41]. Traditional comics are printed on pa- pers, where each page is composed of a set of panels presenting a specifc situation. The panels are often separated by a white space area named gutter [11]. Because of this gutter, all comics have the concept of closure , the “phenomenon of observing the parts but perceiving the whole” [41]. With closure, readers can comprehend the meaning between neighboring panels.",0.08790522875816993,0.3062686505050505,0.4829415475490195,0.4282868323232323,,
39,31,2,Paragraph,"[(228, 238), (238, 249), (249, 260), (260, 271), (271, 283), (283, 296), (296, 305), (305, 316), (316, 329), (329, 339), (339, 350), (350, 358)]","After a slump in the traditional comic industry, comics turned their stage to digital platforms to bring a broader audience. The infltration of comics into the digital world has not only challenged the traditional structure of comics but also enabled anyone to pub- lish and access comics [34]. Webtoon is a type of digital comics that adopts a vertical layout so that users can scroll down on web platforms and mobile devices. The amount of materials published in webtoon form has reached an equal amount as that published ofine 4 . While there are other types of web comics published in online platforms or social media such as Tumblr, Tapastic, Face- book, and Instagram, we focus on webtoons based on their large readership and the relatively structured and consistent format.",0.08790522875816993,0.4308014787878788,0.4829377397058823,0.5943297616161617,,
40,31,2,Paragraph,"[(358, 367), (367, 379), (379, 391), (391, 402), (402, 412), (412, 423), (423, 433), (433, 442), (442, 449)]","Research around comics has been conducted in contents analy- sis [11, 15, 52, 53], content adaptation [6, 37], or interactions [59, 70, 76, 77]. Iyyer et al. [29] explored how computers can under- stand the closure-driven narratives in comics. To address the loss of author-intended segmentation in digital comics, Wang et al. [76, 77] introduced the concept of phasels , a new segmentation of panels that describes a strong relation among certain panels. Our work builds upon prior research on comics interactions by exploring non-visual interaction techniques while focusing on webtoons.",0.08756862745098039,0.5968456707070707,0.482947831372549,0.7188638525252525,,
41,32,2,Section,"(449, 453)",2.3 Making Comics Accessible,0.08790522875816993,0.7328382602272727,0.3423974194444444,0.7466123763888889,,
42,33,2,Paragraph,"[(453, 465), (465, 477), (477, 488), (488, 498), (498, 507), (507, 517), (517, 529), (529, 542), (542, 554)]","To make an accessible version of comics, tactile books have been de- signed. For example, braille formats [12, 14, 23, 51, 69] and textured images [58] were proposed so that BLV people can read comics by touch. Other approaches have investigated how to improve ac- cessibility in digital platforms by translating the visual content to speech and sound efects. For instance, GraphicAudio [10] and Unseen [3] presents audiobooks that can be read without an aid of a screen reader. Rayar et al. [59] devised a digital comic viewer with magnifying features for people with low vision, and Lee et al. [36]",0.0874656862745098,0.752043903030303,0.4829491566993464,0.8740608222222223,,
43,34,2,Footnote,"[(554, 556)]",4 http://everything.explained.today/Webtoon/,0.08769934640522875,0.8844750296717171,0.298175668627451,0.8951665345959596,,
44,35,2,Paragraph,"[(556, 565), (565, 577), (577, 588), (588, 598), (598, 610), (610, 619), (619, 629), (629, 639), (639, 651), (651, 661)]","explored BLV people’s preferences on audiobook and eBook format of digital comics. To create comic descriptions, Samson et al. [63] ex- plored the potential of fan readers in describing comics. NNELS [55] has worked on developing a standardized approach to creating a “Described Comic Book”, yet, to the best of our knowledge, no prior work has attempted to create descriptions specifcally for webtoons. Rayar et. al. [58] argued that desirable solution approaches should be less expensive, less time-consuming and allowing access to an important part of the culture. Inspired by this, we explore how to support BLV population to be part of the webtoon culture.",0.5178790849673203,0.10956031717171716,0.914571704738562,0.24541435757575758,,
45,36,2,Section,"(661, 668)",2.4 Comments as a Supplemental Knowledge Source,0.5195343137254902,0.25899356325757567,0.894044429738562,0.28911755530303035,,
46,37,2,Paragraph,"[(668, 680), (680, 691), (691, 700), (700, 709), (709, 718), (718, 728), (728, 737), (737, 745), (745, 756), (756, 760)]","Comments are a type of user engagement to express opinion or feel- ings visible to platform users [2]. Comments are capable of adding supplemental information to the original content, which can be interesting to other users [45]. To understand the commenting and rating behavior, Siersdorfer et al. [64] analyzed dependencies between comments and other metadata of the content. To under- stand how people contextualize comments, Yarmand et al. [81] explored reference patterns in YouTube comments. Also, Momeni et al. [44, 45] analyzed properties of useful comments for annotation of social media objects.",0.5195343137254901,0.2945514787878788,0.9145661475490194,0.43040551919191916,,
47,37,2,Paragraph,"[(760, 767), (767, 778), (778, 789), (789, 799), (799, 807), (807, 813), (813, 819)]","While reader comments contain diverse topics, conventional ranking mechanisms for comments (e.g., by posted date or by likes fail to provide an overview of topics discussed in comments [40]. For efcient browsing of comments, Opinion Space [19] and Star- ryThoughts [33] support navigation of diverse comments through visualization. RevMiner investigated extractive presentation of attribute-value pairs to summarize unstructured texts.",0.5191683006535948,0.43292142828282826,0.9145742879084966,0.5272641050505051,,
48,37,2,Paragraph,"[(819, 829), (829, 838), (838, 846), (846, 858), (858, 867), (867, 878), (878, 887), (887, 896), (896, 905)]","There have been suggestions for utilizing online reviews in the accessibility domain [38, 67]. Revamp [75] supports interactive in- formation retrieval by leveraging customer reviews. For comments, Voykinska et al. [74] has observed that BLV people rely on oth- ers’ comments to understand visual contents in social networking services. Still, to our knowledge, no prior work has attempted to leverage comments to augment visual description. We look into the opportunities and challenges of using comments, which are inherently more personal and diverse in context than reviews.",0.5190506535947712,0.5297800141414142,0.9145778709150327,0.651798195959596,,
49,37,2,Paragraph,"[(905, 916), (916, 924), (924, 929)]","Our work builds upon prior work on comments mining by analyz- ing patterns of webtoon comments, and designing comments-driven interaction techniques for webtoon accessibility.",0.5195317460784316,0.6543128424242425,0.9145779379084968,0.693308296969697,,
50,38,2,Section,"(929, 932)",3 FORMATIVE STUDY,0.5195343137254902,0.7068875026515151,0.7144007862745098,0.7206616188131313,,
51,39,2,Paragraph,"[(932, 941), (941, 951), (951, 961), (961, 972), (972, 980)]","We conducted semi-structured interviews with 10 BLV people fol- lowed by a webtoon co-reading exercise to better understand their needs in webtoon descriptions. We frst present methods and results of the study. Then, we discuss challenges and design goals, and present guidelines for webtoon descriptions refecting the fndings.",0.5188316993464052,0.7260931454545454,0.914570577777778,0.7927615797979798,,
52,40,2,Section,"(980, 982)",3.1 Methods,0.5195343137254902,0.8063407854797979,0.6297483093137255,0.8201149016414142,,
53,41,2,Paragraph,"[(982, 993), (993, 1003), (1003, 1017), (1017, 1028)]","3.1.1 Participants. We recruited 10 BLV people (6 male, 4 female) interested in webtoons through mailing lists (Table 8). They aver- aged 26 years old (min = 22, max = 35) and described their vision as either blind (7 participants) or low vision (3 participants). All",0.5195343137254902,0.8429214282828282,0.9145672171568626,0.8957527414141414,,
54,42,3,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.3363069709150327,0.0871209477272727,,
55,43,3,Paragraph,"[(10, 13)]",Huh et al.,0.8658485718954247,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
56,44,3,Table,"[(13, 16), (16, 18), (18, 24), (24, 25), (25, 26), (26, 27), (27, 30), (30, 33), (33, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44)]",Webtoons ID Genre Title (episode) Length Drawing Style Popularity (likes) URL W1 W2 Sports Slice of life The Boxer (1) Independent Diary (77) long short reality abstract 99K+ 99K+ [31] [30],0.18126470588235294,0.11076612525252529,0.8187435137254903,0.1554622707070707,,
57,45,3,Caption,"[(44, 57), (57, 66), (66, 76), (76, 85)]","Table 1: Two webtoons used in the formative study for the co-reading exercise participants were screen reader users. Three participants had prior experience with paper comics before losing sight. We provided a 15,000 KRW (approx. $12 USD) compensation for their time.",0.08756862745098039,0.16209650101010087,0.7576782274509807,0.24299895353535353,,
58,46,3,Paragraph,"[(85, 92), (92, 101), (101, 109), (109, 119), (119, 123)]","3.1.2 Semi-structured Interview. In the semi-structured interviews, participants were asked about their motivations for reading webtoons, prior experiences with webtoons, their perceived accessibility, strate- gies to access webtoons, and accessibility barriers that they encoun- tered with those approaches.",0.08790522875816993,0.25330147878787873,0.4911211263071896,0.3199711757575757,,
59,46,3,Paragraph,"[(123, 131), (131, 142), (142, 151), (151, 159), (159, 170), (170, 182), (182, 193), (193, 201), (201, 203)]","3.1.3 Co-reading Exercise. We also conducted webtoon co-reading exercises to see what kind of information is desired in webtoon descriptions. BLV participants read two webtoons through a default image description provided by researchers. After one researcher read the default image description of each panel out loud, partici- pants were encouraged to ask for further details they wish to know from the panel. We selected two webtoons (Table 1) from Naver’s most-popular-by-genre 5 page that represents diferent genres and drawing styles.",0.08790522875816993,0.33027370101010095,0.48294370915032675,0.4522906202020202,,
60,46,3,Paragraph,"[(203, 214), (214, 223), (223, 232), (232, 241), (241, 250), (250, 259), (259, 269)]","In the default image description, we provided a brief visual de- scription of scene (story setting, objects, characters, actions) per panel following established guidelines of [55]. All textual content such as dialogue and onomatopoeia were delivered. The reason behind providing a minimum default description was to encourage BLV participants to actively request additional information so that we can analyze types of information desired in webtoon description.",0.08736437908496732,0.4548065292929293,0.482935179738562,0.5491504686868687,,
61,46,3,Paragraph,"[(269, 278), (278, 290), (290, 299), (299, 309), (309, 319), (319, 328), (328, 336), (336, 345), (345, 355), (355, 367), (367, 379), (379, 387), (387, 397), (397, 409), (409, 418)]","3.1.4 Analysis. Each session of interview and co-reading exercise was audio-recorded, transcribed 6 , and coded by three of the re- searchers following the thematic analysis methods [24]. To analyze the types of information requested in the co-reading exercise, two of the researchers labeled and divided questions into three groups based on the underlying intent: visual information seeking, opin- ion seeking, and confrmation seeking. The distinction between information seeking and opinion seeking questions was based on whether the question asks about factual information such as “What is the color of his shirt?” or asks about relatively subjective informa- tion that involves researchers’ opinion like “Do you think the guy is good-looking?” . Questions were categorized as confrmation seeking if they were yes-no question about the correctness of participants’ interpretation’s such as “So the character is still in the gym, right?” or if they asked researchers to repeat the description.",0.08736437908496732,0.559452993939394,0.48294674591503267,0.7644926404040404,,
62,47,3,Section,"(418, 420)",3.2 Results,0.08790522875816993,0.7788609875000001,0.18794096274509806,0.7926351036616162,,
63,48,3,Paragraph,"[(420, 430), (430, 438), (438, 447), (447, 457), (457, 464)]","In this section, we present BLV people’s motivation for reading webtoons (Section 3.2.1), prior experiences with webtoons (Sec- tion 3.2.2), and the desired information in webtoon descriptions (Section 3.2.3). Then, we present challenges and design goals to improve webtoon reading experiences for BLV readers.",0.0873694843137255,0.7946070343434344,0.4829508972222222,0.8612767313131313,,
64,49,3,Footnote,"[(464, 468)]",5 https://www.webtoons.com/en/top 6 https://clovanote.naver.com,0.08790522875816993,0.8738550801767677,0.2552738950980392,0.8951665345959596,,
65,50,3,Paragraph,"[(468, 476), (476, 488), (488, 499), (499, 506), (506, 518), (518, 532), (532, 542), (542, 554), (554, 564), (564, 573), (573, 584), (584, 595), (595, 604), (604, 614), (614, 623), (623, 630)]","3.2.1 Motivation for reading webtoons. All participants answered that they wish to be able to read webtoons. In particular, seven participants specifed that they wish to get involved in what is considered the ‘mainstream’. They frequently encountered web- comics posted in social media (P1, P3, P7) or movies and drama adaptations of webtoon (P1, P4, P7), and wished to be able to join in on the conversation elicited from these particular forms of media. Six of the participants (P1-P3, P7, P8) said that they believe they can have a better understanding of context in conversations with other people by reading webtoons. Participants noted that their friends often talked about how they enjoyed an episode (P5, P8), or sometimes referred to a webtoon character as a metaphor (P8). Four of the participants mentioned the unique characteristics of webtoons as the motivation for reading webtoons. They found the visual storytelling technique interesting (P5, P6), and admired the quick speed of reading (P3, P4, P5).",0.5189918300653594,0.20400349898989895,0.9145628003267974,0.42288026666666667,,
66,50,3,Paragraph,"[(630, 639), (639, 649), (649, 658), (658, 668), (668, 679), (679, 688), (688, 700), (700, 708), (708, 719), (719, 730), (730, 740), (740, 751), (751, 762), (762, 774), (774, 787), (787, 800), (800, 809)]","3.2.2 Prior experiences with webtoons. We asked participants about their prior experiences encountering webtoons on the web as a whole. None of the participants had encountered webtoons with an embedded alt-text. For workarounds, they asked friends or fam- ily members to verbally describe the content (P1, P3, P7, P10), watched an adaptation such as webtoon-based dramas or movies (P3), searched for summaries in the fandom wiki (P6), or used image recognition apps (P1,P6). Still, participants mentioned the limita- tions of these approaches. Asking others for help was not always viable and referring to adaptations or summaries did not cover some information that they could only gain from reading the original material. Also, image recognition apps were not reliable due to low accuracy. Six of the participants mentioned that they refer to com- ments posted by other readers for each episode to guess the content (P1, P3, P5, P6, P9, P10). However, they reported that it is difcult to utilize comments as many of them are not directly related to the story and thus not helpful in understanding the content.",0.5189918300653594,0.4253961757575757,0.9145723062091503,0.658108802020202,,
67,50,3,Paragraph,"[(809, 818), (818, 832), (832, 844), (844, 853), (853, 865), (865, 876), (876, 886), (886, 894), (894, 904), (904, 917), (917, 928), (928, 936), (936, 947)]","3.2.3 Desired information in webtoon descriptions. In the co-reading exercise, a total of 307 questions (117 asked in W1, 190 in W2) were asked by the participants. More than half of the questions (62%) con- sisted of participants seeking visual information and among them, only 1% of the questions were about drawing styles or view angle and none of the participants asked about the panel layout. Twenty percent were opinion seeking questions and eighteen percent of the questions were confrmation seeking questions. We observed that this type of questions were frequent especially when reading panels near the end, as readers had to keep track of all the information received up to that point. Also, participants who asked many ques- tions about visual information requested for confrmation more often compared to others, as they had to process more information.",0.5195343137254901,0.6606247111111111,0.917193662745098,0.8379901151515151,,
68,51,4,Header,"[(0, 8), (8, 18)]","Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
69,52,4,Section,"(18, 23)",3.3 Challenges and Design Goals,0.08790522875816993,0.10765139154040401,0.362023104248366,0.12142550770202028,,
70,53,4,Paragraph,"[(23, 31), (31, 41), (41, 47)]","Combining both interview responses and observations from co- reading exercise, we identifed four challenges that BLV people face when reading webtoons via image descriptions:",0.08736437908496732,0.1268557717171717,0.48293371535947693,0.16585122626262624,,
71,54,4,List,"[(47, 56), (56, 59), (59, 68), (68, 71), (71, 83), (83, 93), (93, 96)]",C1. Confusion in understanding the plot with recurring details in image descriptions C2. Difculty in processing the given information when reading the long description C3. Lack of control over the amount of details in the description C4. Difculty in fnding comments that are useful for under- standing the content,0.10176907450980391,0.18047445858585856,0.4829472323529411,0.274818397979798,,
72,55,4,Paragraph,"[(96, 106), (106, 116), (116, 126), (126, 137), (137, 148), (148, 157), (157, 166), (166, 176), (176, 188), (188, 195), (195, 203)]","The frst challenge (C1) is the issue of repetitive information interrupting the fow of the plot. During the co-reading exercise, P2 reported that “Sometimes, same details are repeated, like the character’s pose even when it hasn’t changed. It disturbs the read- ing process.” While webtoons consist of a sequence of images, the individual images come together to form a comprehensive plot [41]. This means that following traditional image description prac- tices would not be appropriate for webtoons. We recognized that there would be a need to create a separate, webtoon-specifc set of description guidelines to minimize information redundancy and would be more optimized for conveying the plot.",0.08736437908496732,0.2894416303030303,0.4829479781045751,0.43913279191919197,,
73,55,4,Paragraph,"[(203, 215), (215, 224), (224, 235), (235, 245), (245, 256), (256, 268), (268, 276), (276, 285), (285, 294), (294, 304), (304, 313), (313, 323), (323, 332), (332, 342), (342, 352), (352, 363), (363, 376)]","The second challenge (C2) is that BLV readers fnd it difcult to process all given information when listening to long descriptions. Compared to image posts on social media where few alt-texts can cover the content, webtoon descriptions tend to be much longer. Also, unlike a described video where audio cues help in picturing the scene, the delivery of a webtoon’s content has to rely solely on written description. When given a long-winded description, BLV readers often face difculty in processing and remembering the information provided. P7 mentioned “Whoa, can you repeat everything again? There is just too much information to handle. Descriptions, dialogues, and for dozens of images!”. This becomes especially challenging when readers are not familiar with the setting or characters. For efcient delivery of information, three partici- pants (P2, P5, P10) suggested diferentiating the amount of details depending on the importance of elements. For example, when P5 was presented with an image of multiple characters, he asked for each character “Do you think this person is important? If not, can I",0.08737093464052283,0.441648701010101,0.48293371535947704,0.6743613272727272,,
74,55,4,Paragraph,"[(376, 390), (390, 401), (401, 413), (413, 423), (423, 432), (432, 442), (442, 451), (451, 463), (463, 471)]","skip? I’m not interested in his hair color if he won’t show up again.” The third challenge (C3) is that BLV readers have no control over how much detail is provided in the description. From the co- reading exercise, we observed that the number of questions asked by participants varied considerably, which implies that every BLV reader has a diferent taste in description. However in conventional image descriptions, the amount of information delivered is solely decided by describers. This takes away one of the unique values of reading webtoons, the active control over reading pace.",0.08790522875816993,0.6768746363636363,0.4829381321895424,0.7988941555555555,,
75,55,4,Paragraph,"[(471, 482), (482, 491), (491, 502), (502, 511), (511, 520), (520, 529), (529, 541)]","The fnal challenge (C4) is that BLV readers have no efcient ways to cherry-pick comments that are helpful for understanding the content. While referring to comments was one of the main approaches to guess the content or complement the description, all participants who attempted to read comments reported the inconvenience of utilizing them. For example, P1 mentioned “Most comments are like “This is hilarious!” and I am left wondering what",0.08736437908496732,0.8014100646464647,0.48208190996732014,0.8957527414141414,,
76,56,4,Figure,"[(541, 557), (557, 569), (569, 571), (571, 572), (572, 573), (573, 577), (577, 581), (581, 585), (585, 589)]","Pixie the cat and a porcupine is in a meadow. Pixie has just discovered the porcupine. Pixie is taking a step toward the porcupine. Brutus appears from behind. Panel Panel Panel Phasel Description for a phasel Description for a panel Same characters, continued event New character, new action",0.5417938603039216,0.11979077393560617,0.9210812729901962,0.29307497081818185,,
77,57,4,Caption,"[(589, 602), (602, 611), (611, 621), (621, 632), (632, 642), (642, 650), (650, 655)]","Figure 2: A phasel is a set of multiple panels that can be grouped or summarized into a single unit. Our guidelines suggest grouping similar panels, such as when the same peo- ple are appearing or a single action is described over mul- tiple panels, into a single phasel to avoid repetition. The grouping can only happen between neighboring panels. Im- age from [WEBTOON SERIES] (2022).",0.5195343137254902,0.3172636848484848,0.9147152078431376,0.41159202424242414,Figure,6.0
78,58,4,Paragraph,"[(655, 666), (666, 674)]","is hilarious.” From the feedback, we distilled the need for extracting comments that are useful for understanding the content.",0.5195343137254902,0.45028001414141416,0.9120965704248366,0.4754383474747475,,
79,58,4,Paragraph,"[(674, 684)]","Based on the challenges, we identifed four design goals for",0.5358137254901961,0.47795425656565654,0.912371484640523,0.48927546868686866,,
80,59,4,List,"[(684, 693)]",systems to improve webtoon reading experience for BLV people.,0.5195397593137256,0.4917887777777777,0.9143590305555559,0.5031099898989898,,
81,60,4,Paragraph,"[(693, 705), (705, 717), (717, 724)]","Two of the design goals address how the description can be more efcient (D1, D2), and the other two design goals address how users can interactively control the reading (D3, D4).",0.51909477124183,0.5056284989898989,0.9120966560457516,0.5446239535353535,,
82,61,4,List,"[(724, 731), (731, 733), (733, 744), (744, 748), (748, 758), (758, 765), (765, 775), (775, 777)]","D1. Provide descriptions with minimized information redun- dancy (C1). D2. Provide adaptive levels of detail for each panel based on readers’ collective interest (C2). D3. Present additional details on demand to support active con- trol over the reading pace (C2, C3). D4. Support selective access to comments that describe the con- tent (C4).",0.5326029882352941,0.5530199131313132,0.9145763173202615,0.6611997111111112,,
83,62,4,Section,"(777, 782)",3.4 Guidelines for Webtoon Descriptions,0.5195343137254902,0.6782928056818182,0.8590354879084967,0.6920669218434344,,
84,63,4,Paragraph,"[(782, 793), (793, 803), (803, 812), (812, 824), (824, 832)]","Based on the fndings from our formative study, we also discovered a need for an improved methodology of describing webtoons. We aimed to improve upon existing comic description protocols by focusing on the unique aspects of the webtoons, as well as focusing on the user needs reported while reading webtoons.",0.5195343137254902,0.6940388525252525,0.9124706467320262,0.7607085494949496,,
85,63,4,Paragraph,"[(832, 841), (841, 850), (850, 859), (859, 868), (868, 879), (879, 889), (889, 898), (898, 908), (908, 915)]","3.4.1 Methods. To create the guideline, four researchers individ- ually created descriptions for a single webtoon. After collecting the descriptions, we grouped common themes and strategies used and discussed which strategies were most successful in fulflling the challenges, as well as what might be potential ambiguities that might arise due to diference in comprehension. Then, we devel- oped comprehensive guidelines that detailed how and what we should be described. The structure of our guideline was followed other guidelines for image accessibility [8, 17].",0.5195343137254902,0.7737358222222221,0.9145706062091505,0.8957527414141414,,
86,64,5,Header,"[(0, 10), (10, 13)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Huh et al.",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
87,65,5,Table,"[(13, 14), (14, 16), (16, 17), (17, 22), (22, 25), (25, 26), (26, 27), (27, 28), (28, 31), (31, 35), (35, 40), (40, 44), (44, 48), (48, 51), (51, 56), (56, 59), (59, 62), (62, 65), (65, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 88)]","Genre Title (episode) Length Drawing Style Popularity (likes) URL Drama & Romance Fantasy Comedy Action Slice of life True Beauty (140, 141) The Remarried Empress (72, 73) Lore Olympus (162, 163) Familiar Feelings (11-1, 11-2) ZomCom (138, 139) Pixie and Brutus (94, 95) unOrdinary (223, 224) Hybrid (29, 30) Murrz (510, 511) Safely Endangered (672, 673) long long long medium short short long medium short short reality reality semi-abstract semi-reality semi-abstract reality semi-reality semi-reality semi-abstract",0.17562418300653596,0.11076612525252529,0.8243695947712418,0.26817625858585864,,
88,66,5,Paragraph,"[(88, 89)]",abstract,0.5768119790849674,0.25685504646464646,0.6241199947712419,0.26817625858585864,,
89,67,5,Table,"[(89, 90), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 102), (102, 103), (103, 104), (104, 105), (105, 106), (106, 107), (107, 108), (108, 109)]",6.4M 1.9M 5.2M 293.8K 724.7K 730.2K 5.1M 114.1K 773.6K 1M [80] [4] [65] [32] [18] [21] [73] [9] [49] [42],0.7106633359477126,0.13030653737373737,0.8233879790849675,0.26817625858585864,,
90,68,5,Caption,"[(109, 118)]",Table 2: 10 Webtoons used in the comments analysis,0.32664648888888914,0.27481048888888876,0.67287845751634,0.28613170101010094,Table,2.0
91,69,5,Table,"[(118, 119), (119, 120), (120, 121), (121, 123), (123, 125), (125, 131), (131, 136), (136, 137), (137, 141), (141, 148), (148, 154), (154, 157), (157, 158), (158, 163), (163, 170), (170, 171), (171, 175), (175, 180)]","Category Sub-category Defnition Example Comment Visual description Visually describe a specifc part Description ""John’s power towards the end (12.7%) (e.g.. appearance, color, shape) with all of the black tentacles (42.2%) and sickly colors is like the physical representation of trauma"" Conceptual description Conceptually describe a ""I can see that Brutus is nervous."" (15%) specifc part (e.g., personality or emotion of a character)",0.1776784620915033,0.31048009873737387,0.817844305882353,0.4275189088383837,,
92,70,5,Paragraph,"[(180, 188), (188, 193), (193, 195), (195, 201), (201, 204)]","Description of readers’ Describe what readers felt with ""Her glowing eyes sent chills emotion (14.5%) a specifc part of the content down my spine!”",0.28694345915032676,0.4299306313131312,0.7920221441176472,0.45398296666666654,,
93,71,5,Table,"[(204, 206)]",Question (4.9%),0.2869294633986928,0.4568489148989897,0.375606545751634,0.4676638138888887,,
94,72,5,Paragraph,"[(206, 210), (210, 216), (216, 218), (218, 230)]","Ask for clarifcation for ""Wait, did the knife hit him misunderstood part or did he step on it or something?” Non-description Opinion on upcoming",0.17807042483660132,0.4568489148989897,0.8205124614379086,0.494095498989899,,
95,73,5,Table,"[(230, 235), (235, 240), (240, 241), (241, 243), (243, 249), (249, 256), (256, 257), (257, 261), (261, 263), (263, 268), (268, 272), (272, 274), (274, 276), (276, 280), (280, 285), (285, 290), (290, 296), (296, 304), (304, 307), (307, 313), (313, 320), (320, 321), (321, 327), (327, 329), (329, 332), (332, 339), (339, 343), (343, 344), (344, 347)]","Speculate about or wish for “Once they know why Navier (57.8%) plot (7.9%) what will happen next in the said yes, I bet Kosair will be story 100% behind Heinrey succeeding” Communication (8.9%) Communicate to the creator or “Happy Early Birthday Author” other readers Self-expression (7.6%) Share one’s experience or “I’m hoping me seeing this opinion outside of the story means I will get the interview for the city job I just applied to!” Assessment of content Assess the quality of the work “Can we take a moment to talk (4.3%) about how amazing the plot is?"" Others (24.2%) Inconclusive comments with ""Call the FBI. The rice has gone minimum context (e.g., puns, rogue"" emoji stufngs, etc)",0.17765055228758164,0.4832806,0.8190058709150326,0.6526851777777776,,
96,74,5,Caption,"[(347, 355)]",Table 3: Taxonomy of descriptiveness with example comments.,0.2904918300653595,0.6568760585858586,0.7090263869281046,0.6681972707070708,Table,3.0
97,75,5,Paragraph,"[(355, 363), (363, 374), (374, 388), (388, 400), (400, 413), (413, 423), (423, 434), (434, 445), (445, 456), (456, 465), (465, 475), (475, 484)]","3.4.2 Contents of Guidelines. The guideline (in supplementary material) consists of three main themes: 1) how to describe metadata not directly related to the plot, such as title. 2) how to subdivide a webtoon into, and write descriptions for, scenes and phasels , and 3) writing styles such as how to express speech and level of detail in image descriptions. In the second theme in particular, we borrow and adapt Wang et al.’s concept of phasels to ensure non-redundant descriptions of elements that span multiple panels [76, 77]. In this paradigm, panels could be combined into a single phasel (Figure 2), reducing the need for redundant explanations and user interactions (C1). In later sections, we describe how descriptions following our guidelines are used as input data for our system.",0.08736437908496732,0.7075918828282828,0.4820723856209149,0.8711201656565656,,
98,76,5,Section,"(484, 488)",4 WEBTOON COMMENTS ANALYSIS,0.5195343137254902,0.7056829571969696,0.834596964869281,0.7194570733585859,,
99,77,5,Paragraph,"[(488, 497), (497, 508), (508, 519), (519, 529), (529, 539), (539, 548), (548, 558), (558, 567)]","From the formative study, we discovered that participants often refer to comments to infer webtoons’ visual content, yet have dif- culty in utilizing them. To further explore the potential of webtoon comments as the meta data for accessibility, we conducted com- ments analysis. This section presents our approach to explore the feasibility and limitations of using webtoon comments for eliciting useful information for BLV readers. In particular, we analyzed what elements the comments describe and how they are described.",0.5195343137254902,0.7352661252525252,0.9145723062091503,0.8434459232323231,,
100,78,6,Header,"[(0, 8), (8, 18)]","Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
101,79,6,Section,"(18, 20)",4.1 Methods,0.08790522875816993,0.10765139154040401,0.19811922434640528,0.12142550770202028,,
102,80,6,Paragraph,"[(20, 30), (30, 40), (40, 52), (52, 64), (64, 75), (75, 88), (88, 98), (98, 107), (107, 119), (119, 128), (128, 138), (138, 140)]","4.1.1 Data collection. First, we crawled 1K webtoon comments (Ta- ble 2) such that the dataset would contain diverse representations of genre, length, popularity (by the number of likes ), and draw- ing style – from realistic to abstract in McCloud’s pyramid of the comic universe [41]. We sampled 20 webtoon episodes (5 genres × 2 webtoons × 2 episodes) from Naver 7 , which operates the world’s largest webtoon platform. For each webtoon, we chose their two latest episodes and collected 50 most-liked comments to simulate the default comment order set by the platform (20 episodes × 50 comments per episode). By only collecting top-level (no threaded) comments, we ensured that each comment is not dependent to another comment.",0.08790522875816993,0.1268557717171717,0.4829514205882353,0.2903840545454545,,
103,80,6,Paragraph,"[(140, 149), (149, 160), (160, 168), (168, 178), (178, 188), (188, 197), (197, 206), (206, 216), (216, 226), (226, 235), (235, 240)]","4.1.2 Analysis. We followed the thematic analysis methods [24]. As a frst step, the lead researcher open-coded 100 comments and extracted initial themes. Then, two other researchers reviewed the same data, and discussed further for confict resolution and validation. All researchers read all webtoons before analysis to reach an agreement on the interpretation. With another 100 comments, all three researchers iteratively developed the themes. Based on the themes, we derived a taxonomy for determining whether a comment is a Description or a Non-description (Table 3). Additionally, we developed a taxonomy for categorizing the Description Target and Description Range (Table 4).",0.08736437908496732,0.30175223636363635,0.48272128888888893,0.4514660404040404,,
104,81,6,Section,"(240, 242)",4.2 Results,0.08790522875816993,0.4668774016414141,0.18794096274509806,0.48065151780303034,,
105,82,6,Paragraph,"[(242, 251), (251, 261), (261, 272), (272, 282), (282, 290), (290, 301), (301, 311), (311, 320), (320, 329), (329, 337), (337, 342)]","4.2.1 What makes a comment descriptive or non-descriptive? Re- fecting the information needs of BLV readers in webtoon read- ing identifed from our formative study, we set a broader range of ‘descriptiveness’ than a mere visual description. We added the conceptual description and description of readers’ emotion, which are not directly related to visual attributes of webtoons. Still, as Wang et al. [75] noted, these high-level attributes inferred from the visuals can lead to meaningful interpretations. Comments were categorized as Non-description (Table 3) when they included self- expression, communication, assessment of the content, or opinions on upcoming plots, or questions.",0.08720286290849678,0.48608304444444445,0.48294865931372544,0.6357742060606061,,
106,82,6,Paragraph,"[(342, 354), (354, 363), (363, 372), (372, 380), (380, 389), (389, 399), (399, 409), (409, 419), (419, 428), (428, 445), (445, 456), (456, 467), (467, 479), (479, 493), (493, 507), (507, 515)]","4.2.2 What do comments describe? As depicted in Table 4, we found that people describe characters more often than other elements of webtoons. For description range, most comments (46.1%) were describing a single panel. Comments categorized as multi-panel descriptions either referred to continuous panels with a single event, or contained several sentences each of which referred to a diferent panel. We found that descriptive comments were not evenly distributed in all webtoon elements. For example, only few characters were mentioned frequently, and only a small number of panels (min = 2 out of 4, max = 8 out of 46) were referred to in comments. While we did not analyze the intent behind each description, we speculated that people often describe a part of the story to deliver how impressive it was (e.g., Her dress was gorgeous, especially when it was fying! ), or to call attention to a notable part that other readers are likely to miss (e.g., The color of his eyes slightly changed during the dance! Did anyone notice? ).",0.08790522875816985,0.6471411252525252,0.48206684460784316,0.8660405353535353,,
107,83,6,Footnote,"[(515, 517)]",7 https://www.webtoons.com/en/,0.08769934640522875,0.8844750296717171,0.23980997647058822,0.8951665345959596,,
108,84,6,Table,"[(517, 518), (518, 519), (519, 522), (522, 524), (524, 526), (526, 529), (529, 531), (531, 532), (532, 533), (533, 535), (535, 537), (537, 539), (539, 541), (541, 544), (544, 546), (546, 548), (548, 550), (550, 553), (553, 556)]",Category Sub-category e Description Rang Single-panel (46.1%) Multi-panel (28.6%) Entire episode (7.3%) Inconclusive (18.0%) Category Sub-category Description Targe Character (55.7%) Object (7.4%) Dialogue (10.8%) Event (19.3%) t Setting (2.7%) Others (1.6%) Inconclusive (2.5%) (a) Target specifcity (b) Range specifcity,0.5895735294117647,0.11076612525252529,0.8420539320261438,0.35366275656565654,,
109,85,6,Caption,"[(556, 562)]",Table 4: Taxonomy of description specifcity,0.5689596718954248,0.3574327202020202,0.8621696235294117,0.36875393232323234,Table,6.0
110,86,6,Paragraph,"[(562, 573), (573, 583), (583, 590), (590, 600), (600, 609), (609, 620), (620, 631), (631, 641), (641, 649), (649, 660), (660, 670), (670, 681), (681, 689), (689, 700), (700, 709)]","4.2.3 What makes it dificult to utilize comments? Despite the abun- dance of descriptive comments, there exist barriers to fully utilize comments. First, comments are inherently unstructured. Sentences that are incomplete or conveying meanings with emojis have the risk of misunderstanding [71]. Second, echoing the C4 distilled from the formative study, it is difcult to locate descriptive sen- tences as the majority of comments are non-descriptive, or lack a clear reference target. With no fltering feature supported by the platform, BLV readers go through many non-descriptive sentences to fnd a useful comment. Third, the presentation order of com- ments is not helpful for utilizing comments as descriptions. Most platforms only provide the sorting of comments by the number of likes or the time uploaded. Consequently, descriptive comments are not presented as the story order and users cannot selectively read comments relevant to the panel of their interest.",0.5195343137254902,0.4145388525252525,0.914567524836601,0.6195772363636364,,
111,86,6,Paragraph,"[(709, 719), (719, 729), (729, 733)]","While webtoon comments have the potential to add rich descrip- tions, we believe that efcient fltering and sorting would support better utilization of comments.",0.5195343137254902,0.6220931454545454,0.9145779379084968,0.6610885999999999,,
112,87,6,Section,"(733, 735)",5 COCOMIX,0.5195343137254902,0.6761829571969695,0.6375379771241831,0.6899570733585858,,
113,88,6,Paragraph,"[(735, 746), (746, 754), (754, 762), (762, 772), (772, 782), (782, 791), (791, 801), (801, 808)]","With four design goals (Section 3.3) in mind, we present Cocomix, a prototype webtoon reader supporting efcient and interactive webtoon reading for BLV users through comments-driven interac- tion techniques. Below, we walk through a scenario illustrating how BLV readers can use Cocomix for reading webtoons (Section 5.1) and subsequently describe how the system features are enabled using comments (Section 5.2-5.3). We then also describe the com- putational pipeline that powers Cocomix (Section 5.4).",0.5188316993464052,0.6953886,0.9145750400326796,0.803568397979798,,
114,89,6,Section,"(808, 811)",5.1 User Scenarios,0.5195343137254902,0.8186627551767677,0.67812624624183,0.8324368713383838,,
115,90,6,Paragraph,"[(811, 823), (823, 834), (834, 846)]","5.1.1 Scenario 1. Sean is a college student with low vision who uses a screen reader to access web contents. Sean’s friends often talk about how they enjoyed the new episode of a popular webtoon,",0.51953431372549,0.8567585494949496,0.9137046671568626,0.8957527414141414,,
116,91,7,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.3363069709150327,0.0871209477272727,,
117,92,7,Paragraph,"[(10, 13)]",Huh et al.,0.8658485718954247,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
118,93,7,Figure,"[(13, 19), (19, 26), (26, 32), (32, 39), (39, 41), (41, 43), (43, 51), (51, 53), (53, 55), (55, 57), (57, 59), (59, 64), (64, 70), (70, 71), (71, 73), (73, 74), (74, 75)]",Leto’s eyes are glowing bright yellow. Smoke is blooming in front of her. Leto’s eyes are glowing bright yellow. Smoke is blooming in front of her. Double tap Swipe up “Her glowing eyes sent chills down my spine!” Abridged Description Selective Details Panel-anchored Comments Reader Comments “I want more details here” “What did others think about this?” A Swipe up B C,0.17547833841993463,0.12468292840909091,0.8176369901143792,0.31831097592045454,,
119,94,7,Caption,"[(75, 92), (92, 112), (112, 131), (131, 138)]","Figure 3: Readers can have personalized control over description using Cocomix. (A) For non-key panels, abridged description is provided as default. (B) When readers want to uncover more details, they can double-tap to listen to additional description. (C) When readers are curious about descriptive comments relevant to the current panel, they can swipe-up to access panel- anchored comments. Image from [WEBTOON SERIES] (2022).",0.08745104836601318,0.386269997979798,0.9147186562091505,0.43909477373737377,Figure,4.0
120,95,7,Paragraph,"[(138, 149), (149, 159), (159, 172), (172, 182), (182, 191), (191, 200), (200, 213), (213, 225), (225, 237), (237, 249), (249, 262), (262, 273), (273, 282), (282, 293), (293, 304), (304, 313), (313, 324), (324, 337), (337, 348), (348, 354)]","“Lore Olympus”, so he is interested in reading the webtoon. He wants to enjoy webtoons while commuting to school by subway, and he decides to use Cocomix, a mobile webtoon reader to read the webtoon. Unlike other audio comics that he has experienced before where the description was played continuously, Sean can navigate between panels by swiping the screen left-to-right. Because the gestures are similar to those of the screen reader he uses, he easily gets used to the system. As Sean progresses into a diferent scene, the system informs him of the transition of time and setting: “Night- time, at the kitchen”. In the story, the character Persephone is doing the dishes. As he moves on to the next panel, the system description tells him that Persephone has looked up from the dishes, looking confused, at her visitor. The visual diference between neighboring panels is clearly described, so he has no difculty in connecting the panels to understand the whole story. Sean notices that some panels have shorter descriptions than others especially when the scene is focused on the dialogue, or when only walk-on characters appear in the panel. Because he wants to fnish the episode in 10 minutes before arriving at school, he just listens to the abridged version of the description (Figure 3.A).",0.08625,0.46340248888888885,0.48293706258169933,0.7376277414141413,,
121,95,7,Paragraph,"[(354, 365), (365, 375), (375, 388), (388, 399), (399, 409), (409, 419), (419, 430), (430, 442), (442, 452), (452, 462)]","5.1.2 Scenario 2. After taking all classes, Sean comes home and decides to continue reading the next episode while relaxing. This time, he wants to slow down the pace of reading and peek into more details to picture the scene clearly in mind. While listening to the default description, Sean realizes that the main characters, Persephone and Hades, are described in more detail than other minor characters such as nymphs. This time he is curious about how the nymphs look so he double-taps the panel (Figure 3.B) where nymphs appear to request more details. From the audio feedback, Sean notices that some comments are anchored to specifc panels.",0.08790522875816989,0.7598987010101009,0.48271947189542475,0.8957527414141414,,
122,95,7,Paragraph,"[(462, 474), (474, 484), (484, 494), (494, 503), (503, 513), (513, 523), (523, 531)]","While most comments are shown at the end of the episode, these anchored comments can be read while following the main story. When reading the scene where Persephone dances, he checks other readers’ comments on the scene. By reading the panel-anchored comments (Figure 3.C), he could access extra visual information that was omitted in the description (e.g., “Some nymphs were holding hands while watching the couple dance. How cute!”).",0.5188316993464052,0.46340248888888885,0.9143535849673202,0.5577464282828283,,
123,96,7,Section,"(531, 537)",5.2 Adaptive Description with Selective Details,0.5195343137254902,0.5820680582070706,0.908465119117647,0.5958421743686869,,
124,97,7,Paragraph,"[(537, 547), (547, 556), (556, 565), (565, 577), (577, 586), (586, 599), (599, 610), (610, 621), (621, 631), (631, 643), (643, 654), (654, 664), (664, 676), (676, 683)]","To address D2 and D3, Cocomix supports an adaptive description method based on the description generated through our guide- line 3.4. The adaptive description provides an abridged description for panels that were mentioned in fewer comments, and a full de- scription for panels mentioned in many comments (key panels). This is so that readers would not have to spend excessive time listen- ing to details in less important panels. Through this approach, the average description length is reduced and readers can have less cog- nitive load while reading webtoons. Details on the abridged panels omitted by default can still be accessed on demand, which we call as selective details [48]. When reading with Cocomix, users are pro- vided with audio feedback which informs them whether the panel has extra details to disclose or not, as to minimize the frustration when users face unavailable details or unknown-unknowns.",0.5189918300653594,0.597814105050505,0.9145738390522875,0.7890166303030304,,
125,98,7,Section,"(683, 686)",5.3 Panel-anchored Comments,0.5195343137254902,0.8133382602272727,0.7799979888888888,0.8271123763888888,,
126,99,7,Paragraph,"[(686, 695), (695, 705), (705, 714), (714, 724), (724, 736)]","To address D3 and D4, Cocomix provides panel-anchored com- ments where readers can refer to descriptive comments relevant to the current panel during reading. Cocomix frst extracts comments that are descriptive or helpful for understanding the story then link them to the relevant panels. Comments that are not linked to",0.5195343137254902,0.829084307070707,0.9145625599673204,0.8957527414141414,,
127,100,8,Header,"[(0, 8), (8, 18)]","Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
128,101,8,Paragraph,"[(18, 31), (31, 35)]",specifc parts of the story yet still are descriptive are shown at the end of the episode.,0.08790522875816993,0.10956031717171716,0.48046782843137253,0.13471865050505044,,
129,102,8,Section,"(35, 38)",5.4 Computational Pipeline,0.08790522875816993,0.14949735113636367,0.3226647826797385,0.16327146729797995,,
130,103,8,Paragraph,"[(38, 47), (47, 56), (56, 65), (65, 74), (74, 81)]","The computational pipeline that powers Cocomix has three main components: 1) scoring and abridging descriptions, 2) extraction of descriptive comments, and 3) linking comments to relevant panels (Figure 4). The frst component enables adaptive descriptions while the other two components implement panel-anchored comments.",0.0874656862745098,0.16524339797979795,0.4804730411764706,0.23191309494949497,,
131,103,8,Paragraph,"[(81, 91), (91, 101), (101, 110), (110, 119), (119, 136), (136, 149)]","5.4.1 Data Collection and Preprocessing. There are two main data sources for our pipeline: 1) webtoon descriptions and 2) webtoon comments. After we collect top-level comments we discard ones that are 3-words-long or shorter. Then, we penalize low-quality comments using like − score = number _ of _ likes − 2 ∗ number _ of _ dislikes and dropping comments with a like − score less than 10.",0.08719585359477122,0.24262597373737377,0.48129257516339874,0.3231402505050504,,
132,103,8,Paragraph,"[(149, 158), (158, 167), (167, 176), (176, 188), (188, 202), (202, 210), (210, 220), (220, 230), (230, 239), (239, 246), (246, 254), (254, 264), (264, 273), (273, 283), (283, 293), (293, 298)]","Because comments are inherently noisy and informal, we go through several preprocessing steps prior to applying Natural Lan- guage Processing (NLP) techniques. First, we remove emojis and emoticons as often the translated text does not align with the intent of usage. Next, we use Bing Spell Check API 8 for correction of mis- spelling and informal languages. Refecting the observation from the comments analysis, we apply fltering and scoring techniques in sentence-level instead of comment-level, as often the sentences in a single multi-sentence comment refer to diferent panels. To improve the sentence-tokenization accuracy, we correct punctuation errors and casing errors by running transformer-based punctuator [43] and truecaser [68]. We run a language-detection algorithm to drop non-English comments [43] limiting our scope to producing out- puts in English. Finally, we run co-reference resolution [82] before tokenizing each comment into sentences to recover from the loss of named entities when split.",0.08790522875816993,0.3256474383838383,0.48294441013071887,0.544524206060606,,
133,103,8,Paragraph,"[(298, 308), (308, 319), (319, 329), (329, 337), (337, 346), (346, 358), (358, 371), (371, 383), (383, 394), (394, 403), (403, 413), (413, 423), (423, 435), (435, 445), (445, 458), (458, 470), (470, 494), (494, 495)]","5.4.2 Scoring and abridging descriptions. We now describe a method to detect key panels of webtoons. Ideally, key panels should capture the visual, conversational, or plot highlights, or have a signifcant role in understanding neighboring panels. However, current com- puter vision techniques are not advanced enough to automatically detect key panels in webtoons due to their high variety of visual styles and the scarcity of labeled data [11]. Thus, we rely on com- ments data to detect “key panels” based on our insight that an important panel is likely to be mentioned more frequently in the comments. Our pipeline frst runs a part-of-speech tagger [28] to populate proper nouns, nouns, and verbs which correspond to characters, objects, and events with an intuition that people are likely to refer to those in comments. This refects the fndings from our comments analysis and the prior research in cognitive psy- chology [83]. We derive the focus score of a sentence S i using scores of all pre-populated words, as we represent each sentence as  S i = w 1 , w 2 , ..., w k . We defne a sentence’s focus score Foc ( S i ) as:",0.08746992156862744,0.5470401151515151,0.4858517379084968,0.7935872626262627,,
134,104,8,Equation,"[(495, 503), (503, 504), (504, 514), (514, 524), (524, 544), (544, 564)]",Õ 1 Foc ( S i ) = · CF ( w k )/ PF ( w k ) (1) | S i | w k ∈ S i CF ( w k ) = ( # of comments that has w k )/( total # of comments ) PF ( w k ) = ( # of panels that has w k )/( total # of panels ),0.09659640522875816,0.7930778080808081,0.48047053333333334,0.8695053868686868,,
135,105,8,Paragraph,"[(564, 566)]",8 https://www.microsoft.com/en-us/bing/apis/bing-spell-check-api,0.08790522875816993,0.8844750296717171,0.39500058725490206,0.8951665345959596,,
136,105,8,Paragraph,"[(566, 583), (583, 593), (593, 606), (606, 618), (618, 631), (631, 643), (643, 652), (652, 663), (663, 673), (673, 685), (685, 695), (695, 705), (705, 716), (716, 727), (727, 728)]","where CF ( w i ) denotes Comment Frequency and DF ( w i ) denotes Panel Frequency . Similar to document frequency, both represent the num- ber of comments or panels containing word w i . Here we penalize with panel frequency of a word to avoid naively giving high scores to frequently appearing elements. In the equation, | S i | is the length of sentence S i in number of words including stopwords. A panel’s focus score is determined by summing all sub-descriptions in the panel. We observed that the score computed is dependant on the type of webtoon, or other external factors—for example, when the published date was around the time of the content cre- ator’s birthday, most comments were for creators. Thus, instead of setting a threshold score to determine key panels, we empirically set top 30% scored panels as key panels with which researchers could perceive the efect of abridging while still retaining the key information.",0.5189918300653594,0.10864329898989904,0.91918587124183,0.314598701010101,,
137,105,8,Paragraph,"[(728, 738), (738, 752), (752, 761), (761, 773), (773, 781), (781, 790), (790, 801), (801, 809), (809, 819), (819, 824)]","For abridging the remaining panels, each of the panel descrip- tions is shortened to 30 50% of the original length. We do not run the abridging technique with dialogues (texts wrapped in speech bubbles) as a gap in the conversation can be critical in following the story. We apply extractive summarization using fne-tuned BERT [39]. Compared to abstract summarization which is similar to the process of paraphrasing and more prone to factual inconsis- tency [50], extractive summarization keeps the original structure. Sentences not included in the resulting summary can be accessed on demand (with a double-tap).",0.5195343137254902,0.3171146101010101,0.9145722019607843,0.4529699131313132,,
138,105,8,Paragraph,"[(824, 832), (832, 842), (842, 854), (854, 868), (868, 878), (878, 889), (889, 899), (899, 909), (909, 919), (919, 929), (929, 940), (940, 949), (949, 962), (962, 973), (973, 981), (981, 990), (990, 1001), (1001, 1012), (1012, 1027), (1027, 1039), (1039, 1050), (1050, 1064), (1064, 1074), (1074, 1077)]","5.4.3 Extraction of descriptive comments. Refecting the fndings from the formative study and comments analysis, we extract com- ments that meet all of the following criteria: 1) have clear reference to an element of the webtoon, 2) be a form of statement, and 3) describe what happened rather than the upcoming plot (Figure 5). First, we run a part-of-speech tagger [28] to the webtoon description to pre-populate proper-nouns, nouns, and verbs ( webtoon keywords ) which correspond to characters, objects and actions. Then, only the sentences from comments data with exact word-level match with at least one of webtoon keywords are extracted. Also, comments that quote a dialogue with a continuous overlap with more than three words are extracted. The intuition is that descriptive comments would have a clear reference to an element of the story. Then we run a dialogue act classifer [57] which outputs tags that summarize syntactic, semantic, and pragmatic information. We only extract sentences with the tags ‘Statement-non-opinion’ (e.g., Leto was hid- ing in the middle of the crowd! or ‘Statement-opinion’ (e.g., Thanatos’ reaction was priceless, sweating bullets ) among 43 diferent tags. As a fnal step, we check the tense of a root word of a sentence and only keep sentences in past or present tense. This flters out com- ments with guesses (e.g., Persephone will soon catch Leto’s arms! or hopes (e.g., I will be so happy when they eventually get married! ) for upcoming stories as they are irrelevant to understanding the plot of the webtoon.",0.5189872861111111,0.4624037515151515,0.914569647875817,0.7919762262626262,,
139,105,8,Paragraph,"[(1077, 1088), (1088, 1098), (1098, 1110), (1110, 1122), (1122, 1132), (1132, 1141), (1141, 1151)]","5.4.4 Linking comments to relevant panels. In Section 4, we ob- served that most descriptive comments consist of two parts: 1) making reference to a part of the story (e.g., When Persephone was dancing [...] ) and 2) adding extra description (e.g., [...] she looked so happy. ). Thus we assumed that descriptive comments would have high similarity with the relevant panel’s description espe- cially because of the frst part. Semantic similarity between them",0.5195343137254901,0.8014100646464647,0.9145628003267974,0.8957527414141414,,
140,106,9,Header,"[(0, 10), (10, 13)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Huh et al.",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
141,107,9,Figure,"[(13, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 23), (23, 25), (25, 26), (26, 27), (27, 29), (29, 32), (32, 33)]",Calculate Comment-focus Score Descriptive Comments Panel-anchored Comments 3-stage Extraction Comments-based Adaptive Description Extractive Summarization Similarity-based Linking Comments Webtoon Description Ordering,0.2949290764836601,0.14989294853535357,0.7212700918169934,0.3989551490808081,,
142,108,9,Caption,"[(33, 50), (50, 69), (69, 85), (85, 101), (101, 119), (119, 132), (132, 142), (142, 152), (152, 162), (162, 174), (174, 185), (185, 196), (196, 208), (208, 216), (216, 227), (227, 238), (238, 251), (251, 262), (262, 264)]","Figure 4: Pipeline overview of Cocomix. It takes two inputs: webtoon description and comments. For comments-based adaptive description, our pipeline frst calculates the comment-focus score to detect key panels. Then, to abridge the total length of description, we apply extractive summarization to non-key panels. The resulting description has diferent amount of details for each panel depending on its comment-focus score. For panel-anchored comments, our system frst extracts descriptive comments via 3-stage extraction. Then, extracted comments are linked to relevant panels based on the similarity score and named entity match. Finally, comments are presented in an order that ensures non-redundancy. was computed using the sentence embeddings method of Arora et al. [7]. Here we use sentence-level linking instead of paragraph- level (entire comment or panel description) to keep the accuracy from dropping when there are multiple topics in a comment or a panel. Also, we check for named entities in the matched comment and drop if one not covered in the corresponding panel descrip- tion appears. While the comment can be still relevant, we take the precision-frst approach and flter out potential false-positives. For the decision of linking a comment, we check its similarity score distribution with all panels. The comment is linked only when the score is above a threshold in a single panel or neighboring panels. In this paper, we empirically set the threshold to produce an accuracy of 85%.",0.08736437908496732,0.4322106545454546,0.9120961307189543,0.714382791919192,Figure,3.0
143,109,9,Paragraph,"[(264, 272), (272, 283), (283, 294), (294, 306), (306, 318), (318, 329), (329, 338), (338, 348), (348, 360), (360, 370), (370, 379)]","5.4.5 Presenting comments. Readers can read relevant comments anchored at the end of each panel description. When the comment is anchored to multiple neighboring panels, we provide it in the latest panel to avoid giving a spoiler. Also, when there are multi- ple comments anchored in a same panel, we order them so that neighboring comments have low similarity score. This is to avoid re- curring of similar comments, refecting the formative study fndings. Comments extracted as descriptive but not linked are presented at the end of the episode. The presentation order is determined by the number of descriptive sentences in the comment and likes. While removed for the text analysis, we deliver emoji-descriptions while",0.08790522875816993,0.7460615797979798,0.48293706258169944,0.8957514040404041,,
144,109,9,Paragraph,"[(379, 389), (389, 400), (400, 407)]","reading comments for rich delivery. Still, we follow the recommen- dations from Tigwell et al. [71] by removing repeated emojis and skipping comments with emojis in the middle.",0.5195343137254902,0.5370173878787878,0.9145753362745099,0.5760128424242424,,
145,110,9,Section,"(407, 409)",6 EVALUATION,0.5195343137254902,0.5946602299242424,0.6597661596405229,0.6084343460858586,,
146,111,9,Paragraph,"[(409, 419), (419, 429), (429, 436), (436, 437)]","To demonstrate the feasibility of our pipeline, we conducted pipeline evaluation study. Then, we conducted user evaluation study to eval- uate the efectiveness of Cocomix’s comments-driven interaction techniques.",0.51909477124183,0.6138658727272728,0.9145717032679739,0.6666984484848484,,
147,112,9,Section,"(437, 440)",6.1 Pipeline Evaluation,0.5195343137254902,0.6853458359848484,0.7193027511437908,0.6991199521464647,,
148,113,9,Paragraph,"[(440, 450), (450, 458)]","To evaluate the efectiveness of our comments fltering and linking techniques, we collected subjective ratings on Cocomix’s output",0.51909477124183,0.7045514787878788,0.912097751633987,0.7297098121212121,,
149,113,9,Paragraph,"[(458, 464), (464, 474), (474, 483), (483, 493), (493, 504), (504, 513), (513, 523), (523, 531), (531, 540), (540, 551), (551, 561), (561, 569)]","comments from 10 sighted webtoon readers. 6.1.1 Materials. We selected fve diferent webtoons that span dif- ferent attributes (length, number of comments, genre, and drawing style). To acquire descriptions for the selected webtoons to power the system, we recruited 10 people to describe the webtoons by following our guidelines (Section 3.4). Each webtoon was randomly assigned by two describers to ensure task completion, but they worked independently. We recruited external describers with no prior experience in description tasks to ensure the generalizibility of our pipeline. For episodic webtoons we selected the frst episode to reduce the dependency on background context. For system input, one description was randomly selected in each webtoon.",0.5189963313725492,0.7322257212121212,0.9145661475490198,0.8957527414141414,,
150,114,10,Header,"[(0, 8), (8, 18)]","Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
151,115,10,Figure,"[(18, 24), (24, 35), (35, 40), (40, 45), (45, 46), (46, 57), (57, 58), (58, 63), (63, 67)]","Check Verb Tense from POS Tags “The nymphs are holding hands when watching the dance! So cute!” “This is hilarious 😂 ” “What is Hades wearing? ” Pre-processing “Leto will eventually be caught by Hades, I can see ” Comments Check Reference to Pre-populated Keywords Check Dialogue Act Tags",0.3031315676633987,0.11334108234848488,0.7276173317401962,0.3843578574810606,,
152,116,10,Caption,"[(67, 84), (84, 101), (101, 120), (120, 136), (136, 157), (157, 169)]","Figure 5: Descriptive comments are extracted through three stages of extraction. Prior to fltering, comments are pre- processed to auto-correct noisy and informal languages. Then, our pipeline frst checks for reference to pre-populated webtoon keywords in each comment. This flters out comments with only non-clear reference. Second, it checks for dialogue act tags and leave out all comments except those with ‘Statement-non-opinion” or ‘Statement-opinion” tags. In this stage, comments in a question or response form are removed. Finally, the pipeline checks for the tense of the root word from Part-Of-Speech tags. This removes comments expressing wish or guess on the upcoming plot.",0.08790522875816985,0.4083293414141414,0.9147186562091504,0.4888231595959595,Figure,3.0
153,117,10,Paragraph,"[(169, 179), (179, 187), (187, 197), (197, 207), (207, 216), (216, 226), (226, 236), (236, 245), (245, 253), (253, 262), (262, 273)]","6.1.2 Procedure. We ran our pipeline with the fve descriptions and comments, and distributed evaluation questionnaires to 10 evaluators (none of them participated in the description task). Here we recruited sighted evaluators to measure how well the output comments are in compliance with extraction criteria and liking criteria based on original visual content instead of descriptions. We also report BLV readers’ evaluation of the comments (Section 6.2). To evaluate our extraction of descriptive comment, we randomly selected three from the pipeline-extracted comments and three from the 50 most-liked comments. For each comment, evaluators were asked to report how much they agree about the following:",0.08736437908496732,0.5131360747474747,0.4827187336601307,0.6628272363636364,,
154,118,10,List,"[(273, 285), (285, 296), (296, 307), (307, 318), (318, 328), (328, 337)]","1) The comment is related to the story, 2) The comment includes objective descriptions of (a part of) the story, 3) The comment includes subjective descriptions of (a part of) the story, 4) The comment refers to specifc elements of the content (e.g., a scene, character, object, setting, mood). To evaluate our linking to relevant panels, we randomly selected one of our panel-anchored comments.",0.08757359444444449,0.6653405454545455,0.4827216924836601,0.7458473636363636,,
155,119,10,Paragraph,"[(337, 346), (346, 357)]",Evaluators were presented with the panel-comment pair and asked to answer how much they agree with the following statements: 1),0.08790522875816989,0.7483606727272728,0.4813426562091503,0.7735242060606061,,
156,120,10,List,"[(357, 369), (369, 371)]",The comment is relevant to the panel; 2) The comment is targeting the panel.,0.0874656862745098,0.7760388525252525,0.48046794117647057,0.8011971858585859,,
157,121,10,Paragraph,"[(371, 384), (384, 392), (392, 404), (404, 414), (414, 423), (423, 435)]","6.1.3 Results. The results can be seen in the Table . While Cocomix- picked comments received higher scores than most-liked comments in every criteria, note that the scores were comparable for the cov- erage of subjective information (Table 5). Also, results about linking show that the accuracy of panel-anchored comments is acceptable (82% of the outputs were rated above 4 out of 7). Cocomix-picked",0.0874656862745098,0.8152471858585858,0.4829425349673203,0.8957527414141414,,
158,121,10,Paragraph,"[(435, 443), (443, 454)]",comments received low scores when they covered information about multiple panels but were matched to only one of them.,0.5195343137254902,0.5131360747474747,0.912097047875817,0.5382944080808081,,
159,122,10,Section,"(454, 457)",6.2 User Evaluation,0.5195343137254902,0.5531816945707071,0.6891957741830065,0.5669558107323233,,
160,123,10,Paragraph,"[(457, 465), (465, 474), (474, 484), (484, 491), (491, 494)]","To assess the feasibility of using comments-driven interaction techniques to enhance webtoon experience for BLV readers, we conducted a user study comparing the webtoon readers with and without our comments-driven interaction techniques. We explored three research questions:",0.5189918300653594,0.5689277414141414,0.9120991748366012,0.6355961757575758,,
161,124,10,List,"[(494, 504), (504, 509), (509, 519), (519, 521), (521, 530)]",(1) RQ1. How do BLV readers use Cocomix’s interactive features while reading the webtoon description? (2) RQ2. How do BLV readers perceive comments data presented in Cocomix? (3) RQ3. How usable and useful are Cocomix’s features?,0.5373539241830065,0.6418681272727272,0.9120940151960785,0.708535494949495,,
162,125,10,Paragraph,"[(530, 538), (538, 547), (547, 558), (558, 567), (567, 578), (578, 583)]","6.2.1 Interfaces. The baseline interface stripped all comments- driven features from Cocomix, but still supported navigation be- tween panels and unfltered comments provided at the end of the episode. Both the system and the baseline provided description that followed our guideline as there is no established guidelines for webtoon description to compare with.",0.5189918300653594,0.719358296969697,0.9145777442810457,0.7998638525252526,,
163,125,10,Paragraph,"[(583, 594), (594, 605), (605, 616), (616, 627), (627, 640)]","Gesture logs were collected using Google Firebase 9 . While our prototype works without an aid of a screen reader, we provide sufcient audio feedback like using a screen reader (e.g., when the user makes double taps, it gives a short “click-click” sound efect). In our study, we did not display images in order to explore how",0.5195343137254902,0.7999101503787879,0.9143485568627452,0.8690494585858586,,
164,126,10,Footnote,"[(640, 642)]",9 https://frebase.google.com/,0.5195343137254902,0.8844750296717171,0.653389996732026,0.8951665345959596,,
165,127,11,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.3363069709150327,0.0871209477272727,,
166,128,11,Paragraph,"[(10, 13)]",Huh et al.,0.8658485718954247,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
167,129,11,Table,"[(13, 14), (14, 15), (15, 16), (16, 19), (19, 20), (20, 21), (21, 24), (24, 25), (25, 32), (32, 43), (43, 54), (54, 62), (62, 64), (64, 66), (66, 68), (68, 70), (70, 72), (72, 74), (74, 76), (76, 78), (78, 80), (80, 88), (88, 89), (89, 91), (91, 100), (100, 101), (101, 103)]",Pipeline Component Statements Ratings for Comments Most-liked Cocomix Extraction of descriptive comments The comment is related to the story. The comment includes objective information of what happened in the story. The comment includes subjective information of what happened in the story. The comment refers to specifc part of content. 4.33 (1.78) 3.47 (1.71) 4.67 (1.92) 3.43 (1.82) 5.37 (1.45) 4.80 (1.54) 4.89 (1.45) 5.23 (1.39) Linking to The comment is relevant to the presented panel. - 5.46 (1.79) relevant panels The comment is targeting the presented panel. - 5.02 (1.89),0.13740855163398683,0.10979390303030295,0.8625588261437906,0.22511176969696922,,
168,130,11,Caption,"[(103, 123), (123, 140), (140, 155)]","Table 5: In the pipeline evaluation, we collected subjective ratings (in a 7-point Likert scale) on output comments of two pipeline components: Extraction of descriptive comments and Linking to relevant panels. The value represents the mean of scores (SD). Results show that Cocomix’s output comments outperformed most-liked comments in describing the story.",0.08740475555555545,0.2317459999999994,0.912079139869281,0.2707362545454539,Table,12.0
169,131,11,Paragraph,"[(155, 166), (166, 175), (175, 186), (186, 195)]","well the visual content can be replaced with our description and comments-driven features. However, we agree that low vision users can beneft from images displayed, and wish to explore how features of Cocomix complement the image in our future work.",0.08736437908496732,0.3101714282828283,0.48046748545751633,0.36300400404040406,,
170,131,11,Paragraph,"[(195, 206), (206, 218), (218, 229), (229, 238), (238, 250), (250, 262), (262, 272), (272, 282), (282, 288)]","6.2.2 Participants. We recruited 12 BLV people (8 male, 4 female, Table 8) using mailing lists and social media. The criteria for invi- tation were interest in webtoon, using screen reader due to visual impairment, and basic English profciency as the description was provided in English. They averaged 30.33 years old (min = 22, max = 53) and described their visual impairment as totally blind (9 par- ticipants) or low vision (3 participants). None of the participants has participated in the formative study. We compensated each par- ticipant $30 USD for their time.",0.0874656862745098,0.37383051919191923,0.48294253496732026,0.49584743838383843,,
171,131,11,Paragraph,"[(288, 298), (298, 310), (310, 320), (320, 330), (330, 341), (341, 351), (351, 361), (361, 372), (372, 380), (380, 381)]","6.2.3 Materials. We selected three webtoons from Naver (Table 6). We selected the frst webtoon (W3) with short length to use in tutorial sessions. Webtoons used in main studies were selected from most-popular ongoing series, and were similar in terms of length (the number of panels), the amount of information provided in text (e.g., dialogue and descriptions), and the number of comments, but diferent in genre and creator. Following our guidelines (Section 3.4), two authors of this paper drafted the description for each webtoon and cross-checked for major omission, grammatical errors, and typos.",0.08720261437908497,0.5066739535353535,0.4827216924836601,0.642527993939394,,
172,131,11,Paragraph,"[(381, 390), (390, 401), (401, 411), (411, 420), (420, 430), (430, 442), (442, 449)]","6.2.4 Procedure. A 90-minute study was conducted remotely us- ing Zoom 10 for safety during COVID-19. Each study was audio- recorded and transcribed 11 for further analysis. The study was approved by our institution’s IRB. After collecting informed con- sent, participants were given a tutorial of the Cocomix interface, and they were asked to go through a practice session using tutorial webtoon (W3) to familiarize with the features.",0.08736437908496732,0.6533557717171717,0.48294152124183015,0.7476984484848485,,
173,131,11,Paragraph,"[(449, 457), (457, 468), (468, 477), (477, 488), (488, 500)]","Then, a within-subjects study was conducted where participants were asked to read two webtoons. The presentation order of reader type (baseline vs. Cocomix) and webtoon type was counterbalanced. To ensure they pay attention to the content, we instructed partic- ipants that they will be asked to summarize the story after each",0.0873714148692811,0.7502143575757576,0.4829337173202614,0.8168840545454547,,
174,132,11,Footnote,"[(500, 509), (509, 519)]",condition. We provided summary of previous episodes before read- ing each episode to provide background context to the participants,0.08790522875816993,0.819398701010101,0.48293661111111114,0.8445570343434343,,
175,133,11,Paragraph,"[(519, 529)]",to minimize the efect of prior knowledge in study results.,0.08790522875816993,0.8470729434343435,0.4353971816993464,0.8583941555555555,,
176,134,11,Footnote,"[(529, 533)]",10 https://zoom.us 11 https://clovanote.naver.com,0.08769934640522875,0.8738550801767677,0.2257526133986928,0.8951665345959596,,
177,135,11,Figure,"[(533, 541), (541, 543), (543, 544), (544, 545), (545, 546), (546, 547), (547, 548), (548, 553), (553, 554), (554, 555), (555, 556), (556, 557), (557, 558), (558, 559), (559, 560), (560, 561), (561, 562), (562, 563), (563, 564), (564, 569), (569, 574)]",P e r c e n t a g e 0 25 50 75 100 Pa rt icipant ID P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 Requests for additional details (%) Requests for panel-anchored comments (%),0.5239696132810457,0.30942758020202016,0.8947415937254901,0.41566169567676764,,
178,136,11,Caption,"[(574, 583), (583, 592)]",Figure 6: The percentage of panels where participants re- quested details or comments is shown for each participant.,0.5195343137254902,0.43293792727272723,0.9147152078431373,0.4580936606060606,Figure,6.0
179,137,11,Paragraph,"[(592, 600), (600, 602)]",The usage pattern of Cocomix’s interactive features varied by participant.,0.5190508313725491,0.4606069696969696,0.9120926823529413,0.485762703030303,,
180,137,11,Paragraph,"[(602, 610), (610, 620), (620, 630), (630, 640), (640, 650), (650, 659), (659, 660)]","After each episode, participants completed a questionnaire on the usability of each interface and their experience and opinions about comments data. All gestural interactions were logged by the server for analysis. We ran semi-structured interviews at the end to gain a deeper understanding of the strengths and weaknesses of Cocomix and the participants’ strategies for using interactive features.",0.5195343137254901,0.5197370848484848,0.912097047875817,0.6140810242424243,,
181,138,11,Section,"(660, 662)",6.3 Results,0.5195343137254902,0.6299973511363636,0.6195700477124183,0.6437714672979797,,
182,139,11,Paragraph,"[(662, 672), (672, 681), (681, 694), (694, 705), (705, 717), (717, 729), (729, 742), (742, 755), (755, 767), (767, 779), (779, 791), (791, 800), (800, 811)]","6.3.1 Usage Patern of Interactive Features (RQ1). In both baseline and Cocomix conditions, all users sequentially followed the descrip- tion order instead of going back or skipping a panel. Four of the participants (P11, P15, P18, P20) checked the details most of the time (Figure 6). P11 and P15 mentioned that they usually enjoy the slow pace in reading with lots of details. P18 and P20 explained the fear of missing out led them to request details every time. P18 noted “I’m afraid of skipping any detail when I know (from the au- dio feedback) there is something. What if the system didn’t read it to me and it later turned out to be important?” Other participants (P14, P16, P19, P21) mostly skipped the details and read the default description. P19 mentioned “The information given was enough for me! I was happy with that so just quickly moved on.”",0.51909477124183,0.649202993939394,0.9145661475490195,0.8265910404040404,,
183,139,11,Paragraph,"[(811, 821), (821, 832), (832, 843), (843, 856), (856, 866)]","While we couldn’t observe a pattern from one participant, three participants showed a decrease in the number of requests for details as the reading continued. The reduced reliance on the details in the later parts of the story was due to getting familiar with the story context and building trust with the system. P17 commented",0.5195343137254902,0.829084307070707,0.9120979901960783,0.8957527414141414,,
184,140,12,Header,"[(0, 8), (8, 18)]","Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
185,141,12,Table,"[(18, 24), (24, 31), (31, 32), (32, 33), (33, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 43), (43, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 53), (53, 57), (57, 60), (60, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 69), (69, 70)]","Webtoon ID Title (# of panels) # of Comments (Top-level / Total) Genre Creator Foolery el Smythe URL [22] [66] W3 Pixie and Brutus (6) 521 / 744 Comedy Pet W4 W5 Lore Olympus (72) The Remarried Empress (64) 5,226 / 6,681 6,078 / 7,614 Romance Fantasy Alphatart Rach & Sumpul [5]",0.13668954248366014,0.11076612525252529,0.8633049450980393,0.16929680707070716,,
186,142,12,Caption,"[(70, 78)]",Table 6: Webtoons used in the evaluation study,0.344059477124183,0.1759309828282827,0.6554661071895426,0.18725223434343435,,
187,143,12,Paragraph,"[(78, 91), (91, 103), (103, 115), (115, 126), (126, 134)]","“I requested details in the frst few panels to see how important the hidden details would be. Then I soon realized they weren’t necessary at all!” Also, P12 and P13 mentioned that they checked the details only when there were some comments left on the panel, as anchored- comments can be an indicator of notable points.",0.08570751633986928,0.2178632626262627,0.482945791993464,0.28451031717171726,,
188,143,12,Paragraph,"[(134, 144), (144, 158), (158, 168), (168, 180), (180, 192), (192, 204), (204, 214), (214, 215)]","In the baseline, two participants (P19, P21) skipped the descrip- tion and moved on to the next panel even when the audio was still being played. P19 reported “After I experienced the short descrip- tion in the frst round (Cocomix condition), listening to the long one (baseline condition) was too boring. I didn’t want to waste time.” He mentioned that he frst listened to the few words of the description and when it was about minor character’s appearance, he would skip.",0.08701143790849673,0.28702622626262625,0.482940654248366,0.39520602424242424,,
189,143,12,Paragraph,"[(215, 224), (224, 232), (232, 242), (242, 252), (252, 262), (262, 275), (275, 286), (286, 295), (295, 304), (304, 316), (316, 327), (327, 339), (339, 350), (350, 362), (362, 373)]","6.3.2 Perception of Comments Data (RQ2). All participants were positive about reading other readers’ comments. Participants men- tioned the usefulness of comments in various aspects. First, P16, P18, P20, and P21 noted comments complement the description with additional details omitted in the original description. P18 noted “I didn’t realize that the bird’s eye was purple before I read the com- ment! Describers are humans and they miss things... so thank you, commenters!"" P14, who never requested more details but actively checked most of the panel-anchored comments reported, “I didn’t even need to request more details as the comments gave details! And in a more fun way. In addition, subjective comments were found to be helpful in picturing the scene more vividly. P18 noted “The description said her eyes were glowing. The comment said ‘Her glow- ing eyes sent chills down my spine!’ I defnitely prefer the comment’s style, as it tells me how I’m supposed to feel it.”",0.08790522875816993,0.4139744585858586,0.48293288316993466,0.6190354848484848,,
190,143,12,Paragraph,"[(373, 382), (382, 392), (392, 401), (401, 409), (409, 421), (421, 433), (433, 444), (444, 455), (455, 468), (468, 480), (480, 492), (492, 504), (504, 508)]","Participants also noted that comments were useful even when they did not provide additional information about the story. They enjoyed comments as an independent component of a webtoon experience, where they could read humor, communicate, discuss the work and be part of the community and culture. P15 mentioned “Even when I fully understood the story and didn’t need further infor- mation, I just read comments because I’m curious about other readers’ thoughts.” Also, P18 compared the two roles of comments and how the content should be diferent for each purpose. “I enjoy some of the comments as a social activity after I fnish an episode. Comments with this purpose can have more personal opinions, and it’s totally fne. I feel like I’m connected to other readers and part of the conversation going around the story.”",0.08570845375816992,0.6215287515151515,0.481932002124183,0.7989167979797979,,
191,143,12,Paragraph,"[(508, 518), (518, 527), (527, 537), (537, 548), (548, 558), (558, 569), (569, 580)]","One unexpected fnding is that two participants (P15, P20) found comments useful for assurance of understanding the content of webtoons. P15 noted “When I read other readers’ comments and nothing unexpected is there, then I know whether I understood the scene correctly.” P20 also mentioned that “When the comment adds additional details to what I heard from the description, it’s good. But when the comment talks about completely new stuf, then (I",0.08736724640522876,0.8014100646464647,0.4820669660130719,0.8957740464646465,,
192,143,12,Paragraph,"[(580, 590), (590, 599), (599, 609), (609, 620), (620, 625)]","experience) a dramatic loss of confdence” Similarly, three of the participants(P13, P15, P20) noted that they checked the details whenever comments were anchored to the panel. P13 mentioned I can guess that something important is going on when many people are commenting about it (panel).",0.5189918300653594,0.21783722222222215,0.912096185620915,0.2845329595959597,,
193,143,12,Paragraph,"[(625, 634), (634, 641), (641, 651), (651, 661), (661, 673), (673, 681), (681, 691), (691, 701), (701, 712), (712, 720), (720, 730), (730, 741), (741, 751), (751, 763), (763, 772), (772, 783)]","6.3.3 Usability and Usefulness of Cocomix (RQ3). All participants reported Cocomix’s interactive gestures (double-tapping to access selective details and swiping-up to access comments) were easy to learn. When asked which interface they wanted to continue using, 10 of 12 participants chose Cocomix (1 chose the baseline, 1 chose both). Most participants including participants who checked details every time (P15, P18, P20) mentioned that they preferred Cocomix as it provides more options in reading webtoons. Similarly, when asked if they had sufcient control over the amount of description, participants rated Cocomix signifcantly higher than the baseline (Figure 7). While the participants could still skip the description with the baseline, they mentioned it was difcult to interrupt the description is being played. P16 specifed “The amount of description accessible was the same in both conditions, but when the frst version (baseline) was just dumping everything, second one (Cocomix) was more like asking “Would you like to know about this?’ ”",0.518640522875817,0.2973532464646464,0.913706207843137,0.5162526565656566,,
194,143,12,Paragraph,"[(783, 790), (790, 799), (799, 809), (809, 819), (819, 830), (830, 836)]","Participants rated Cocomix’s comments signifcantly more help- ful for story understanding than the most-liked comments in the baseline. Still, one participant (P12) mentioned that the panel- anchored comments can interrupt the reading fow. He commented “I still prefer to access comments after I fnish an episode. Switching narratives is just not for me.”",0.5195343137254902,0.5187446606060606,0.9155999240196079,0.5992741212121212,,
195,143,12,Paragraph,"[(836, 844), (844, 855), (855, 866), (866, 876), (876, 885), (885, 897), (897, 908), (908, 919)]","Room for improvement was noted. Some participants expressed that the interactive features were burdensome if they wanted to use them often, and suggested supporting selection of the level of detail when starting the episode. Also, one participant (P11) pointed out that the presentation order is sometimes unnatural when listening to the on-demand details. She noted “First I listened to what the character is doing and the conversation, and when I double-tapped, I listened to what he’s wearing. The order doesn’t make sense sometimes.",0.5189918300653594,0.6017673878787878,0.9136903091503268,0.7099698282828283,,
196,143,12,Paragraph,"[(919, 927), (927, 936), (936, 944), (944, 952), (952, 962), (962, 973), (973, 985), (985, 989)]","The appropriateness of amount of description (baseline: 5.50 & Cocomix: 5.83) was comparable, showing that participants were satisfed with descriptions generated by following our guidelines. One participant (P17) commented how concise the descriptions were compared to conventional audio descriptions. “I never listen to audio descriptions for movies because they’re just so obsessed with de- scribing everything shown in the scene. But, here I think both versions did the task well.”",0.5189918300653594,0.712463094949495,0.9143506127450979,0.8206655353535355,,
197,144,12,Section,"(989, 991)",7 DISCUSSION,0.5195343137254902,0.8375529066919192,0.6514773826797385,0.8513270228535355,,
198,145,12,Paragraph,"[(991, 1003), (1003, 1014), (1014, 1023)]","In this section, we refect on our study fndings on adaptive descrip- tion and interactive description, then discuss how to scale up and generalize our approach. Next, we discuss how broader interaction",0.5195343137254902,0.8567585494949496,0.9145661475490195,0.8957527414141414,,
199,146,13,Header,"[(0, 10), (10, 13)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Huh et al.",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
200,147,13,Table,"[(13, 16), (16, 17), (17, 18), (18, 19), (19, 24), (24, 26), (26, 28), (28, 29), (29, 36), (36, 38), (38, 40), (40, 41), (41, 50), (50, 52), (52, 53), (53, 54), (54, 62), (62, 64), (64, 66), (66, 67), (67, 76), (76, 80), (80, 81), (81, 90), (90, 92), (92, 94), (94, 95), (95, 102), (102, 104), (104, 106), (106, 107), (107, 118), (118, 119), (119, 124), (124, 126), (126, 128)]",Statement ID Statement Baseline Cocomix S1 I could concentrate during reading. 6.75 (0.45) 6.50 (1.00) S2 I could process all the given information. 6.42 (0.90) 6.83 (0.39) S3 The amount of detail was appropriate in each panel 5.50 (1.57) 5.83(1.03) S4 I had full control over the reading time. 4.83 (1.40) 5.58 (1.38) S5 I had full control over the amount of description.* 2.92 (2.23) 6.50 (0.80) S6 I am confdent in comprehending and feeling the story. 5.08 (1.98) 5.67 (1.37) S7 I can read stories in efcient manners. 5.67 (1.37) 5.92 (1.38) S8 Comments were helpful for understanding the content.* 4.25 (1.76) 5.25 (1.66) S9 Comments were fun to read. 6.17 (1.59) 6.42 (1.25),0.1892075163398693,0.11076612525252529,0.8107900104575162,0.252303919191919,,
201,148,13,Caption,"[(128, 148), (148, 167)]",Table 7: We collected participants’ subjective ratings on Cocomix in a 7-point Likert scale. The value represents the mean of scores (SD). Wilcoxon test was run and a statistical signifcant diference ( p < 0.05) is marked with *.,0.08742715555555539,0.2589381494949492,0.912101539869281,0.28412784646464617,Table,8.0
202,149,13,Figure,"[(167, 168), (168, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 175), (175, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 182), (182, 183), (183, 184), (184, 185)]",1 2 3 4 5 6 7 S1 S2 S3 S4 S5 S6 S7 S8 S9 Baseline Cocomix,0.0916597993366013,0.32300437618686867,0.460881809379085,0.4252710764027777,,
203,150,13,Caption,"[(185, 196), (196, 210), (210, 222), (222, 230), (230, 240)]","Figure 7: Boxplot of mean scores for the baseline and Co- comix (1 = low, 7 = high). Wilcoxon test was run and a statis- tical signifcant diference ( p < 0.05) is marked with *. Co- comix signifcantly outperformed the baseline in S5 (Con- trol over the amount of description) and S8 (Usefulness of",0.08789057777777794,0.44553514949494955,0.4830861228758171,0.5121944464646464,Figure,7.0
204,151,13,Paragraph,"[(240, 245)]",comments in helping content understanding).,0.08789057777777794,0.5147077555555555,0.3917519111111113,0.5260289676767677,,
205,151,13,Paragraph,"[(245, 254), (254, 263)]","data can power data-drive accessibility. We also discuss limitations of our work, which represent opportunities for future research.",0.08790522875816993,0.5506853171717171,0.4804614787581699,0.5758436505050505,,
206,152,13,Section,"(263, 269)",7.1 Adapting Descriptions into Broader Contexts,0.08790522875816993,0.5894304319444444,0.4155703859477125,0.619554423989899,,
207,153,13,Paragraph,"[(269, 279), (279, 289), (289, 298), (298, 309), (309, 318), (318, 328), (328, 333)]","Cocomix adapts each panel’s length of description by how fre- quently it was mentioned in comments. In the evaluation study, the confdence in story comprehension was comparable in both conditions, implying that our adaptation did not result in any major information loss. However, there was no signifcant diference in reading efciency as participants had to adapt the description to their preference in each panel.",0.08790025947712407,0.6215287515151515,0.4829436900326796,0.715872690909091,,
208,153,13,Paragraph,"[(333, 344), (344, 355), (355, 366), (366, 377), (377, 388), (388, 399), (399, 408), (408, 419), (419, 429), (429, 441), (441, 448)]","There are two dimensions in which we can consider the adap- tation of description: 1) adapting the total length of description to user context 2) adapting each panel’s length to story context. In this paper, we explored the second dimension. As a future work, we plan to investigate how the description can be adapted under various user contexts. From the usage pattern of detail requests, we found that the desired length depends on individual’s preference, and familiarity with the story. From the interview feedback, we also discovered that various reading contexts, such as whether they are in a hurry or relaxing, should be considered so that the description length is adjusted to desired reading time.",0.08736724640522876,0.7183886,0.48293916094771233,0.8680797616161615,,
209,153,13,Paragraph,"[(448, 456), (456, 466)]","To provide appropriate descriptions under various contexts, the mode selection can be provided as suggested by one participant.",0.08790522875816993,0.8705956707070707,0.4827245,0.8957527414141414,,
210,153,13,Paragraph,"[(466, 475), (475, 483), (483, 484)]","As a step forward from ‘one-description-fts-all’, we envision per- sonalized adaptation of description by leveraging users’ interaction patterns.",0.5190212418300654,0.32350476161616154,0.9145657756535949,0.3625002161616162,,
211,154,13,Section,"(484, 488)",7.2 Interacting with Description,0.5195343137254902,0.3772082097222222,0.7899266959150326,0.39098232588383836,,
212,155,13,Paragraph,"[(488, 498), (498, 506), (506, 515), (515, 525), (525, 533), (533, 540)]","With Cocomix, BLV users can interact with the description by actively requesting further details and exploring relevant comments. Interestingly, one participant noted that the details would stay longer in memory when it was provided on-demand than granted. This suggests additional applicability of interactive descriptions for highlighting information in addition to providing options.",0.5188316993464052,0.3964138525252525,0.9143507774509807,0.47691940808080807,,
213,155,13,Paragraph,"[(540, 548), (548, 558), (558, 568), (568, 579), (579, 590), (590, 601), (601, 610), (610, 621)]","We distilled considerations for introducing interactivity in de- scriptions. First, the presentation order should be carefully set, so that the default description alone is sufcient for following the storyline and selective details do not harm the reading fow. This means that not only the dependencies between sentences of a single image (panel), but also dependencies of a sentence on later panels’ descriptions should be considered. Prior research [48] has suggested determining the order as how a sighted person sees an image.",0.5195343137254902,0.4794353171717171,0.9145682459150326,0.5876163777777778,,
214,155,13,Paragraph,"[(621, 631), (631, 640), (640, 649), (649, 659), (659, 670), (670, 681), (681, 692), (692, 703), (703, 708)]","Also, the notice of availability for interaction should be carefully designed. During our iterative design process, we discovered that users unaware of availability are concerned about ‘unknown un- knowns’ or frustrated when the requested detail is unavailable. Co- comix provides clear audio cues to inform users that they can explore details or read comments in the panel. However, one unex- pected fnding was that the notifcation made it difcult to ignore details for some users, leaving a question of whether BLV users truly had ‘freedom’ in control.",0.5195343137254902,0.5901310242424243,0.9145723062091503,0.712149206060606,,
215,155,13,Paragraph,"[(708, 719), (719, 729), (729, 739), (739, 749), (749, 757), (757, 767), (767, 779), (779, 788), (788, 796)]","Finally, the unit of interaction impacts the reading fow. As shown in our results, the concentration measure was slightly lower when BLV readers used the interactive features. We also observed that the desired level of interactivity varied by individuals. For example, one participant suggested sentence-level control to gain complete control over what he receives. Another participant noted the burden of having to make decisions in every panel whether to access details. Interactions in micro-level can support full-control, but its relation to enhanced reading experience is an open question.",0.5195343137254901,0.7146651151515152,0.9143507774509805,0.8366820343434344,,
216,156,13,Section,"(796, 801)",7.3 Scalability of Webtoon Descriptions,0.5195343137254902,0.8513900279040404,0.8486076717320261,0.8651641440656566,,
217,157,13,Paragraph,"[(801, 810), (810, 820)]","We proposed guidelines (Section 3.4) for describing webtoons. Yet, there are several challenges with respect to scalability. First, most",0.5188316993464052,0.8705956707070707,0.913699885620915,0.8957527414141414,,
218,158,14,Header,"[(0, 8), (8, 18)]","Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
219,159,14,Paragraph,"[(18, 27), (27, 36), (36, 48), (48, 58), (58, 67), (67, 78), (78, 89), (89, 100), (100, 107)]","webtoons are episodic and periodical thus require consistent eforts for making descriptions. Second, the describer should be familiar with the story context. Prior work has suggested the potential of in- volving fans to crowdsource comic descriptions [63]. An example of dedicated webtoon fans’ contribution is Naver Webtoon Translate 12 where fans translate webtoons to share with a global audience. As a globally large population who are familiar with the story and motivated to share the story with others, we believe that webtoon fans have potential to scale up descriptions.",0.08736437908496732,0.10956031717171716,0.48294181715686274,0.23157723636363645,,
220,159,14,Paragraph,"[(107, 116), (116, 126), (126, 134), (134, 145), (145, 153), (153, 164), (164, 173), (173, 183), (183, 194), (194, 204), (204, 213), (213, 218)]","To move toward a more streamlined and automated process, we suggest using templates [26, 46] for ensuring consistency of lengths, expressions, and contents between describers. We believe that data collected can be used to augment future pipelines, such as automatic generation of human-like descriptions. Also, some initial steps, such as grouping continuous panels into phasels , can be automated with the advancement of computer vision techniques. While existing research around comics focuses on detection [52, 53, 62] and extraction [13, 16, 56] of sub-elements, we believe higher level of image analysis such as grouping panels into semantically meaningful units or summarization into key panels can further catalyze meaningful interactions with comics.",0.0872016864379085,0.23409314545454543,0.48272169248366015,0.3976214282828283,,
221,160,14,Section,"(218, 225)",7.4 Generalizability of Cocomix and Comments-driven Accessibility,0.08790522875816993,0.4159304319444444,0.3909001336601307,0.44605442398989903,,
222,161,14,Paragraph,"[(225, 235), (235, 248), (248, 259), (259, 269), (269, 278), (278, 288), (288, 297), (297, 306), (306, 320), (320, 330), (330, 340), (340, 348)]","We believe our approach on mining comments to improve accessi- bility can be used in other types of media (e.g., social media images, YouTube videos, etc). To see how our approach can generalize to other types of webcomics, we sampled webcomics from social me- dia and analyzed whether the commenting behavior is diferent. From Facebook and Instagram, we searched with the query “comics” and selected top-ten webcomics that are regularly published. Then, two researchers analyzed the comments using same taxonomy used in Section 4. We found that more comments are in the form of ‘Com- munication’ or ‘Self-expression’. It is partially due to the inherent characteristics of the platforms, and also due to ‘Mention’ feature which readers actively use to share with friends.",0.08720261437908497,0.4514883474747475,0.4829484287581698,0.6150166303030303,,
223,161,14,Paragraph,"[(348, 359), (359, 368), (368, 377), (377, 387), (387, 398), (398, 412), (412, 423), (423, 434), (434, 444), (444, 453), (453, 463), (463, 475), (475, 487), (487, 496), (496, 507), (507, 517), (517, 528)]","The performance of our approach relies on the number of com- ments. While our guidelines for descriptions are applicable re- gardless, the core features’ efectiveness might not be highlighted well with newly launched webtoons with less viewership. Also, a certain amount of delay is unavoidable to wait for webtoon readers to comment on each episode. Yet, we believe this is not to a great degree as we observed from the collected comments data that ma- jority of readers commented within a few hours each webtoon was uploaded. To better utilize the comments, we make the following suggestions for designers of social features in webcomics platforms: First, support referencing to a panel, or within-panel elements such as characters or objects that are shown to described often (Table 4). This not only increases the accuracy of our linking results, but also encourages readers to leave more specifc comments by reducing the burden to specify the target. Second, prompt readers to com- ment on various elements and attributes. When most readers talk about batman, system can ask ‘What do you think about joker’s",0.08736437908496732,0.6175325393939394,0.48294215228758175,0.8502690707070707,,
224,162,14,Footnote,"[(528, 530)]",12 https://translate.webtoons.com/guide,0.08769934640522875,0.8844750296717171,0.2699769888888889,0.8951665345959596,,
225,163,14,Paragraph,"[(530, 538), (538, 548), (548, 556)]","hair?’. Third, acknowledge and incentivize their prosocial behav- iors. For example, active commenters can be provided early access to latest episodes not yet available to public.",0.5195343137254902,0.10956031717171716,0.9145628003267972,0.14855577171717174,,
226,164,14,Section,"(556, 562)",7.5 Availability of Other Interaction Data,0.5195343137254902,0.16888118952020204,0.8635452955882355,0.1826553056818183,,
227,165,14,Paragraph,"[(562, 573), (573, 583), (583, 592), (592, 604), (604, 616), (616, 627), (627, 635), (635, 648), (648, 658), (658, 666), (666, 676), (676, 687), (687, 697), (697, 704)]","Reinholt et al. [61] proposed the concept of use-driven accessibility , a concept that an image’s accessible description can arise from sighted users’ interaction with the image, rather than conscious work to create an image description. In this paper, we focused on one type of such data, user comments, which are available at low cost and contain rich contexts. We believe that Cocomix can further beneft from incorporating other interaction data. For example, scroll or gaze data can be analyzed to identify the panels or elements where readers viewed longer, and laughter [35] can be collected to note interesting scenes. However, there remain unanswered questions with leveraging ‘byproduct’ data such as “Will the output align with the creator’s intent?”, and “Can we expect the same focus-patterns from BLV readers as sighted readers?” We hope our research catalyzes future work in data-driven accessibility.",0.5189918300653594,0.18808683232323228,0.9137109949346405,0.3792893575757576,,
228,166,14,Section,"(704, 709)",7.6 Limitations and Future Work,0.5195343137254902,0.39961477537878787,0.796129909640523,0.41338889154040404,,
229,167,14,Paragraph,"[(709, 720)]",Our work has several limitations which we address in this section.,0.5195343137254902,0.4188204181818182,0.9142518143790851,0.4301416303030304,,
230,168,14,List,"[(720, 731), (731, 741), (741, 749), (749, 760), (760, 769), (769, 778), (778, 789), (789, 795), (795, 806), (806, 814), (814, 824), (824, 832), (832, 834), (834, 844), (844, 853), (853, 861), (861, 869), (869, 879), (879, 890), (890, 896), (896, 899), (899, 908), (908, 918), (918, 928), (928, 938), (938, 948), (948, 958), (958, 966), (966, 973), (973, 978), (978, 990), (990, 1001), (1001, 1010)]","• We did not explore how peculiarities of comics medium such as aesthetics can be conveyed well in the description. From our formative study, we discovered that most participants were not interested in the drawing style or the layout of panels as readers born with visual impairments lacked the sensory experience. To support readers to picture the scene more quickly as they do with drawings, it would be enriching to complement descriptions with audio augmentations. • We did not investigate the efectiveness of our webtoon de- scription guidelines in terms of unambiguity and output consistency. Our next step is to assess and improve our guidelines with feedback from both professional and non- professional describers. • Cocomix defnes key panels as panels with elements fre- quently mentioned in comments. Thus, it has limited capa- bility of detecting panels with setting introductions which play a signifcant role in understanding forthcoming panels but less mentioned in comments. One solution may be to consider other data such as time spent on each panel, or dependencies between panel-descriptions to cover various context of ‘importance’. • Both studies involved limited number of participants (for- mative study: 10, evaluation study: 12). We need to better account for the diverse needs of people with diferent de- grees of sight loss, usages of assistive tools, and technical skills [20, 54]. Our pipeline evaluation only covered fve dif- ferent webtoons and we need to further explore the pipeline’s robustness in diverse webtoons. Also, we evaluated extracted comments individually, not rating how diverse information they provide as a whole. • We did not explore how users perceive and react with in- correct output of the system as no user reported that they noticed irrelevant comments in our user study. While we",0.545617077124183,0.4407316828282828,0.9145772251633989,0.8957488040404041,,
231,169,15,Header,"[(0, 10), (10, 13)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Huh et al.",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
232,170,15,Paragraph,"[(13, 21), (21, 30), (30, 38)]","showed the feasibility of pipeline through separate evalua- tion, incorrect links can still exist where human validation such as reporting from users would be valued.",0.1277892156862745,0.10956031717171716,0.4829463888888889,0.14855577171717174,,
233,171,15,Section,"(38, 40)",8 CONCLUSION,0.08790522875816993,0.16205543194444438,0.23132780816993465,0.17582954810606063,,
234,172,15,Paragraph,"[(40, 49), (49, 56), (56, 62), (62, 71), (71, 79), (79, 89), (89, 101), (101, 109), (109, 112)]","We present Cocomix, an interactive webtoon reader that utilizes comments to improve non-visual webtoon accessibility. Cocomix provides adaptive description through comments-based scoring and abridging, and supports easy access to relevant comments through panel-anchored comments. We hope our research catalyzes future work in making broader types of visual media accessible. We also encourage web platforms to take note of the success of these comments-driven interaction methods and integrate them to improve non-visual accessibility.",0.08720261437908497,0.18126107474747477,0.4827245,0.3032779939393939,,
235,173,15,Section,"(112, 113)",ACKNOWLEDGMENTS,0.08790522875816993,0.3167789167929293,0.27999095032679744,0.3305530329545456,,
236,174,15,Paragraph,"[(113, 123), (123, 131), (131, 139), (139, 146), (146, 157), (157, 165), (165, 172), (172, 179), (179, 181)]","This work was supported by Institute of Information & commu- nications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2021-0-01347, Video Interac- tion Technologies Using Object-Oriented Video Modeling). This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2021-2020-0-01460) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation)",0.0874656862745098,0.335983296969697,0.4829484287581699,0.45800147878787884,,
237,175,15,Section,"(181, 182)",REFERENCES,0.08790522875816993,0.47150113901515145,0.20132778316993463,0.4852752551767677,,
238,176,15,Bibliography,"[(182, 205), (205, 227), (227, 251), (251, 273), (273, 287), (287, 301), (301, 322), (322, 340), (340, 352), (352, 364), (364, 379), (379, 403), (403, 417), (417, 441), (441, 460), (460, 477), (477, 496), (496, 520), (520, 531), (531, 552), (552, 561), (561, 573), (573, 598), (598, 620), (620, 638), (638, 653), (653, 679), (679, 696), (696, 717), (717, 742), (742, 761), (761, 787), (787, 805), (805, 827), (827, 833), (833, 858), (858, 880), (880, 893), (893, 906), (906, 920), (920, 944), (944, 967), (967, 984), (984, 1008), (1008, 1020), (1020, 1044), (1044, 1069), (1069, 1087), (1087, 1117), (1117, 1135), (1135, 1148), (1148, 1171), (1171, 1188), (1188, 1203), (1203, 1219), (1219, 1241)]","[1] Dragan Ahmetovic, Nahyun Kwon, Uran Oh, Cristian Bernareggi, and Sergio Mascetti. 2021. Touch Screen Exploration of Visual Artwork for Blind People. In Proceedings of the Web Conference 2021 . 2781–2791. [2] Kholoud Khalil Aldous, Jisun An, and Bernard J Jansen. 2019. View, like, comment, post: Analyzing user engagement by topic at 4 levels across 5 social media platforms for 53 news organizations. In Proceedings of the International AAAI Conference on Web and Social Media , Vol. 13. 47–57. [3] Chad Allen. [n.d.]. Unseen . Retrieved August 30, 2021 from https://www. unseencomic.com/ [4] Alphatart&Sumpul. [n.d.]. The Remarried Empress . Retrieved September 2, 2021 from https://www.webtoons.com/en/fantasy/the-remarried-empress/list?title_ no=2135 [5] Alphatart/Sumpul. [n.d.]. The Remarried Empress . Retrieved August 30, 2021 from https://www.webtoons.com/en/fantasy/the-remarried-empress/list?title_ no=2135 [6] Kohei Arai and Herman Tolle. 2010. Automatic e-comic content adaptation. International Journal of Ubiquitous Computing 1, 1 (2010), 1–11. [7] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2016. A simple but tough-to-beat baseline for sentence embeddings. (2016). [8] artweb. [n.d.]. DESCRIVEDDO GUIDELINES . Retrieved September 9, 2021 from https://artweb.netlify.app/desc_en [9] asimplebengo. [n.d.]. Hybrid . Retrieved September 2, 2021 from https://www. webtoons.com/en/challenge/hybrid/list?title_no=211861 [10] audiobook. [n.d.]. a movie in your mind . Retrieved August 30, 2021 from https://www.graphicaudiointernational.net/ [11] Olivier Augereau, Motoi Iwata, and Koichi Kise. 2018. A survey of comics research in computer science. Journal of imaging 4, 7 (2018), 87. [12] braille comics. [n.d.]. Laville Braille . Retrieved August 30, 2021 from http: //www.lavillebraille.fr/des-livres-a-voir-et-a-toucher/ [13] João MC Correia and Abel JP Gomes. 2016. Balloon extraction from complex comic books using edge detection and histogram scoring. Multimedia Tools and Applications 75, 18 (2016), 11367–11390. [14] Jakob Dittmar. 2014. Comics for the blind and for the seeing. International Journal of Comic Art; 1 16 (2014). [15] David Dubray and Jochen Laubrock. 2019. Deep CNN-based speech balloon detection and segmentation for comic books. In 2019 International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 1237–1243. [16] Arpita Dutta and Samit Biswas. 2019. CNN based extraction of panels/characters from bengali comic book page images. In 2019 International Conference on Docu- ment Analysis and Recognition Workshops (ICDARW) , Vol. 1. IEEE, 38–43. [17] Teresa Kardoulias Sarah Stephenson Keyes Elisabeth Salzhauer Axel, Vir- ginia Hooper and Francesca Rosenberg. [n.d.]. AEB’s Guidelines for Verbal De- scription . Retrieved September 9, 2021 from http://www.artbeyondsight.org/ handbook/acs-guidelines.shtml [18] EmiMG. [n.d.]. ZomCom . Retrieved September 2, 2021 from https://www. webtoons.com/en/challenge/zomcom/list?title_no=70195 [19] Siamak Faridani, Ephrat Bitton, Kimiko Ryokai, and Ken Goldberg. 2010. Opinion space: a scalable tool for browsing online comments. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems . 1175–1184. [20] Anita Fidyka and Anna Matamala. 2018. Audio description in 360º videos. Trans- lation Spaces (2018). [21] Pet Foolery. [n.d.]. Pixie and Brutus . Retrieved September 2, 2021 from https: //www.webtoons.com/en/challenge/pixie-and-brutus/list?title_no=452175 [22] Pet Foolery. [n.d.]. Pixie and Brutus . Retrieved August 30, 2021 from https: //www.webtoons.com/en/challenge/pixie-and-brutus/list?title_no=452175 [23] Benjamin Fraser. 2020. Tactile comics, disability studies and the mind’s eye: on “A Boat Tour”(2017) in Venice with Max. Journal of Graphic Novels and Comics (2020), 1–13. [24] Graham R Gibbs. 2007. Thematic coding and categorizing. Analyzing qualitative data 703 (2007), 38–56. [25] Cole Gleason, Amy Pavel, Himalini Gururaj, Kris Kitani, and Jefrey P Bigham. 2020. Making GIFs Accessible.. In ASSETS . 24–1. [26] Cole Gleason, Amy Pavel, Xingyu Liu, Patrick Carrington, Lydia B Chilton, and Jefrey P Bigham. 2019. Making memes accessible. In The 21st International ACM SIGACCESS Conference on Computers and Accessibility . 367–376. [27] Cole Gleason, Amy Pavel, Emma McCamey, Christina Low, Patrick Carrington, Kris M Kitani, and Jefrey P Bigham. 2020. Twitter A11y: A browser extension to make Twitter images accessible. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1–12. [28] Matthew Honnibal and Mark Johnson. 2015. An Improved Non-monotonic Tran- sition System for Dependency Parsing. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics, Lisbon, Portugal, 1373–1378. https://aclweb.org/anthology/D/D15/ D15-1162 [29] Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd- Graber, Hal Daume, and Larry S Davis. 2017. The amazing mysteries of the gutter: Drawing inferences between panels in comic book narratives. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition . 7186–7195. [30] Jakka. [n.d.]. Independant Diary . Retrieved August 30, 2021 from https://comic. naver.com/webtoon/list?titleId=748105&no=78&weekday=thu [31] JH. [n.d.]. The Boxer . Retrieved August 30, 2021 from https://comic.naver.com/ webtoon/list?titleId=736989&weekday=thu [32] Violet Karim. [n.d.]. Familiar Feelings . Retrieved September 2, 2021 from https: //www.webtoons.com/en/challenge/familiar-feelings/list?title_no=323558 [33] Hyunwoo Kim, Haesoo Kim, Kyung Je Jo, and Juho Kim. 2021. StarryThoughts: Facilitating Diverse Opinion Exploration on Social Issues. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–29. [34] Jefrey SJ Kirchof. 2013. It’s just not the same as print (and it shouldn’t be): Rethinking the possibilities of digital comics. Technoculture: An Online Journal of Technology in Society 3, 1 (2013). [35] Soyoung Kwon and Kun-Pyo Lee. 2016. What makes readers laugh? value of sensing laughter for humor webtoon. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct . 867–874. [36] Yunjung Lee, Hwayeon Joh, Suhyeon Yoo, and Uran Oh. 2021. AccessComics: an accessible digital comic book reader for people with visual impairments. In Proceedings of the 18th International Web for All Conference . 1–11. [37] Luyuan Li, Yongtao Wang, Liangcai Gao, Zhi Tang, and Ching Y Suen. 2014. Comic2CEBX: A system for automatic comic content adaptation. In IEEE/ACM Joint Conference on Digital Libraries . IEEE, 299–308. [38] Guanhong Liu, Xianghua Ding, Chun Yu, Lan Gao, Xingyu Chi, and Yuanchun Shi. 2019. "" I Bought This for Me to Look More Ordinary"" A Study of Blind People Doing Online Shopping. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1–11. [39] Yang Liu. 2019. Fine-tune BERT for extractive summarization. arXiv preprint arXiv:1903.10318 (2019). [40] Zongyang Ma, Aixin Sun, Quan Yuan, and Gao Cong. 2012. Topic-driven reader comments summarization. In Proceedings of the 21st ACM international conference on Information and knowledge management . 265–274. [41] Scott McCloud. 1993. Understanding comics: The invisible art. Northampton, Mass (1993). [42] Chris McCoy. [n.d.]. Safely . Retrieved September 2, 2021 from https://www. webtoons.com/en/comedy/safely-endangered/list?title_no=352 [43] Mimino666. [n.d.]. langdetect: Port of Google;s language-detection library . Retrieved September 9, 2021 from https://github.com/Mimino666/langdetect# languages [44] Elaheh Momeni, Claire Cardie, and Myle Ott. 2013. Properties, prediction, and prevalence of useful user-generated comments for descriptive annotation of",0.0878987039215686,0.11149001944444449,0.9140209119281048,0.8951665345959596,,
239,177,16,Title,"[(0, 8)]",Cocomix: Utilizing Comments to Improve Non-Visual Webtoon Accessibility,0.08790522875816993,0.0783156446969697,0.444674353267974,0.0871209477272727,,
240,178,16,Header,"[(8, 18)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636881375816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
241,179,16,Bibliography,"[(18, 36), (36, 62), (62, 89), (89, 110), (110, 133), (133, 154), (154, 179), (179, 191), (191, 213), (213, 224), (224, 243), (243, 265), (265, 289), (289, 303), (303, 322), (322, 342), (342, 370), (370, 394), (394, 408), (408, 426), (426, 448), (448, 468), (468, 489), (489, 514), (514, 531), (531, 555), (555, 569), (569, 590), (590, 612), (612, 627), (627, 649), (649, 665), (665, 679), (679, 693), (693, 717), (717, 736), (736, 758), (758, 773), (773, 796), (796, 820), (820, 840), (840, 860), (860, 878), (878, 882), (882, 884), (884, 889), (889, 891), (891, 915), (915, 930), (930, 953), (953, 977), (977, 1003), (1003, 1020), (1020, 1033), (1033, 1060), (1060, 1085), (1085, 1104), (1104, 1117), (1117, 1143), (1143, 1160), (1160, 1183), (1183, 1208), (1208, 1213)]","social media objects. In Proceedings of the International AAAI Conference on Web and Social Media , Vol. 7. [45] Elaheh Momeni, Ke Tao, Bernhard Haslhofer, and Geert-Jan Houben. 2013. Identi- fcation of useful user comments in social media: A case study on Flickr commons. In Proceedings of the 13th ACM/IEEE-CS joint conference on Digital libraries . 1–10. [46] Valerie S Morash, Yue-Ting Siu, Joshua A Miele, Lucia Hasty, and Steven Landau. 2015. Guiding novice web workers in making image descriptions using templates. ACM Transactions on Accessible Computing (TACCESS) 7, 4 (2015), 1–21. [47] Giulio Mori, Maria Claudia Buzzi, Marina Buzzi, and Barbara Leporini. 2010. Structured audio podcasts via web text-to-speech system. In Proceedings of the 19th international conference on World wide web . 1281–1284. [48] Meredith Ringel Morris, Jazette Johnson, Cynthia L Bennett, and Edward Cutrell. 2018. Rich representations of visual content for screen reader users. In Proceedings of the 2018 CHI conference on human factors in computing systems . 1–11. [49] Murrz. [n.d.]. Murrz . Retrieved September 2, 2021 from https://www.webtoons. com/en/slice-of-life/murrz/list?title_no=1281 [50] Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen McKeown, and Bing Xiang. 2021. Entity- level Factual Consistency of Abstractive Text Summarization. arXiv preprint arXiv:2102.09130 (2021). [51] Anime News Network. [n.d.]. Japanese Volunteers Transcribe Manga for Blind People . Retrieved August 30, 2021 from https://www.animenewsnetwork.com/ news/2007-07-24/japanese-volunteers-transcribe-manga-for-blind-people [52] Nhu-Van Nguyen, Christophe Rigaud, and Jean-Christophe Burie. 2017. Comic characters detection using deep learning. In 2017 14th IAPR international confer- ence on document analysis and recognition (ICDAR) , Vol. 3. IEEE, 41–46. [53] Toru Ogawa, Atsushi Otsubo, Rei Narita, Yusuke Matsui, Toshihiko Yamasaki, and Kiyoharu Aizawa. 2018. Object detection for comics using manga109 annotations. arXiv preprint arXiv:1803.08670 (2018). [54] Pilar Orero, Stephen Doherty, Jan-Louis Kruger, Anna Matamala, Jan Pedersen, Elisa Perego, Pablo Romero-Fresco, Sara Rovira-Esteva, Olga Soler-Vilageliu, and Agnieszka Szarkowska. 2018. Conducting experimental research in audio- visual translation (AVT): A position paper. JosTrans: The Journal of Specialised Translation 30 (2018), 105–126. [55] Rachel Sarah Osolen and Leah Brochu. 2020. Creating an Authentic Experience. The International Journal of Information, Diversity, & Inclusion (IJIDI) 4, 1 (2020). [56] Xiaoran Qin, Yafeng Zhou, Yonggang Li, Siwei Wang, Yongtao Wang, and Zhi Tang. 2019. Progressive deep feature learning for manga character recognition via unlabeled training data. In Proceedings of the ACM Turing Celebration Conference- China . 1–6. [57] Vipul Raheja and Joel Tetreault. 2019. Dialogue act classifcation with context- aware self-attention. arXiv preprint arXiv:1904.02594 (2019). [58] Frédéric Rayar. 2017. Accessible comics for visually impaired people: Challenges and opportunities. In 2017 14th IAPR International Conference on Document Anal- ysis and Recognition (ICDAR) , Vol. 3. IEEE, 9–14. [59] Frédéric Rayar, Bernard Oriola, and Christophe Joufrais. 2020. ALCOVE: an accessible comic reader for people with low vision. In Proceedings of the 25th International Conference on Intelligent User Interfaces . 410–418. [60] Kyle Rector, Keith Salmon, Dan Thornton, Neel Joshi, and Meredith Ringel Morris. 2017. Eyes-free art: Exploring proxemic audio interfaces for blind and low vision art engagement. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 3 (2017), 1–21. [61] Kyle Reinholt, Darren Guinness, and Shaun K Kane. 2019. Eyedescribe: Combining eye gaze and speech to automatically create accessible touch screen artwork. In Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces . 101–112. [62] Christophe Rigaud. 2014. Segmentation and indexation of complex objects in comic book images . Ph.D. Dissertation. Université de La Rochelle. [63] Christine Samson, Casey Fiesler, and Shaun K Kane. 2016. "" Holy Starches Batman!! We are Getting Walloped!"" Crowdsourcing Comic Book Transcriptions. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility . 289–290. [64] Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl, and Jose San Pedro. 2010. How useful are your comments? Analyzing and predicting YouTube comments and comment ratings. In Proceedings of the 19th international conference on World wide web . 891–900. [65] Rachel Smythe. [n.d.]. Lore Olympus . Retrieved September 2, 2021 from https: //www.webtoons.com/en/romance/lore-olympus/list?title_no=1320 [66] Rachel Smythe. [n.d.]. Lore Olympus . Retrieved August 30, 2021 from https: //www.webtoons.com/en/romance/lore-olympus/list?title_no=1320 [67] Abigale J Stangl, Esha Kothari, Suyog D Jain, Tom Yeh, Kristen Grauman, and Danna Gurari. 2018. Browsewithme: An online clothes shopping assistant for people with visual impairments. In Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility . 107–118. [68] Raymond Hendy Susanto, Hai Leong Chieu, and Wei Lu. 2016. Learning to capitalize with character-level recurrent neural networks: an empirical study. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing . 2090–2095. [69] tactile comics. [n.d.]. Life - A tactile comic for blind people | Philipp Meyer . Re- trieved August 30, 2021 from https://www.hallo.pm/life/ [70] Carla Tamburro, Timothy Neate, Abi Roper, and Stephanie Wilson. 2020. Acces- sible Creativity with a Comic Spin. In The 22nd International ACM SIGACCESS Conference on Computers and Accessibility . 1–11. [71] Garreth W Tigwell, Benjamin M Gorman, and Rachel Menzies. 2020. Emoji Ac- cessibility for Visually Impaired People. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems . 1–14. [72] Web Accessibility Tutorials. [n.d.]. Images Concepts - Images - WAI . Retrieved September 9, 2021 from https://www.graphicaudiointernational.net/ [73] uru chan. [n.d.]. unOrdinary . Retrieved September 2, 2021 from https://www.webtoons.com/en/super-hero/unordinary/episode- 223/viewer?title_no=679&episode_no=234 [74] Violeta Voykinska, Shiri Azenkot, Shaomei Wu, and Gilly Leshed. 2016. How blind people interact with visual content on social networking services. In Pro- ceedings of the 19th acm conference on computer-supported cooperative work & social computing . 1584–1595. [75] Ruolin Wang, Zixuan Chen, Mingrui Ray Zhang, Zhaoheng Li, Zhixiu Liu, Zihan Dang, Chun Yu, and Xiang’Anthony’ Chen. 2021. Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision Users. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1–14. [76] Xinwei Wang, Jun Hu, Bart Hengeveld, and Matthias Rauterberg. 2018. Segmen- tation of panels in d-Comics. In Interactivity, Game Creation, Design, Learning, and Innovation . Springer, 28–37. [77] Xinwei Wang, Jun Hu, Bart Hengeveld, and Matthias Rauterberg. 2019. Express- ing segmentation in d-comics. In International Conference on Human-Computer Interaction . Springer, 402–409. [78] Shaomei Wu and Lada A Adamic. 2014. Visually impaired users on an online social network. In Proceedings of the sigchi conference on human factors in computing systems . 3133–3142. [79] Shaomei Wu, Jefrey Wieland, Omid Farivar, and Julie Schiller. 2017. Automatic alt-text: Computer-generated image descriptions for blind users on a social network service. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing . 1180–1192. [80] Yaongyi. [n.d.]. True Beauty . Retrieved September 2, 2021 from https://www. webtoons.com/en/romance/truebeauty/list?title_no=1436 [81] Matin Yarmand, Dongwook Yoon, Samuel Dodson, Ido Roll, and Sidney S Fels. 2019. "" Can you believe [1: 21]?!"" Content and Time-Based Reference Patterns in Video Comments. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1–12. [82] Deming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. 2020. Coreferential reasoning learning for language representation. arXiv preprint arXiv:2004.06870 (2020). [83] Jefrey M Zacks, Barbara Tversky, and Gowri Iyer. 2001. Perceiving, remembering, and communicating structure in events. Journal of experimental psychology: General 130, 1 (2001), 29.",0.08789870392156861,0.11148816717171715,0.9140264529411763,0.7421399532828283,,
242,180,17,Header,"[(0, 10), (10, 13)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Huh et al.",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
243,181,17,Section,"(13, 14)",A,0.08790522875816993,0.10765139154040401,0.10109597058823529,0.12142550770202028,,
244,182,17,Title,"[(14, 17)]",STUDY PARTICIPANTS DEMOGRAPHICS,0.1189212973856209,0.10765139154040401,0.46202318758169947,0.12142550770202028,,
245,183,17,Table,"[(17, 23), (23, 24), (24, 31), (31, 32), (32, 33), (33, 34), (34, 36), (36, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 64), (64, 65), (65, 66), (66, 67), (67, 68), (68, 69), (69, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 85), (85, 86), (86, 87), (87, 88), (88, 89), (89, 90), (90, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 99), (99, 100), (100, 101), (101, 102), (102, 103), (103, 104), (104, 106), (106, 107), (107, 108), (108, 109), (109, 110), (110, 111), (111, 113), (113, 114), (114, 115), (115, 116), (116, 117), (117, 118), (118, 120), (120, 121), (121, 122), (122, 123), (123, 124), (124, 125), (125, 127), (127, 128), (128, 129), (129, 130), (130, 131), (131, 132), (132, 134), (134, 135), (135, 136), (136, 137), (137, 138), (138, 139), (139, 141), (141, 142), (142, 143), (143, 144), (144, 145), (145, 146), (146, 148), (148, 149), (149, 150), (150, 151), (151, 152), (152, 153), (153, 155), (155, 156), (156, 157), (157, 158), (158, 159), (159, 160), (160, 162), (162, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 169), (169, 170), (170, 171), (171, 172), (172, 173), (173, 174), (174, 176), (176, 177), (177, 178), (178, 179), (179, 180), (180, 181), (181, 183), (183, 184), (184, 185)]",PID Age Gender Visually impairment description Onset Experienced comics before or after vision loss P1 22 Female Totally blind Congenital Never P2 22 Male Low vision Acquired Before&After P3 22 Female Totally blind Congenital Never P4 23 Male Low vision Acquired Before&After P5 26 Male Totally blind Acquired Before P6 28 Male Totally blind Congenital Never P7 29 Female Totally blind Congenital Never P8 35 Female Totally blind Congenital Never P9 25 Male Totally blind Acquired Never P10 28 Male Low vision Congenital Never P11 23 Male Totally blind Congenital After P12 53 Male Totally blind Acquired Before P13 29 Male Totally blind Congenital Never P14 23 Female Low vision Acquired Never P15 28 Female Totally blind Congenital Before&After P16 35 Male Totally blind Congenital After P17 23 Female Low vision Acquired Never P18 22 Female Totally blind Congenital Never P19 29 Male Totally blind Congenital Never P20 36 Male Totally blind Congenital Before&After P21 36 Male Totally blind Congenital Never P22 27 Male Low vision Acquired Never,0.1474656862745098,0.1420918828282828,0.8525294666666668,0.46398790707070664,,
246,184,17,Caption,"[(185, 200), (200, 203)]","Table 8: Study Participants (P1-P10: Formative study participants, P11-P22: Evaluation study participants. All participants are screen reader users.)",0.0874259686274504,0.4706221373737369,0.912100352941176,0.4957778707070703,Table,0.0
