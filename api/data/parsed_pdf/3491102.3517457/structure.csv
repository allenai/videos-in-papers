,index,page,type,intervals,text,x1,y1,x2,y2,block_type,block_id
0,1,0,Title,"[(0, 8), (8, 14)]",MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech,0.1132156862745098,0.10411012500000007,0.8875429996732024,0.15100456944444438,,
1,2,0,Author,"[(14, 16), (16, 19), (19, 23), (23, 24), (24, 26), (26, 29), (29, 33), (33, 34), (34, 36), (36, 38), (38, 41), (41, 42), (42, 44), (44, 46), (46, 49), (49, 50), (50, 51), (51, 53), (53, 56), (56, 60), (60, 61), (61, 64), (64, 66), (66, 67), (67, 71), (71, 72), (72, 74), (74, 77), (77, 81), (81, 82), (82, 85), (85, 88), (88, 92), (92, 93)]","Young-Ho Kim University of Maryland College Park, MD, USA yghokim@younghokim.net Diana Chou University of Maryland College Park, MD, USA dchou4@umd.edu Bongshin Lee Microsoft Research Redmond, WA, USA bongshin@microsoft.com Margaret Danilovich CJE SeniorLife Chicago, IL, USA margaret- wente@northwestern.edu Amanda Lazar University of Maryland College Park, MD, USA lazar@umd.edu David E. Conroy Pennsylvania State University University Park, PA, USA conroy@psu.edu Hernisa Kacorri University of Maryland College Park, MD, USA hernisa@umd.edu Eun Kyoung Choe University of Maryland College Park, MD, USA choe@umd.edu",0.1082549019607843,0.16431485050505032,0.8887136228758169,0.3270104618686877,,
2,3,0,Figure,"[(93, 95), (95, 98), (98, 103), (103, 106), (106, 109), (109, 113), (113, 114)]",(a) Default (c) Record screen No interactions for 8 sec (d) Review screen (e) Submission feedback (b) Prompt is active Watchfaces,0.12291099208496727,0.4691320362575757,0.8948384075620917,0.6356333830252525,,
3,4,0,Paragraph,"[(114, 119), (119, 124), (124, 129), (129, 134)]",“Finished half an hour of leisurely walk about a mile and a quarter. Moderate effort because of the heat outside.”,0.343927445627451,0.54978338525,0.5559555259215685,0.6054773578636364,,
4,5,0,Figure,"[(134, 135), (135, 136)]",Voluntaryreporting Promptedreporting,0.2706208122271242,0.37143962647979806,0.33419030827124185,0.5182226009116162,,
5,6,0,Caption,"[(136, 154), (154, 175), (175, 194), (194, 208), (208, 209)]","Figure 1: MyMove supports collecting in-situ activity labels using speech on a smartwatch. People can initiate the reporting from the watchface either voluntarily (a) or upon a prompt message (b); describe an activity, time span, and effort level (c); and review & submit the recording (d). MyMove displays a visual confirmation after the submission (e). The example verbal report is from P7. Please refer to our supplementary video which demonstrates the interactions. ABSTRACT",0.08790522875816993,0.6558432303030303,0.9132242562091506,0.7308156592171717,Figure,1.0
6,7,0,Abstract,"[(209, 210)]","Currentactivitytrackingtechnologiesarelargelytrainedonyoungeradults’data,whichcanleadtosolutionsthatarenotwell-suitedforolderadults.Tobuildactivitytrackersforolderadults,itiscrucialtocollecttrainingdatawiththem.Tothisend,weexamine",0.08790522875816993,0.7362471858585858,0.4850464240522878,0.789078498989899,,
7,8,0,Footnote,"[(210, 241), (241, 270), (270, 287), (287, 297)]","Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9157-3/22/04.",0.08711928104575163,0.8038422921717172,0.4822189222875817,0.8851034032828282,,
8,9,0,Paragraph,"[(297, 298)]",https://doi.org/10.1145/3491102.3517457,0.08790522875816993,0.8863612315656566,0.2727792993464053,0.8951665345959596,,
9,9,0,Paragraph,"[(298, 308), (308, 318), (318, 327), (327, 337), (337, 347), (347, 356), (356, 366), (366, 375), (375, 385), (385, 396), (396, 404), (404, 407)]","the feasibility and challenges with older adults in collecting activ- ity labels by leveraging speech. Specifically, we built MyMove, a speech-based smartwatch app to facilitate the in-situ labeling with a low capture burden. We conducted a 7-day deployment study, where 13 older adults collected their activity labels and smartwatch sensor data, while wearing a thigh-worn activity monitor. Partici- pants were highly engaged, capturing 1,224 verbal reports in total. We extracted 1,885 activities with corresponding effort level and timespan, and examined the usefulness of these reports as activ- ity labels. We discuss the implications of our approach and the collected dataset in supporting older adults through personalized activity tracking technologies.",0.5188316993464052,0.7189504686868687,0.914574910054902,0.8824787515151514,,
10,9,0,Paragraph,"[(407, 409), (409, 411), (411, 413), (413, 414), (414, 415), (415, 416), (416, 418), (418, 420), (420, 421), (421, 423), (423, 424), (424, 425), (425, 427), (427, 428)]",a r X i v : 2204 . 00145v1 [ c s . H C ] 1 A p r 2022,0.02669934640522876,0.2715151515151515,0.059379084967320264,0.7007575757575758,,
11,10,1,Paragraph,"[(0, 10), (10, 27)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.08790522875816993,0.0783156446969697,0.912089879738563,0.0871209477272727,,
12,11,1,Title,"[(27, 29), (29, 37), (37, 47)]",CCS CONCEPTS • Human-centered computing → Empirical studies in ubiq- uitous and mobile computing ; Sound-based input / output .,0.08790522875816993,0.10765139154040401,0.4830950039215686,0.15201410505050497,,
13,12,1,Keywords,"[(47, 48), (48, 56), (56, 59)]","KEYWORDS activity labeling, older adults, smartwatch, speech interaction, ex- perience sampling method",0.08790522875816993,0.1669910380050504,0.4829417374640523,0.2113550141414141,,
14,13,1,Paragraph,"[(59, 71), (71, 93), (93, 117), (117, 123)]","ACM Reference Format: Young-Ho Kim, Diana Chou, Bongshin Lee, Margaret Danilovich, Amanda Lazar, David E. Conroy, Hernisa Kacorri, and Eun Kyoung Choe. 2022. MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech. In CHI Conference on Human Factors in Computing Systems (CHI ’22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 21 pages. https://doi.org/10.1145/3491102.3517457",0.08742320261437908,0.22124115290404034,0.48246502925816986,0.30679928446969706,,
15,14,1,Section,"(123, 125)",1 INTRODUCTION,0.08790522875816993,0.3255844723484849,0.2557663312091503,0.33935858851010114,,
16,15,1,Paragraph,"[(125, 136), (136, 145), (145, 155), (155, 166), (166, 177), (177, 189), (189, 200), (200, 210), (210, 223), (223, 233), (233, 244), (244, 255)]","Scarcity of older adults’ activity datasets may lead to biased and inaccurate activity recognition systems. For example, a recent study showed that Fitbit Ultra, a consumer health tracking device, signifi- cantly under-reports steps at slow speed of 0.9 m/s, a representative walking speed of older adults [132]. When people walk slowly, with a cane, or a walker, such activity recognition systems have a ten- dency to not register steps accurately. A recent study looking at older adults’ technology usage for activity tracking shows that more than a half do not trust the accuracy of these devices [102], which are typically trained on younger adults data. To develop activity tracking systems that are inclusive of and beneficial to older adults, it is imperative to collect older adults’ movements and activity data.",0.08736437908496732,0.34478885252525254,0.4829358038169936,0.5083171353535354,,
17,15,1,Paragraph,"[(255, 262), (262, 271), (271, 281), (281, 291), (291, 302), (302, 310), (310, 318), (318, 328), (328, 338), (338, 353), (353, 364), (364, 372), (372, 383), (383, 388)]","Activity tracking technologies can provide meaningful feedback that supports people’s motivations, playing an important role in enhancing physical activity [32, 80, 122]. Like individuals in many age groups, physical activity is important for older adults, favor- ably influencing their healthy daily routine [27] and active life ex- pectancy [20], chronic health conditions including coronary heart disease, hypertension, and type 2 diabetes [127], psychological health and wellbeing [20], enjoyment [92, 93], and social wellbe- ing [6]. However, the adoption rate of activity tracking technologies for older adults is relatively low ( e.g. , 10% for age 55+ whereas 28% for ages 18–34 and 22% for ages 35–54 [78]). Meanwhile, studies continuously report that younger, more affluent, healthier, and more educated groups are more likely to use activity tracking tech- nologies [18, 75, 78, 126].",0.08790522875816993,0.5108330444444444,0.48293961307189565,0.7020355696969697,,
18,15,1,Paragraph,"[(388, 397), (397, 406), (406, 419), (419, 427), (427, 440), (440, 448), (448, 456), (456, 469), (469, 480), (480, 492), (492, 500)]","We suspect that the current activity tracking technologies are designed with little understanding of older adults’ lifestyles and perspectives ( e.g. , types of activities they engage in and care about) and do not account for heterogeneous physiological characteristics ( e.g. , gait and locomotion [12]). Our ultimate goal is to support older adults’ agency by designing and developing personalized activity tracking technologies that better match their preferences and patterns. As a first step, we set out to develop an activity labeling tool that older adults can use to collect in-situ activity labels along with their sensor data. These labels could be used to train and fine-tune classifiers based on inertial sensors.",0.0874656862745098,0.7045514787878788,0.4813498452287581,0.8542426404040404,,
19,15,1,Paragraph,"[(500, 511), (511, 520), (520, 528)]","To this end, we conducted a 7-day deployment study with 13 older adult participants (age range: 61–90; average: 71.08), where they collected activity descriptions while wearing a smartwatch",0.08790522875816993,0.8567585494949496,0.4804691647581702,0.8957527414141414,,
20,15,1,Paragraph,"[(528, 537), (537, 548), (548, 557), (557, 567), (567, 577), (577, 586), (586, 596), (596, 605), (605, 614), (614, 624), (624, 633), (633, 643), (643, 652), (652, 659), (659, 669), (669, 680), (680, 690), (690, 701), (701, 711), (711, 723), (723, 726)]","and a thigh-worn activity monitor; the thigh-worn activity moni- tor served as a means for collecting ground-truth sensor data for our analysis and later model development. To facilitate collecting in-situ descriptions with a low data capture burden, we designed and developed an Android Wear reporting app, called MyMove , leveraging speech input, an accessible modality for many older adults [98]. With MyMove on a smartwatch, participants can de- scribe activity type, associated timespan, and perceived effort level. Many smartwatches are equipped with a microphone, which al- lows people to flexibly describe their activities using speech. As an on-body device, a smartwatch can collect continuous activity sensing data and deliver notifications, which is necessary to collect in-situ data through an experience sampling method (ESM) [64]. Furthermore, prior work co-designing wearable activity trackers with older adults showed that the “watch-like” form factor was mostly preferred due to its ability to tell time, on-body position, and public acceptance [121]. Through our deployment study, with a focus on feasibility, we explore the following questions: (1) How do older adults capture their activities using speech on a smartwatch? and (2) How useful are their verbal reports as an information source for activity labeling?",0.5189918300653594,0.10956031717171716,0.9145741189019609,0.3976214282828283,,
21,15,1,Paragraph,"[(726, 736), (736, 746), (746, 757), (757, 766), (766, 776), (776, 783), (783, 792), (792, 803), (803, 812), (812, 821), (821, 830), (830, 837), (837, 849), (849, 857), (857, 869), (869, 879), (879, 888), (888, 897), (897, 907), (907, 916), (916, 927), (927, 936), (936, 944), (944, 952), (952, 962)]","Our results show that participants were highly engaged in the data collection process, submitting a total of 1,224 verbal reports (avg. 13.45 reports per day per participant) and wearing the smart- watch and monitor throughout the seven-day study period. From these reports, we extracted 1,885 activities with 29 different activity types that comprehensively capture participants’ daily lifestyles. Participants provided time-related information for about a half of the activities but they were more likely to provide complete time information when reporting a single activity or when reporting voluntarily as opposed to being prompted. Participants’ effort level categories were aligned with sensor-based intensity metrics in the corresponding time segments. However, activities that participants evaluated as moderate to high intensity did not meet the standard in- tensity level according to the sensor-based intensity measurements. All of the 1,224 verbal reports were valid and could be transcribed and understood by a researcher. Furthermore, the word error rates of these reports by two state-of-the-art speech recognition systems were relatively low: 4.93% with Microsoft Cognitive Speech and 8.50% with Google Cloud Speech. Through our study, we demon- strated that by leveraging speech, MyMove can facilitate collecting useful activity labels. We also identified how we can further improve speech-based activity labeling tools for older adults; for example, by leveraging multi-device environments to collect more accurate and fine-grained data and by providing self-monitoring feedback to enhance engagement. The key contributions of this work are:",0.5189918300653594,0.40013733737373736,0.9145732560732027,0.7435469333333333,,
22,16,1,List,"[(962, 972), (972, 982), (982, 992), (992, 997), (997, 1007), (1007, 1015), (1015, 1023), (1023, 1025), (1025, 1035), (1035, 1046)]","(1) Design and development of MyMove, an Android Wear report- ing app for supporting older adults in collecting their activ- ity descriptions with a low data capture burden by leveraging speech input on a smartwatch. (2) Empirical results from a deployment study conducted with 13 older adults using MyMove, demonstrating the feasibility of collecting rich in-situ activity descriptions from older adults via speech. (3) Examining the characteristics and usefulness of the data col- lected with MyMove, in terms of activity type, time, and effort",0.5195343137254902,0.7598987010101009,0.9145732172444445,0.8957527414141414,,
23,17,2,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
24,18,2,Paragraph,"[(24, 35), (35, 38)]",level as well as the quality of the voice recording (automatic speech recognition error).,0.10996241830065359,0.10956031717171716,0.4804690103006537,0.13471865050505044,,
25,19,2,Section,"(38, 41)",2 RELATED WORK,0.08790522875816993,0.15262108851010095,0.25749538790849674,0.16639520467171723,,
26,20,2,Paragraph,"[(41, 55), (55, 63), (63, 73)]","In this section, we cover the related work in the areas of (1) under- standing older adults’ activities, (2) collecting in-situ behavioral data, and (3) in-situ data labeling for human activity recognition.",0.08790522875816993,0.17182673131313136,0.48294118072679754,0.2108221858585858,,
27,21,2,Section,"(73, 78)",2.1 Understanding Older Adults’ Activities,0.08790522875816993,0.22872462386363632,0.4420766468954248,0.24249874002525257,,
28,22,2,Paragraph,"[(78, 85), (85, 94), (94, 102), (102, 111), (111, 120), (120, 129), (129, 140), (140, 148), (148, 157), (157, 166), (166, 176), (176, 185), (185, 197), (197, 208), (208, 216), (216, 227), (227, 229)]","Researchers, healthcare providers, and government officials have been interested in understanding daily activities of older adults because it helps establish and improve health-related guidelines, policies, and interventions [13, 34, 112]. Researchers have defined “activity” differently depending on their research focus. For exam- ple, there is a focus on assessing the independence/dependence with functional tasks, as reflected in the concept of ADL (Activities of Daily Living—basic self-maintenance activities, such as eating, dressing, bathing, or toileting) [56] and IADL (Instrumental ADL— higher-level activities that require complex skills and mental load, such as making a phone call, shopping, housekeeping, or financ- ing) [65]. Another subset of research categorizes activities based on the level of energy expenditure ( c.f. , classification of energy costs of daily activities [1]), as reflected in many physical activity questionnaires they developed to assess older adults’ intensity- specific duration for behavior ( e.g. , MOST [33], CHAMPS [114], LASA [125]).",0.08625,0.24793026666666673,0.4829406629960787,0.4806428929292929,,
29,22,2,Paragraph,"[(229, 239), (239, 251), (251, 262), (262, 275), (275, 285), (285, 296), (296, 308), (308, 317), (317, 327), (327, 341), (341, 357), (357, 361)]","Domestic and leisure activities are prevalent in older adults’ daily activities [49, 55, 79, 83]. According to the national time use surveys from 14 countries, older adults (aged 60–75) spent around 6 hours on leisure and ≥ 2.5 hours on domestic work daily [55]. From the interviews with U.S. older adults, Moss and Lawton found that participants spend about 5 hours a day on obligatory personal care & household activities and more than 6 hours a day on discretionary leisure activities [83]. Another study with Australian older adults reported that participants spend the longest time on solitary leisure ( avg. 4.5 hours a day) excluding sleep, followed by IADL ( avg. 3.1 hours a day), social leisure ( avg. 2.7 hours a day), and ADL ( avg. 2.6 hours a day) [79].",0.0874656862745098,0.483158802020202,0.480853899503268,0.6466870848484848,,
30,22,2,Paragraph,"[(361, 370), (370, 381), (381, 390), (390, 402), (402, 407), (407, 418), (418, 430), (430, 441), (441, 456), (456, 469), (469, 479), (479, 492), (492, 500), (500, 510), (510, 519), (519, 530), (530, 539), (539, 553)]","Researchers have further examined what kinds of activities older adults engage in during their leisure time [55] grouping them as active ( e.g. , relaxing, socializing, volunteering, organization work, religion, going out, sports and exercising) and passive ( e.g. , reading, listeningto theradio,watching television,and browsing theinternet on a computer) with the latter often involving screen time. Screen time is one of the most prevalent leisure time activities [88]; studies consistently report that older adults spend longer than 2 hours a day watching TV ( e.g. , avg. 2.5 hours [49], avg. 3.5 hours [83], and over 3 hours for 54.6% of an older population [43]). Screen time is known to be a strong indicator of discretionary sedentary behav- iors ( i.e. , low energy expenditure activities in a seated or reclined posture while awake [110]). Decreased physical activity during leisure time and increased sedentary time is another common char- acteristic of older adults that may be disproportionately affected by many other factors such as the socioeconomic status of their neighborhood [3]. The U.S. national surveys in 2015–2016 revealed that 64% of older adults aged 65+ reported being inactive ( i.e. , no",0.08790522875816993,0.649202993939394,0.48294657228758187,0.8957753838383838,,
31,22,2,Paragraph,"[(553, 562), (562, 576), (576, 586), (586, 599), (599, 607), (607, 613)]","moderate or vigorous-intensity activity for 10 minutes per day), and 53% reported that they sit longer than 6 hours a day [131]. In a similar vein, a study using an accelerometer sensor (ActiGraph) found that older adults aged 70+ in the urban UK spend less than 30 minutes on moderate-to-vigorous physical activities and the duration significantly drops with age [23].",0.5195343137254902,0.10956031717171716,0.9137122017254903,0.1900658727272727,,
32,22,2,Paragraph,"[(613, 622), (622, 630), (630, 641), (641, 651), (651, 660), (660, 672), (672, 683), (683, 693), (693, 702)]","This body of knowledge—that is typically based on retrospec- tive recall, surveys, and automated sensing—provides a general understanding of older adults’ activities and time use. In our work, however, the purpose of collecting older adults’ activities is quite different: going beyond understanding how older adults spend their time, we aim to examine the feasibility of creating a training dataset that contains older adults’ activity patterns. To this end, we employ a low-burden, in-situ data collection method that older adults can partake in to collect fine-grained data of their activities.",0.5195343137254902,0.19258178181818186,0.9145694054901962,0.314598701010101,,
33,23,2,Section,"(702, 707)",2.2 Collecting In-Situ Behavioral Data,0.5195343137254902,0.3670945733585858,0.8377876983660131,0.380868689520202,,
34,24,2,Paragraph,"[(707, 717), (717, 729), (729, 741), (741, 752), (752, 762), (762, 773), (773, 785), (785, 794), (794, 801), (801, 811), (811, 821), (821, 831), (831, 841), (841, 855), (855, 863), (863, 875), (875, 887), (887, 897), (897, 910), (910, 919), (919, 929), (929, 941), (941, 949), (949, 951)]","Methods that rely on retrospective recall, such as interviews or surveys, are subject to recall bias [42], which may be affected by the nature of an event and people’s experiences. For example, in re- sponding to a survey, people were likely to accurately estimate the past duration of intensive physical activities [9, 52, 109], whereas they were likely to underestimate or omit light and sedentary activi- ties [9, 52, 66, 107, 109]. To collect more ecologically valid self-report data, researchers devised Diary Study [8] and Experience Sampling Method (ESM, often interchangeable with ecological momentary assessment or EMA) [64]. Both methods have been employed be- fore the widespread use of smartphones, but smartphones and their notification capability have made it much easier to facilitate these methods. In Diary Studies, people are expected to capture self- report data once (or more) a day using pen and paper or diary apps. Although Diary Studies help researchers collect in-situ self-report data, there can be a delay between when an event happens and when that event is captured. To further reduce recall bias, ESM em- ploys notifications (defined by a certain prompting rule) to signal when to capture data, and people are expected to capture data at the moment of (or shortly after) receiving the notification. Researchers typically employ ESM to collect brief self-report data frequently . Therefore, in an ESM study, it is important to strike the balance between researchers’ data collection needs and participants’ data capture burdens.",0.5189918300653594,0.38630021616161614,0.9145769988705882,0.715872690909091,,
35,24,2,Paragraph,"[(951, 960), (960, 972), (972, 982), (982, 993), (993, 1001), (1001, 1011), (1011, 1021), (1021, 1031), (1031, 1042), (1042, 1052), (1052, 1065), (1065, 1075), (1075, 1086)]","To reduce data capture burdens, researchers have explored smart- watches as a new means to facilitate ESM [45, 134]; wearing smart- watches allows for high awareness of and alertness to incoming notifications with glanceable feedback [16, 96]. In terms of the no- tification delivery, prior work has demonstrated that smartwatch- based ESM can yield shorter response delays [45], higher response rates, and EMA experiences perceived as less distracting [51, 97] when compared to smartphones. On the other hand, an inherent drawback of smartwatches for ESM is their small form factor, which can make it laborious to enter data. Thus, approaches typically employed on smartphones ( e.g. , entering data via a text box) are inefficient. To ease the data entry, researchers have explored more effective input methods such as the ROAMM [59] and PROMPT [77]",0.5189918300653594,0.7183886,0.9145776387555555,0.8957527414141414,,
36,25,3,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.3363069709150327,0.0871209477272727,,
37,26,3,Bibliography,"[(10, 27)]","Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.46807988464052286,0.0783156446969697,0.912089879738563,0.0871209477272727,,
38,27,3,Paragraph,"[(27, 37), (37, 48), (48, 56)]","frameworks, which support radial scales and bezel rotation to spec- ify pain level and activity type. Others have combined touch and motion gestures for answering Likert scale questions [134].",0.08790522875816993,0.10956031717171716,0.4829358038169936,0.14855577171717174,,
39,27,3,Paragraph,"[(56, 63), (63, 72), (72, 85), (85, 97), (97, 108), (108, 120), (120, 131), (131, 142), (142, 152), (152, 165), (165, 176), (176, 189), (189, 199), (199, 208), (208, 217), (217, 228), (228, 239), (239, 249), (249, 259), (259, 268), (268, 279), (279, 289), (289, 299), (299, 309), (309, 321), (321, 332), (332, 336)]","These prior studies predominantly incorporated graphical wid- gets with touch/hand gestures for structured questions with simple choices ( e.g. , “yes” or “no”). One input modality on a smartwatch that has not been actively considered for ESM on a smartwatch is speech , which is widely embedded in consumer devices and digi- tal systems [22]. When people speak, they tend to be faster [105] and more expressive [17, 103] than when they type. Speech in- put requires little to no screen space and researchers found that speech commands can be easier to perform than using graphical widgets on mobile devices ( e.g. , [60, 113]). Recent work has shown promise for speech input for in-situ data collection on digital de- vices ( e.g. , exercise logging on a smart speaker [72], food journaling on a smartphone [71]). For example, Luo and colleagues deployed a speech-based mobile food journal and found that participants provided detailed and elaborate information on their food decisions and meal contexts, with a low perceived capture burden [71]. Using speech input on a smartwatch poses great potential for lowering the data capture burden while enhancing response rate in EMA studies. It allows us to mitigate touch interactions that involve on-screen finger movement, such as scrolling, which may be burdensome for older adults [5]. Given that voice-based interfaces tend to be accessible for many older adults [98] (including those with low technology experience), in this paper, we explore how older adults leverage speech input on a smartwatch to collect in-situ activity data in an open-ended format. This is a novel approach to prior ESM studies that collected responses to structured questions ( e.g. , multiple choice, Likert scale).",0.08736437908496732,0.15107168080808073,0.4829465471895425,0.5221542565656566,,
40,28,3,Section,"(336, 344)",2.3 In-Situ Data Labeling for Human Activity Recognition,0.08790522875816993,0.5444607349747475,0.4640196241830065,0.5745871238636363,,
41,29,3,Paragraph,"[(344, 354), (354, 364), (364, 374), (374, 385), (385, 394), (394, 403), (403, 415), (415, 429), (429, 439), (439, 452), (452, 464), (464, 478), (478, 489), (489, 499), (499, 504)]","Another relevant topic to our work is Human Activity Recogni- tion (HAR), an automated process of relating sensor stream time segments with various human activities ( e.g. , walking, running, sleeping, eating) [63]. HAR has been extensively applied to a wide range of technologies, from broader consumer fitness trackers to specialized tracking systems for older adults in capturing physical activities and ADLs or detecting falling or frailty [34, 121, 122]. The quality of an HAR model depends on how sensor data ( i.e. , input) were collected [63]; models trained with the sensor data captured in the lab tend to yield less accuracy when tested outside [31]. How- ever, gathering both ground-truth activity labels ( i.e. , the type of activity, the start and end time of an activity) and sensor data in the natural context of daily life is generally challenging because it may not be ethical or feasible for researchers to observe participants’ activity outside the lab [48].",0.08736437908496732,0.5800186505050505,0.48294489286274517,0.7850570343434343,,
42,29,3,Paragraph,"[(504, 516), (516, 525), (525, 535), (535, 547), (547, 557), (557, 568), (568, 579), (579, 592)]","To enable in-situ collection of both the sensor data and the ac- tivity labels, the UbiComp and HCI communities have proposed mobile and wearable systems that allow participants to label their own activities, while collecting sensor data in the background ( e.g. , [82, 108, 119]). For example, VoiSense [108] is a conversational agent on Apple Watch that allows people to capture the physio- logical or motion sensor data for a designated duration and then specify a label for the session, though, it has not been evaluated yet",0.08790522875816993,0.7875729434343434,0.4829438367581701,0.8957527414141414,,
43,29,3,Paragraph,"[(592, 602), (602, 614), (614, 625), (625, 633), (633, 642), (642, 656), (656, 668), (668, 682), (682, 694), (694, 702)]","with users. ExtraSensory App [119] is an in-situ activity labeling system that consists of a mobile and smartwatch app. On the mobile app, people can review their activity history and labels for past or near-future time segments. The smartwatch app complements the mobile app by forwarding notifications or receiving binary confirmations about the current status ( e.g. , “ In the past 2 minutes were you still sitting? ”). When labeling on the mobile app, people can select multiple labels from a predefined list ( e.g. , Sitting + At work ) that best describes the time segment. Data collected with the ExtraSensory App typically include younger adults (ages 18-42).",0.5189918300653594,0.10956031717171716,0.9124718497254902,0.24541435757575758,,
44,29,3,Paragraph,"[(702, 713), (713, 724), (724, 736), (736, 746), (746, 756), (756, 766), (766, 777), (777, 785), (785, 797), (797, 808), (808, 819), (819, 827), (827, 829)]","Our work extends this line of research on collecting in-situ ac- tivity labels in two ways. First, unlike prior systems that primarily target younger adults, we aim to work with older adults with inter- faces that are specifically designed for this population (see Design Rationale DR1 in Section 3.1). Second, unlike VoiSense and ExtraSen- sory App, which collect structured label data through multiple steps of speech or touch inputs, we collect activity information as an unstructured verbal description on the activity type, associated timespan, and perceived level of effort. In doing so, we explore how useful such utterances are as a source of information for activity labeling and discuss the implications of our findings for how to design low-burden in-situ activity labeling systems suitable for older adults.",0.5195343137254902,0.24793026666666673,0.9145673543529412,0.42529567070707075,,
45,30,3,Section,"(829, 831)",3 MYMOVE,0.5195343137254902,0.4414089672979798,0.6267001784313725,0.45518308345959596,,
46,31,3,Paragraph,"[(831, 841), (841, 850), (850, 861)]","As a low-burden activity reporting tool intended for older adults, we designed and developed MyMove (Figure 1), a speech-based Android Wear app. MyMove allows people to submit a verbal de-",0.5189918300653594,0.4606146101010101,0.9145727911006537,0.499608802020202,,
47,31,3,Paragraph,"[(861, 872), (872, 883), (883, 895), (895, 905), (905, 916), (916, 925), (925, 935), (935, 944), (944, 955), (955, 966), (966, 979), (979, 988), (988, 1000), (1000, 1010), (1010, 1015)]","scription of their activities (which we call a verbal report through- out the paper) in two different methods: (i) report voluntarily at any time or (ii) report when they are prompted by ESM notifica- tions. MyMove asks people to include activity , time/duration , and perceived effort level in their verbal report. The activity and associated timespan are the two essential components of labeling sensor data for Human Activity Recognition [63]: activity labels can be extracted from activity descriptions and the timespan connects the activity and the sensor values. Capturing the perceived level of effort is important because it varies from person to person even when they perform the same activity ( e.g. , the number of repetitions, speed, weight lifted) [10]. In the background, MyMove captures sensor data streams and transmits them to a backend server. In this section, we describe our design rationales and the MyMove system along with the implementation details.",0.5189918300653594,0.5021247111111111,0.9145725861437909,0.7071643575757576,,
48,32,3,Section,"(1015, 1018)",3.1 Design Rationales,0.5195343137254902,0.7232776541666667,0.7057555027777777,0.7370517703282827,,
49,33,3,Paragraph,"[(1018, 1028), (1028, 1039), (1039, 1050), (1050, 1064), (1064, 1074), (1074, 1077)]","DR1: Prioritize Older Adults. Both the form factor and interac- tion modalities of MyMove are informed by prior work with older adults in support of smartwatches in the context of activity tracking ( e.g. , [29, 121]), voice as an accessible input modality for many older adults [98], and large target buttons associated with tapping or pressing [15, 84].",0.51909477124183,0.7460615797979798,0.9145755432156863,0.826568397979798,,
50,33,3,Paragraph,"[(1077, 1088), (1088, 1097), (1097, 1105), (1105, 1116), (1116, 1128)]","We carefully selected hardware (i.e., Fossil Gen 5) that has a relatively big display among other smartwatch options with similar sensing. Interacting with Android Wear’s native notifications re- quires bezel swiping and scrolling, and we have little control over the text size and layout of a notification. Thus, we designed and",0.5195343137254902,0.829084307070707,0.9145640097254905,0.8957527414141414,,
51,34,4,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
52,35,4,Paragraph,"[(24, 33), (33, 47), (47, 54), (54, 64), (64, 76), (76, 89), (89, 99), (99, 109), (109, 120), (120, 130), (130, 142)]","implemented a custom watchface to display our prompt messages ( e.g. , Figure 1b). We also allowed people to choose either physical or virtual (touchscreen) buttons for most functionalities, considering diverging preferences of older adults on the physical and virtual buttons [77, 128]. We assigned up to two main functions on each screen and placed virtual buttons with a white background ( e.g. , Fig- ure 1a–1d) near the top-right and the bottom-right physical buttons on the side, with each virtual button matching the corresponding physical button. For consistency, we assigned positive actions ( e.g. , confirm, launch the reporting) to the top-right button and negative actions ( e.g. , cancel, dismiss a prompt) to the bottom-right button.",0.0874656862745098,0.10956031717171716,0.48293643524183,0.25927412121212134,,
53,35,4,Paragraph,"[(142, 151), (151, 161), (161, 171), (171, 181), (181, 194), (194, 205), (205, 215), (215, 227), (227, 236), (236, 245), (245, 255), (255, 265)]","DR2: Simplify Data Capture Flow. Considering that data entry is repeated frequently, we streamlined the user interface flow for activity reporting. For example, people can submit an entry by pressing the top-right button twice, first to initiate the recording (Figure 1a or 1b → Figure 1c), and second to end the recording (Figure 1c → Figure 1d). Upon completion of the recording, the review screen (Figure 1d) automatically submits the report so that people do not have to explicitly press the “OK” button. We followed the design of traditional voice recording interfaces, initially allow- ing pausing and resuming the recording. However, throughout the pilot study we found that pausing/resuming was rarely used but rather made the flow more confusing and therefore removed that",0.0874656862745098,0.2653456707070706,0.48293545219346423,0.42887395353535357,,
54,35,4,Paragraph,"[(265, 266)]",functionality.,0.08790522875816993,0.4313898626262626,0.16743075032679736,0.4427110747474748,,
55,35,4,Paragraph,"[(266, 275), (275, 286), (286, 296), (296, 310), (310, 322), (322, 334), (334, 346), (346, 356), (356, 364), (364, 375), (375, 387), (387, 399), (399, 412), (412, 423), (423, 435), (435, 446), (446, 449)]","DR3: Leverage the Flexibility of Natural Language Speech In- put. It can be challenging to specify activity types or time/duration information using only graphical user interface widgets on a smart- watch. The screen is so small that entering data via a text box can be inefficient. Selecting an activity type from a long list of activi- ties is tedious and prone to error ( e.g. , ExtraSensory’s smartphone app [119] supports about 50 activity tags on a hierarchical list, but its companion smartwatch app does not support this tagging activ- ity). Furthermore, specifying time/duration using touch is laborious and inflexible; a smartwatch’s small screen does not afford two time pickers (for start and end) in one screen and existing time pickers are not flexible enough to handle the various ways to specify time ( i.e. , people should specify absolute start and end time) [60]. We also wanted to allow participants to freely describe the effort level to examine what expressions they use to gauge their effort in what situation instead of using the validated scales such as Borg’s CR10 scale [11, 21].",0.08736437908496732,0.4488178505050505,0.4829479348287583,0.6815191555555555,,
56,35,4,Paragraph,"[(449, 458), (458, 468), (468, 477), (477, 494), (494, 506), (506, 518), (518, 521)]","To mitigate these limitations, we leveraged speech input that affords a high level of freedom without requiring much screen space [60]. People can specify multiple information components in a single verbal report ( e.g. , “ I took a 30-minute walk ” to specify an activity with duration; “ I did gardening, fixing flower beds from 9:00 to 10:30, in moderate intensity ” to specify an activity with duration and effort level).",0.08790522875816993,0.6840350646464647,0.48047030040261435,0.7783790040404039,,
57,36,4,Section,"(521, 524)",3.2 Data Collection,0.08790522875816993,0.7933003814393939,0.25425117843137257,0.8070744976010101,,
58,37,4,Paragraph,"[(524, 533), (533, 545), (545, 558), (558, 569)]",Verbal Activity Reports. MyMove collects verbal reports in two different ways: people can submit a report voluntarily at any time or they can submit a report responding to ESM prompts 1 . Each prompt is scheduled to be delivered at random within hourly time blocks,0.08790522875816993,0.8160843070707071,0.48073903581699345,0.8689168828282828,,
59,38,4,Footnote,"[(569, 581)]",1 Refer to our supplementary video that demonstrates the two reporting methods.,0.08769934640522875,0.8844750296717171,0.4660938333333334,0.8951665345959596,,
60,39,4,Paragraph,"[(581, 592), (592, 602), (602, 613), (613, 624), (624, 636), (636, 647), (647, 653)]","while people are wearing the smartwatch. To send the prompts only when people are wearing the watch, we leveraged the smartwatch’s built-in off-body detect sensor. Once a prompt is delivered, the next one is reserved within the next hour window while leaving at least a 30-minute buffer after the previous one. If the user submits a voluntary report, the next prompt is rescheduled based on the submission time following the same rule.",0.5189918300653594,0.10956031717171716,0.9124717453594771,0.203902993939394,,
61,39,4,Paragraph,"[(653, 661), (661, 671), (671, 683), (683, 694), (694, 705), (705, 716), (716, 729), (729, 740), (740, 750), (750, 761), (761, 765)]","We incorporated custom watchfaces to provide coherent visual interfaces (Figure 1). On the default screen, the watchface displays a clock, the number of reports (logs) that were submitted during the day, and a record button to initiate voluntary reporting (Figure 1a). When a prompt is delivered, the smartwatch notifies the user with two vibrations and displays a message “ Describe in detail what you are doing now. ” with the record and dismiss buttons on the watchface (Figure 1b). The prompt on the watchface stays for 15 minutes. However, for safety reasons, prompts are skipped if the system recognizes that the user is driving based on the Google Activity Recognition API [36].",0.5188316993464052,0.206418903030303,0.9143579436862744,0.3561100646464647,,
62,39,4,Paragraph,"[(765, 776), (776, 777), (777, 785), (785, 796), (796, 807), (807, 818), (818, 829), (829, 841), (841, 853), (853, 863), (863, 873), (873, 885), (885, 897), (897, 908), (908, 917), (917, 925), (925, 926)]","When the user starts the recording by tapping on the “Record” or button on the watchface (or corresponding physical button), the watch vibrates three times while displaying the message, “ Start after buzz ,” to indicate initiation. Then MyMove shows the Record screen (Figure 1c), where people can describe an activity in free- form. The screen displays a message, “ Activity, duration, and effort level? ” to remind people of the information components to be in- cluded. Recordings can be as long as 2 minutes; after which the session is automatically canceled and the audio is discarded. The user completes the recording by pressing the “End” button, after which they are sent to the Review screen (Figure 1d) where they can play back the recorded audio (the Fossil watch had a speaker). The recording is submitted upon pressing the “OK” button or after 8 seconds without any interaction. While recording or reviewing, the user can discard the report using the button.",0.5189918300653594,0.3586259737373737,0.9145695740130718,0.565233802020202,,
63,39,4,Paragraph,"[(926, 934), (934, 943), (943, 953), (953, 959), (959, 968), (968, 979), (979, 989), (989, 997), (997, 1007), (1007, 1022)]","Background Sensor Data. MyMove also collects three behavioral and physiological measurements from the onboard sensors and APIs in the background. First, every minute, MyMove records a 20-second window of inertial sensor measurements—accelerometer, rotation vector, magnetometer, and gravity—in 25 Hz (500 samples each). Second, the system records the step counts in one-minute bins and heart rate samples (BPM) at every minute using the smartwatch’s built-in sensors. Lastly, MyMove collects the classification samples from Google Activity Recognition API, a built-in API that classifies the present locomotion status ( e.g. , walking , running , still in position ,",0.5189918300653594,0.5713292565656566,0.9143474366535947,0.7072059393939393,,
64,39,4,Paragraph,"[(1022, 1033)]","in vehicle , on bicycle ) based on the onboard sensors.",0.5195343137254902,0.7096992060606061,0.8270946274509803,0.7210430606060606,,
65,40,4,Section,"(1033, 1035)",3.3 Implementation,0.5195343137254902,0.7368029066919192,0.6917269705882352,0.7505770228535353,,
66,41,4,Paragraph,"[(1035, 1046), (1046, 1059), (1059, 1071), (1071, 1083), (1083, 1092), (1092, 1103), (1103, 1115), (1115, 1122)]","We implemented the MyMove app in Kotlin [53] on Android Wear OS 2 platform. As a standalone app, it does not require a companion app on the smartphone side. 2 The verbal reports and sensor data are cached in local storage and uploaded to the server when the smartwatch has a stable internet connection. To optimize network traffic and disk space, the MyMove app serializes sensor data using Protocol Buffers [37] and writes them in local files. The server stores the received data in a MySQL database.",0.5188316993464052,0.7560072868686868,0.9124798198588238,0.8641883474747475,,
67,42,4,Footnote,"[(1122, 1136)]",2 The Wear OS 2+ watches can be paired with both iPhone and Android.,0.5195343137254902,0.8844750296717171,0.8538639565359475,0.8951665345959596,,
68,43,5,Header,"[(0, 10), (10, 27)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.08790522875816993,0.0783156446969697,0.912089879738563,0.0871209477272727,,
69,44,5,Section,"(27, 30)",4 DEPLOYMENT STUDY,0.08790522875816993,0.10765139154040401,0.2983153862745099,0.12142550770202028,,
70,45,5,Paragraph,"[(30, 40), (40, 49), (49, 61), (61, 72), (72, 82), (82, 92), (92, 101), (101, 112), (112, 120), (120, 131), (131, 141), (141, 150), (150, 159)]","In May–July 2021, we conducted a deployment study using My- Move to examine the feasibility of speech-based activity labeling on a smartwatch with older adults and the usefulness of the ver- bal reports in activity labeling. As part of this study, participants reported their activities using a smartwatch while also wearing an activPAL activity monitor [89] on their thigh; this monitor served to collect ground-truth activity data to complement those captured by the wrist worn smartwatch. Due to the COVID-19 pandemic, all study sessions (introductory, tutorial, and debriefing sessions) were held remotely using Zoom video calls and the study equipment was delivered and picked up by a researcher, complying with COVID-19 prevention guidelines. This study was approved by the Institutional Review Board of the University of Maryland, College Park.",0.08790522875816993,0.1268557717171717,0.48293492475817,0.30422117575757585,,
71,46,5,Section,"(159, 162)",4.1 Pilot Study,0.08790522875816993,0.3203950784090909,0.21751317990196078,0.3341691945707072,,
72,47,5,Paragraph,"[(162, 176), (176, 188), (188, 199), (199, 209), (209, 221), (221, 231), (231, 243), (243, 255), (255, 264), (264, 272), (272, 280), (280, 291), (291, 301), (301, 310), (310, 320), (320, 329)]","We iterated on the MyMove design ( e.g. , data capture flow) and the study procedure ( e.g. , tutorials) via piloting with two older adults. In an attempt to balance the power structure between older adult participants and our research team, our first pilot participant was a retired HCI researcher. We asked them to follow the study pro- cedure, interact with MyMove and the thigh-worn sensor for 3 days, and provide feedback on the overall study, not as a repre- sentative participant but as someone who is both a member of the intended user group and an expert in human-computer interaction. Their feedback informed our design refinement by significantly simplifying the interaction flows, incorporating icons and labels, as well as adding visual feedback making the consequence of users’ interactions more noticeable. Upon refining the app design and cor- responding tutorial materials, we conducted a second pilot session with another older adult (without any HCI background) to ensure that the watch app and tutorial materials are understandable.",0.08720261437908497,0.3396007212121212,0.48293580381699347,0.5584762262626263,,
73,48,5,Section,"(329, 331)",4.2 Participants,0.08790522875816993,0.5746501289141414,0.22633671666666666,0.5884242450757575,,
74,49,5,Paragraph,"[(331, 342), (342, 352), (352, 363), (363, 373), (373, 386), (386, 396), (396, 406), (406, 416), (416, 427), (427, 438), (438, 448), (448, 461), (461, 472), (472, 484), (484, 492)]","We recruited 13 older adults (P1–P13; 10 females and three males) through various local senior community mailing lists in the North- east region of the United States. Since our study required in-person delivery of the study equipment, we recruited participants in the local area. Our inclusion criteria were adults who (1) are aged 60 or older; (2) feel comfortable describing their activity in English; (3) are curious about their activity levels and interested in collecting activity data; (4) have no severe speech, hearing, motor, movement, or cognitive impairments; (5) have stable home Wi-Fi and are able to join Zoom video calls; and (6) are right-handed. We exclusively recruited right-handed people because Fossil Gen 5 is designed to be worn on the left wrist. The physical buttons are on the right side of the display with the fixed orientation, making it difficult to maneuver the buttons with the left hand. This also helped to minimize the effect of handedness on sensor data.",0.08720261437908497,0.5938545090909091,0.48294189862483655,0.7988941555555555,,
75,49,5,Paragraph,"[(492, 502), (502, 512), (512, 521), (521, 535), (535, 543), (543, 552), (552, 561)]","Table 1 shows the demographic information of our study partic- ipants and the average daily activities during the data collection period, measured by activPAL monitors. All participants were na- tive English speakers and their ages ranged from 61 to 90 ( avg = 71.08). Eight participants were retirees, three were self-employed, and two were full-time employees. Participants had diverse occupa- tional backgrounds and all participants had Bachelor’s or graduate",0.08756862745098039,0.8014100646464647,0.482945942295425,0.8957527414141414,,
76,49,5,Paragraph,"[(561, 573), (573, 584), (584, 587)]",degrees; five had Master’s degrees and one had a Ph.D. All partici- pants were smartphone users; seven used an iPhone and six used an Android phone.,0.5195343137254902,0.10956031717171716,0.914566764109804,0.14855577171717174,,
77,49,5,Paragraph,"[(587, 597), (597, 608), (608, 616), (616, 627), (627, 636), (636, 646), (646, 654), (654, 663), (663, 674), (674, 684), (684, 694), (694, 705), (705, 720), (720, 727), (727, 737), (737, 746)]","The 7-day activePAL sensor data we collected during the study show our participants’ activity level in more detail: Based on ex- isting conventions for interpreting older adults’ physical activ- ity volume (i.e., step counts), many of the participants were “low active” (46%; 5000–7499 steps/day) or “sedentary” (15%; < 5000 steps/day) [117]. The majority of the participants (77%) did not meet the 150 min/week of moderate-to-vigorous physical activity (MVPA) recommended in the 2018 Physical Activity Guidelines for Americans [94]. The average daily physical activity volume ( 𝑀 = 7246.69, 𝑆𝐷 = 2302.42 steps/day) was consistent with reduced all- cause mortality risk from previous studies with older women [67]. The mean duration of sedentary behavior was 10 hours and 44 minutes per day ( 𝑆𝐷 = 2 hours and 33 minutes). This high level of sedentary behavior is comparable to device-measured normative values from older adults (10.1 hours/day) [104] and exceeds self- reported normative values from older adults (6.1 hours/day) [135].",0.5190212418300654,0.15107168080808073,0.9145774955294119,0.3699471858585859,,
78,49,5,Paragraph,"[(746, 754), (754, 768), (768, 778), (778, 787), (787, 797), (797, 809), (809, 821), (821, 834), (834, 846), (846, 854), (854, 867)]","In appreciation for their participation, we offered participants up to $150, but we did not tie the activity reporting to the com- pensation to ensure natural data entry behavior. We provided $25 for completing the adaptation period with the introductory and tutorial sessions, and another $25 for a debriefing interview. During the data collection period, we added $10 for each day of device- wearing compliance ( i.e. , wear the smartwatch for longer than 4 hours/day), and provided an extra $30 as a bonus for all seven days of compliance. We did not specify a minimum amount of time for wearing the activPAL monitor. Compensation was provided after the debriefing session in the form of an Amazon or Target gift card.",0.5189918300653594,0.3724630949494949,0.9145640097254905,0.5221542565656566,,
79,50,5,Section,"(867, 870)",4.3 Study Instrument,0.5195343137254902,0.5469758864898989,0.7024756426470588,0.5607500026515151,,
80,51,5,Paragraph,"[(870, 880), (880, 890), (890, 902), (902, 912), (912, 925), (925, 936), (936, 946), (946, 959), (959, 969), (969, 979), (979, 990), (990, 993)]","We deployed a Fossil Gen 5 Android smartwatch, an activPAL4 device, and a Samsung A21 smartphone to each participant. We chose the Fossil Gen 5 Android smartwatch for its large screen size and extended battery life. The smartwatch has a 1.28-inch AMOLED display with a 416 × 416 resolution (328 PPI). To minimize the effort for the initial set up [91], we deployed smartwatches and Samsung A21 smartphones configured in advance. The phone served as an internet hub for the watch and participants did not have to carry it. While the Bluetooth connection between the watch and the phone was active, the watch periodically uploaded the sensor and verbal reports to our server via the phone’s network connection using the participant’s home Wi-Fi.",0.5188316993464052,0.5661815292929293,0.9143495193725493,0.7297098121212121,,
81,51,5,Paragraph,"[(993, 1002), (1002, 1011), (1011, 1020), (1020, 1031), (1031, 1043), (1043, 1053), (1053, 1064), (1064, 1073), (1073, 1082), (1082, 1093), (1093, 1104), (1104, 1114)]","To collect the ground-truth activity postures, we also deployed activPAL4 [89], which is a research-grade activity monitor that uses data from three accelerometers to classify fine-grained body posture and locomotion ( e.g. , stepping, sitting, lying, standing, in vehicle, and biking). The sensor is attached to the midline of the thigh between the knee and hip using hypoallergenic adhesive tape, and the device does not provide feedback to participants. We chose activPAL for three main reasons: First, activPAL can distinguish different stationary postures such as sitting, lying, and standing, more accurately than the wrist-worn or handheld sensors ( e.g. , Google Activity Recognition API supports only a Still class for a stationary state) [109]. Second, activPAL is pervasive because it has",0.5191683006535948,0.7322257212121212,0.9137122017254903,0.8957527414141414,,
82,52,6,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
83,53,6,Caption,"[(24, 44), (44, 61), (61, 77), (77, 94)]","Table 1: Summary of age and gender of our study participants, their employment status and the latest (or current) occupation, education level, technical proficiency, and the average daily activities measured with an activPAL monitor during the data collection period, including step count, the time spent for moderate-to-vigorous physical activity (MVPA, the total duration at least 100 steps/min), and the time spent sedentary (the time spent sitting and lying while waking).",0.08742156862745099,0.10664373535353525,0.9138833725490197,0.15947631111111082,Table,4.0
84,54,6,Table,"[(94, 97), (97, 102), (102, 103), (103, 105), (105, 108), (108, 109), (109, 112), (112, 114), (114, 115), (115, 117), (117, 118), (118, 121), (121, 122), (122, 127), (127, 128), (128, 131), (131, 132), (132, 135), (135, 136), (136, 139), (139, 141), (141, 142), (142, 144), (144, 145), (145, 148), (148, 149), (149, 153), (153, 154), (154, 157), (157, 158), (158, 161), (161, 162), (162, 165), (165, 167), (167, 168), (168, 171), (171, 172), (172, 175), (175, 176), (176, 179), (179, 181), (181, 182), (182, 184), (184, 185), (185, 188), (188, 189), (189, 192), (192, 194), (194, 195), (195, 198), (198, 199), (199, 202), (202, 203), (203, 208), (208, 212), (212, 213), (213, 216), (216, 217), (217, 220), (220, 222), (222, 225), (225, 226), (226, 229), (229, 233), (233, 236), (236, 237), (237, 239), (239, 240), (240, 243), (243, 247), (247, 254), (254, 255), (255, 258), (258, 262), (262, 264), (264, 265), (265, 267), (267, 268), (268, 271), (271, 275)]","activPAL daily average Participant Employment & Latest occupation Education Tech proficiency Steps MVPA Sedentary P1 61 (M) Retired Senior manager Bachelor’s Very confident 10,941 <1m 11h 23m P2 67 (F) Self-employed Visual artist Bachelor’s Enjoy the challenge 6,192 21m 6h 21m P3 77 (F) Retired Qualitative researcher Ph.D./M.D. Very confident 9,655 2m 10h 53m P4 70 (M) Self-employed Landlord Bachelor’s Enjoy the challenge 7,793 32m 9h 7m P5 81 (F) Retired Disability consultant Master’s A little apprehensive 8,773 23m 7h 48m P6 79 (F) Retired Policy analyst Master’s Very confident 7,320 16m 9h 12m P7 69 (F) Full-time Business manager Master’s Enjoy the challenge 6,499 21m 12h 5m P8 90 (F) Self-employed Piano tutor Master-level Enjoy the challenge 6,281 <1m 12h 24m P9 62 (F) Full-time Communications director Master-level Very confident 5,313 5m 13h 50m P10 62 (F) Retired Human resource specialist Bachelor’s Very confident 3,430 <1m 13h 37m P11 67 (F) Retired Technical training manager Master-level Enjoy the challenge 7,296 2m 7h 19m P12 75 (F) Retired Rehabilitation counselor Master’s Very apprehensive 4,148 9m 13h 58m P13 64 (M) Retired",0.16212418300653594,0.17320059785353548,0.8346199838235294,0.41876504255050506,,
85,55,6,Paragraph,"[(275, 277)]",Regulatory specialist,0.32707426045751636,0.40870178497474746,0.43896821339869285,0.41876504255050506,,
86,56,6,Table,"[(277, 278), (278, 282), (282, 285)]","Master’s Enjoy the challenge 10,566 46m 11h 30m",0.48483496732026143,0.40870178497474746,0.8198388343137255,0.41876504255050506,,
87,57,6,Paragraph,"[(285, 296), (296, 305), (305, 317), (317, 325)]","a long battery life (longer than 3 weeks). Third, activPAL yields equivalent reliability to Actigraph devices for physical activity [62, 73] and is more accurate than them for capturing slower gait speeds, which are common in older adults [44, 106].",0.08736437908496732,0.444554004040404,0.48208242813071905,0.49738531717171713,,
88,58,6,Section,"(325, 328)",4.4 Study Procedure,0.08790522875816993,0.5157233612373737,0.26104262794117655,0.5294974773989899,,
89,59,6,Paragraph,"[(328, 338), (338, 347), (347, 358), (358, 368), (368, 376), (376, 386), (386, 389)]","The study protocol consisted of four parts: (1) introductory session and four-day adaptation period, (2) tutorial session, (3) seven-day data collection, and (4) debriefing. We iterated on the study pro- cedure and tutorial materials through the pilot sessions with two older adults. The introductory, tutorial, and debriefing sessions were held remotely on Zoom. All sessions were recorded using Zoom’s recording feature.",0.08736437908496732,0.5349277414141415,0.48293492475817,0.6292716808080808,,
90,59,6,Paragraph,"[(389, 397), (397, 405), (405, 415), (415, 424), (424, 437), (437, 449), (449, 460), (460, 473), (473, 481), (481, 492), (492, 504), (504, 514), (514, 529), (529, 542)]","Introductory Session & Adaptation Period. After receiving the study equipment, the participant joined a 45-minute introductory session via Zoom. The researcher shared a presentation slide (refer to our supplementary material) via screen sharing. After explaining the goal of the study, the researcher guided the participant to set up the smartphone by connecting it to the home Wi-Fi, wear the smart- watch on the left hand, and attach the activPAL (waterproofed with a nitrile finger cot and medical bandage) to a thigh. To ensure that the participant felt comfortable handling the smartwatch buttons and the touchscreen elements, we used a custom app in MyMove which can be monitored by the researcher on a web dashboard; the participant went through several trials of pressing a correct button following the message on the screen ( e.g. , “ Tap the button [A] on the screen ” or “ Push the button [C] on the side ”).",0.08736437908496732,0.6353658727272727,0.4829358038169936,0.8265910404040404,,
91,59,6,Paragraph,"[(542, 550), (550, 560), (560, 571), (571, 580), (580, 592)]","We incorporated the adaptation period to familiarize participants with charging and wearing the devices regularly. During this pe- riod, which lasted for four days including the day of introductory session, participants were asked to wear the smartwatch during waking hours and the activPAL for as long as possible. The activity",0.08736437908496732,0.829084307070707,0.48294700308496735,0.8957527414141414,,
92,59,6,Paragraph,"[(592, 602), (602, 613), (613, 625)]","reporting feature was disabled and invisible to the participants. At 9:00 PM, an automated text reminder was sent to participants’ own phones to remind them to charge the watch before going to bed.",0.5195343137254902,0.444554004040404,0.912095319529412,0.4835481959595959,,
93,59,6,Paragraph,"[(625, 638), (638, 649), (649, 659), (659, 670), (670, 681), (681, 691), (691, 699), (699, 709), (709, 720), (720, 730), (730, 738), (738, 746), (746, 754), (754, 766), (766, 776), (776, 782)]","Tutorial. On the final day of the adaptation period, we held a 1- hour tutorial session on Zoom to prepare participants for the data collection period starting the next day. The tutorial mainly cov- ered the activity reporting, including a guide on what to describe in a verbal report and how to perform prompted and voluntary reporting with MyMove on a smartwatch. We instructed that the verbal reports are “free-response descriptions about your current or recently-finished activity” and they can be freely and naturally phrased using one or more sentences. We went through 10 example reports with images of performing the activity in five categories— moving and aerobic exercises, strength exercises, stretching and balance exercises, housekeeping, and stationary activities. All ex- ample reports contained the three main information components we are interested in: activity detail, time & duration, and effort level. For each category, we encouraged participants to come up with imaginary reports including those three components.",0.5189918300653594,0.4896436505050505,0.9145690057098038,0.7085191555555556,,
94,59,6,Paragraph,"[(782, 790), (790, 799), (799, 810), (810, 819), (819, 829), (829, 838), (838, 850), (850, 860), (860, 863)]","We covered the activity reporting features by demonstrating example flows using animated presentation slides and asking par- ticipants to practice on their own watch. Since the session was remote, we observed the participant’s smartwatch screen via screen sharing feature of MyMove. We gave participants enough time to practice until they felt comfortable interacting with the smartwatch interface. For the rest of the day, participants were also allowed to submit verbal reports as practice; these reports were not included in the analysis.",0.5195343137254902,0.7110350646464647,0.9145749100549019,0.8330532464646464,,
95,59,6,Paragraph,"[(863, 872), (872, 883), (883, 895)]","We also explained the compensation rule (see the Participants section above) in detail using a few example cases. We emphasized that the compensation would not be tied to the number of reports,",0.5195343137254902,0.835567892929293,0.9137130514823532,0.8745633474747474,,
96,60,7,Header,"[(0, 10), (10, 27)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.08790522875816993,0.0783156446969697,0.912089879738563,0.0871209477272727,,
97,61,7,Paragraph,"[(27, 39), (39, 50)]","but it would depend on the weartime of the smartwatch (i.e., they need to wear the smartwatch at least 4 hours a day.)",0.08790522875816993,0.10956031717171716,0.48085234649934677,0.13471865050505044,,
98,61,7,Paragraph,"[(50, 53), (53, 63), (63, 71), (71, 79), (79, 91), (91, 93)]","DataCollection. Thedayfollowingthetutorial,participants started capturing their activities with MyMove, which lasted for one week. During this data collection period, participants received prompt notifications and the device-wearing compliance guideline was in effect. We also sent charging reminders at night just as during the adaptation period.",0.08790522875816993,0.14081284242424236,0.4833621592679738,0.22131966060606065,,
99,61,7,Paragraph,"[(93, 104), (104, 112), (112, 125), (125, 135), (135, 143), (143, 153), (153, 164), (164, 174), (174, 184), (184, 193)]","Debriefing. After the seventh day of the data collection, we con- ducted a semi-structured debriefing interview with each participant on Zoom for about 40 to 70 minutes. We asked participants to share their general reactions to the interface and smartwatch as well as their experiences with specifying information components, dis- cussing when they would use prompted or voluntary methods, and if they had a preference towards virtual or physical buttons and why. To help participants better recall their experience, we tran- scribed their verbal reports in advance and shared a summarized table (similar format as Table 3) via screen sharing.",0.08736437908496732,0.22741385252525256,0.4829470030849676,0.36326789292929296,,
100,61,7,Paragraph,"[(193, 201), (201, 212), (212, 223), (223, 235), (235, 244)]","Three researchers participated in the debriefing interview ses- sions, two of whom led the interviews: following the detailed inter- view script, each researcher covered about a half of the questions. The third researcher observed nine (out of 13) sessions and filled in one session when the second researcher was not available.",0.0874656862745098,0.365783802020202,0.4829433972287584,0.432453498989899,,
101,62,7,Section,"(244, 247)",4.5 Data Analysis,0.08790522875816993,0.4501173006313131,0.2402582968954248,0.4638914167929293,,
102,63,7,Paragraph,"[(247, 258), (258, 267), (267, 276), (276, 284), (284, 295), (295, 305), (305, 316), (316, 327), (327, 339), (339, 350), (350, 356)]","The study produced a rich dataset including the verbal reports that participants submitted, the sensor data captured from the smart- watch and activPAL, and participants’ feedback from the debriefing interviews. We performed both quantitative and qualitative analysis to examine how older adult participants used MyMove to collect in- situ activity labels and to inspect the characteristics and condition of the collected data. We first examined reporting patterns such as the number of reports collected via two reporting methods as well as audio length and word count of the reports. We analyzed the device usage logs from MyMove and the event logs from activPAL to examine the sensor wearing patterns.",0.08736437908496732,0.4693216808080808,0.48293580381699347,0.6190128424242424,,
103,63,7,Paragraph,"[(356, 366), (366, 374), (374, 385), (385, 396), (396, 406), (406, 416), (416, 425), (425, 435), (435, 445), (445, 455), (455, 465), (465, 475), (475, 486), (486, 491)]","We then analyzed the transcripts from the verbal reports to understand the semantics of activities participants captured. Two authors first independently coded a subset of reports after the data collection of the first four participants was completed (80 out of 354; 23%). We resolved discrepancies and developed the first version of the codebook. As we obtained additional verbal reports from new participants, we iterated multiple sessions of discussions to improve the codebook. After the codebook was finalized, the first author reviewed the entire dataset. Through a separate analysis, we extracted the effort levels from the reports. Two authors separately coded a subset of reports (180; 14.7%) and resolved discrepancies through a series of discussions. After we determined nine categories and how to code data consistently under these categories, the first author coded the remaining data.",0.08790522875816993,0.6215287515151515,0.4808395415424839,0.8127312767676769,,
104,63,7,Paragraph,"[(491, 501), (501, 510), (510, 520), (520, 532), (532, 541), (541, 554)]",We further analyzed the transcribed reports to check how dili- gently participants reported the time component and how well the self-reported information is aligned with the sensor data. We classified the reports into three categories: (1) No time cues : the report does not include any time-related information; (2) Incom- plete time cues : the report includes time cues that are not enough,0.08790522875816993,0.8152471858585858,0.4829407600522877,0.8957753838383838,,
105,63,7,Paragraph,"[(554, 566), (566, 577), (577, 588), (588, 600), (600, 611), (611, 622), (622, 628)]","to identify the activity timespan; and (3) Complete time cues : the report includes time cues that are sufficient to identify the activity timespan. For example, one of P8’s prompted reports, “ I’m just finished fixing a little dinner. ” has time-related information ( i.e. , end time) but we cannot determine the timespan for this activity without the start time or duration. Therefore, this report is classified into the Incomplete time cues category.",0.5189918300653594,0.10956031717171716,0.9137059806535949,0.203902993939394,,
106,63,7,Paragraph,"[(628, 637), (637, 647), (647, 657), (657, 667), (667, 675), (675, 685), (685, 694), (694, 704), (704, 712), (712, 721), (721, 723)]","We transcribed the audio recordings of the debriefing interviews. The three researchers who conducted the interviews led the anal- ysis of the debriefing interview data, using NVivo (a qualitative data analysis tool). We grouped the data specific to participants’ usability-related experiences with MyMove according to the follow- ing aspects: (1) reactions to MyMove and smartwatch, (2) reactions to specifying information components, (3) reactions to using vol- untary and prompted methods, and (4) notions on choosing virtual versus physical buttons. When appropriate, we also referenced this information while interpreting the results from the analyses mentioned above.",0.51909477124183,0.206418903030303,0.9145720839947715,0.3561100646464647,,
107,64,7,Section,"(723, 725)",5 RESULTS,0.5195343137254902,0.3809316945707071,0.6230638117647058,0.3947058107323233,,
108,65,7,Paragraph,"[(725, 738), (738, 746), (746, 756), (756, 768), (768, 778), (778, 787), (787, 800), (800, 808), (808, 817), (817, 827), (827, 837), (837, 848), (848, 859), (859, 870), (870, 880), (880, 892), (892, 902), (902, 910), (910, 919), (919, 929), (929, 939), (939, 944)]","We report the results of our study in six parts, aiming to answer the two research questions—first, to demonstrate the feasibility of collecting the activity reports using speech on a smartwatch; and second, to examine the usefulness of the verbal reports as an information source for activity labeling. In Section 5.1, we provide an overview of the collected dataset, including participants’ engage- ment in capturing the data. In Section 5.2, we report the types of activities that participants captured. We specifically discuss how participants’ lifestyles and other study contexts affect the reporting patterns and behaviors. In Section 5.3, we report how participants describe the time information in their verbal reports and discuss how the nature of an activity and reporting methods affect the completeness of the time cues. We also explore how the verbally- reported activities are aligned with those detected by sensors on a timeline. In Section 5.4, we explore how participants described their effort level, and assess the validity of the effort level description in relation to the device-based intensity measures. In Section 5.5, we examine the accuracy of automatic speech recognition technologies in recognizing older adult participants’ verbal reports. We further investigate the erroneous instances in detail. Lastly, in Section 5.6, we report on participants’ experience with MyMove, based on the qualitative analysis of debriefing interviews.",0.5188316993464052,0.40013733737373736,0.9145727124078434,0.7020355696969697,,
109,66,7,Section,"(944, 947)",5.1 Dataset Overview,0.5195343137254902,0.7268571996212122,0.7014061230392157,0.7406313157828283,,
110,67,7,Paragraph,"[(947, 957), (957, 971), (971, 981), (981, 991), (991, 1007), (1007, 1017), (1017, 1023)]","While the minimum requirement was to wear the smartwatch and activPAL for at least five days (longer than four hours a day for the smartwatch), all 13 participants wore both devices for the entire seven days. On average, participants wore the smartwatch for 11.6 hours per day ( 𝑆𝐷 = 1.3, 𝑚𝑖𝑛 = 9.7 [P11], 𝑚𝑎𝑥 = 13.6 [P13]), and activPAL for 23.3 hours per day (10 participants continuously wore activPAL for the entire study period).",0.5188316993464052,0.7460615797979798,0.9121008869019611,0.8404055191919191,,
111,67,7,Paragraph,"[(1023, 1033), (1033, 1043)]","We collected 1,224 verbal reports in total, consisting of 617 prompted and 607 voluntary reports: Table 2 shows the verbal",0.5195343137254902,0.8429214282828282,0.9124324134901962,0.8680797616161615,,
112,68,7,Footnote,"[(1043, 1054), (1054, 1062)]","reports by participants. Although the reporting was not tied to the compensation, all participants submitted verbal reports every day.",0.5195343137254902,0.8705956707070707,0.9143489333333336,0.8957527414141414,,
113,69,8,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
114,70,8,Caption,"[(24, 43), (43, 52)]",Table 2: The number of prompted and voluntary reports submitted by each participant. The cell color intensity indicates the ratio between the two reporting methods for each participant.,0.08742156862745099,0.10664373535353525,0.9120959529411763,0.13180206868686856,Figure,14.0
115,71,8,Table,"[(52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 88), (88, 89), (89, 90), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 102), (102, 103), (103, 104), (104, 105), (105, 106), (106, 107), (107, 108), (108, 109), (109, 110), (110, 111), (111, 112), (112, 113), (113, 114), (114, 115), (115, 116), (116, 117)]",Method Total P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 Prompted 617 32 66 64 59 57 46 44 33 21 77 13 55 50 Voluntary 607 37 20 67 9 55 204 28 62 30 12 25 14 44 Total 1224 69 86 131 68 112 250 72 95 51 89 38 69 94 0% 100% 75% 25% 50%,0.15151470588235294,0.14183616535984855,0.8487560458431371,0.2088713964709597,,
116,72,8,Paragraph,"[(117, 127), (127, 143), (143, 155), (155, 168), (168, 178), (178, 185)]","Participants submitted 94.15 reports on average, with a high vari- ance among them ( 𝑆𝐷 = 52.85, 𝑚𝑖𝑛 = 51 [P9], 𝑚𝑎𝑥 = 250 [P6]). The average audio length of and word count in each report were 18.65 seconds ( 𝑆𝐷 = 13.65) and 32.05 words ( 𝑆𝐷 = 26.15), respectively. The average audio length per report of each participant ranged from 10.08 [P13] to 32.03 [P12] seconds.",0.0874656862745098,0.2349858222222222,0.48294704111895453,0.31549137777777775,,
117,72,8,Paragraph,"[(185, 194), (194, 205), (205, 217), (217, 229), (229, 243), (243, 252), (252, 262)]","As participants often specified multiple activities in a single report, we extracted activities from each report, a unit of continuous task that can be coded with one or (sometimes) two semantics. For example, the report “ Spent the last 12 minutes, eating breakfast , seated in front of the TV . Minimal level of effort. ” [P6], specifies two simultaneous activities. We identified 1,885 activities from 1,224 verbal reports, and grouped them into the following four categories:",0.08753921568627451,0.3180072868686869,0.48219648376470603,0.41235122626262627,,
118,73,8,List,"[(262, 272), (272, 283), (283, 289), (289, 298), (298, 305), (305, 317), (317, 324)]","(1) Singleton : 760 (62.10%) reports contained a single activity, (2) Sequential : 303 (24.75%) reports contained a series of activities ( avg. 2.50 activities per report), (3) Multitasking : 127 (10.38%) had multiple activities performed simultaneously ( avg. 2.09 activities per report), (4) Compound : 34 (2.78%) were a mix of singleton, sequential, or multitasking ( avg. 3.06 activities per report).",0.08790522875816992,0.4280009737373737,0.48074736742483665,0.5223675555555556,,
119,74,8,Section,"(324, 327)",5.2 Captured Activities,0.08790522875816993,0.5469758864898989,0.28503551781045744,0.5607500026515151,,
120,75,8,Paragraph,"[(327, 338), (338, 347), (347, 358), (358, 370), (370, 380), (380, 389), (389, 404)]","From the 1,885 activities, we identified 29 activity types and grouped them into nine high-level semantics: housekeeping , self-maintenance , non-exercise stepping , screen time , exercise , paperwork/desk work , hobby/leisure , resting , and social (Table 3). The activity types were generally consistent with prior work in daily activities of older adults [49, 83]. Each participant captured 19.08 unique activity types on average ( 𝑆𝐷 = 4.35, 𝑚𝑖𝑛 = 12 [P11], 𝑚𝑎𝑥 = 26 [P3]).",0.08790522875816993,0.5661815292929293,0.48207705202614376,0.6605242060606061,,
121,75,8,Paragraph,"[(404, 411), (411, 420), (420, 428), (428, 438), (438, 452), (452, 461), (461, 475), (475, 482)]","Participants frequently captured housekeeping activities such as cleaning , arranging or carrying items. These activities included straightening rooms, vacuuming, washing the dishes, or carrying goods purchased from shopping. Twelve out of 13 participants were living in a house with a yard and 11 of them captured gardening ac- tivities. However, specific tasks varied, ranging from light activities ( e.g. , watering flowers) to heavy activities ( e.g. , fixing flower beds, planting trees). Participants also frequently captured non-exercise",0.0874656862745098,0.6630401151515152,0.4829421389803922,0.7712199131313131,,
122,75,8,Paragraph,"[(482, 491), (491, 502), (502, 513), (513, 527), (527, 536), (536, 545), (545, 555), (555, 565), (565, 575)]","stepping , which involves a lightweight physical activity, mostly brief in nature. For example, these activities included going up & down the stairs, walking around the kitchen at home, and walking to/from a car, as well as pushing a shopping cart in a store. Eleven participants regularly engaged in cardio exercise , which includes walking, biking, and swimming. The most common exercise was taking a walk (including walking the dog) whereas more strenuous exercise such as running was rarely captured. Eight participants en- gaged in strength and stretching exercises , for example, online",0.08736437908496732,0.7737131797979798,0.48293580381699347,0.8957527414141414,,
123,75,8,Paragraph,"[(575, 584), (584, 596), (596, 607), (607, 615)]","yoga classes. Participants also captured brief strength and stretch- ing exercises ( e.g. , leg lifts) they performed during other stationary activities such as TV watching or artwork. Other types of exercises included online meditation sessions, breathing exercises, and golf.",0.5195343137254902,0.2349858222222222,0.9145686980392159,0.2878183979797981,,
124,75,8,Paragraph,"[(615, 622), (622, 631), (631, 642), (642, 654), (654, 668), (668, 681), (681, 694), (694, 706), (706, 720), (720, 734), (734, 744), (744, 752), (752, 766), (766, 777), (777, 790), (790, 802), (802, 814), (814, 823), (823, 834), (834, 837)]","During debriefing, participants mentioned factors that affected their engagement in specific activities. Gardening was often affected by the season and weather. For example, P1, who participated in the study in mid May, noted that he engaged in gardening more than usual: “ This was a high active seven days for me [sic]. Both because of weather and the time of year, we’re trying to transition the garden. ” In contrast, P9, who participated in the study in late June, seldom captured gardening and noted, “ It was really hot, stinky hot and, you know, not a fun thing to do [gardening] (...) in the earlier in the spring when I planted all my flowers and stuff, that feels more like gardening. ” In addition, the COVID-19 lockdown reduced the overall engagement in outdoor physical activities and in-person activities. P4 noted, “ I would bike downtown two or three times a week anyhow. Normally if before COVID, I’ve been down maybe four or five times for the last year. ” Similarly, P11 remarked, “ In pre-COVID, I would have done that [swimming] probably twice, two or three times during the week. ” Many participants were involved in one or more community activities and their meetings transitioned to Zoom due to the lockdown, possibly increasing their screen time in place of the face-to-face interactions.",0.5195343137254902,0.29033304444444447,0.9137080200261438,0.564558296969697,,
125,75,8,Paragraph,"[(837, 847), (847, 859), (859, 870), (870, 882), (882, 895), (895, 910), (910, 922), (922, 932), (932, 943), (943, 952), (952, 961), (961, 976), (976, 990), (990, 1001)]","We learned that some activities were inherently easier to capture than others due to the contexts in which they are performed: this may have led to oversampling of those activities. For example, P3 commented on her high number of reports of watching TV: “ That [watching TV] had so many times because I was sitting down and it was easy to use the watch. You know, I was taking a break, and the break allowed me to do that. ” In addition, common activities were likely to be overlooked, thereby affecting the data capture behavior. For example, P11, who lives with her grandchildren, noted that she did not capture face-to-face interactions with them because such events happened throughout the day, which makes it overwhelming to capture all of them thoroughly: “ If I recorded what I do with my grandkids, I would be recording all day [laughs]. A lot of times that I interact with my grandkids is kind of in short verse. ”",0.5195343137254902,0.5670742060606061,0.9143495193725493,0.7582981111111111,,
126,76,8,Section,"(1001, 1006)",5.3 Reporting Patterns for Time,0.5195343137254902,0.7822044218434343,0.7894988880718954,0.7959785380050506,,
127,77,8,Paragraph,"[(1006, 1017), (1017, 1028), (1028, 1038), (1038, 1050), (1050, 1061), (1061, 1072), (1072, 1080)]","Table 4 summarizes the time cue categories of activities from Sin- gleton , Sequential , and Multitasking reports. We excluded 34 Com- pound reports (104 activities) because it was infeasible to reliably extract time cues for each activity. Overall, 984 out of 1781 activities (55.25%) were mapped with time cues, and 770 of them (78.25%) were mapped with Complete time cues . The remaining 796 activities (44.69%) were not mapped with any time cues.",0.5189918300653594,0.8014100646464647,0.9135603541542483,0.8957527414141414,,
128,78,9,Header,"[(0, 10), (10, 27)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.08790522875816993,0.0783156446969697,0.912089879738563,0.0871209477272727,,
129,79,9,Caption,"[(27, 46), (46, 65)]","Table 3: Nine activity semantics and 29 activity types, number of reports and participants (Ps), and example snippets from reports. Because the activity semantics and types were multi-coded, the percentages of reports add up to more than 100%.",0.08742156862745099,0.10664373535353525,0.9120959529411767,0.13180206868686856,Table,2.0
130,80,9,Table,"[(65, 66), (66, 70), (70, 71), (71, 94), (94, 96), (96, 122), (122, 139), (139, 140), (140, 166), (166, 169), (169, 190), (190, 192), (192, 220), (220, 221), (221, 237), (237, 240), (240, 253), (253, 254), (254, 282), (282, 284), (284, 295), (295, 296), (296, 323), (323, 325), (325, 344), (344, 347), (347, 365), (365, 366), (366, 387), (387, 389), (389, 405), (405, 422), (422, 423), (423, 424), (424, 450), (450, 484), (484, 485), (485, 519), (519, 521), (521, 536), (536, 537), (537, 540), (540, 563), (563, 567), (567, 585), (585, 586), (586, 602), (602, 623), (623, 627), (627, 628), (628, 642), (642, 643), (643, 644), (644, 662), (662, 663), (663, 681), (681, 682), (682, 683), (683, 705), (705, 707), (707, 724)]","Semantics/types Reports Ps Example snippet House-keeping Cleaning/arranging/carrying 263 21% 13 “ I’ve been doing some house cleaning which includes vacuuming. And now I’m polishing and dusting. ” – P3 Preparing food 123 10% 13 “ I’m in the kitchen and I am just preparing breakfast, so I’m standing at the stove and the toaster. ” – P5 Driving/in a vehicle 108 9% 12 “ Just completed a 30-minute drive, as sitting. ” – P1 Gardening 99 8% 11 “ I’m picking lettuce in my garden, stooping over. It’s not exerting, but it is then a bending and stooping. ” – P5 Caring for pets 68 6% 7 “ Fed dog, bending over to get food and vegetables and reaching to get pills. ” – P6 Offline shopping 36 3% 11 “ At Lowe’s, hardware in the garden section. Walking, pushing a stroller and picking up items and plant, for about 40 minutes. ” – P3 Other 12 1% 6 “ I have just been doing some light housekeeping chores. ” – P7 Self-maintenance Eating food 186 15% 13 “ Ate breakfast from 6:30 until 7:03. ” – P13 Dressing 36 3% 9 “ Process of getting dressed for the day. Pulling my clothes together and getting ready for what I’m going to do today. ” – P10 Personal hygiene 24 2% 8 “ Just completed a shower. ” – P6 Treatment 10 1% 6 “ From 11:45 to 12:45 I had a massage. So I was laying down and there was no intensity level whatsoever. ” – P9 Non-exercise stepping 171 14% 12 “ I’m just walking up the stairs to just do some minor things. ” – P5 Screen time Computer 164 13% 11 “ I’m on the computer. I’m looking at all the sales offers. ” – P12 TV 151 12% 12 “ I’m watching TV, just I’ve been watching it for maybe 10 minutes so far. ” – P2 Mobile device 27 2% 4 “ I’m sitting, looking at a webinar on the phone. ” – P4 Device unspecified 17 1% 5 “ I am sitting in watching videos on YouTube. ” – P10 Exercise Cardio 118 10% 11 “ I just returned from a 30 minute walk, fairly easy paced, moderate effort because of the heat and humidity. ” – P7 Strength/stretching 51 4% 8 “ I am doing stretching exercises in preparation for my strength training class, which I will be taking and I’ve been doing stretching for about 10 minutes. ” – P10 Other 10 1% 4 “ I’ve just finished an hour and a half long workshop on meditation. ” – P3 “ In the 4th hole in the golf course, do playing golf. ” – P1 Paperwork/desk work 68 6% 10 “ balancing my checkbook and writing checks for bills. ” – P12 Hobby/leisure Reading on paper 59 5% 10 “ I’m lying on my bed, reading a book. I’ve been doing that for half an hour. ” – P2 Playing puzzle/ table game 17 1% 6 “ I’m sitting at the counter in the kitchen doing a Sudoku. ” – P5 Crafting/artwork 15 1% 4 “ I’ve been working, doing some woodworking in the basement. – P13 ” Seeing at a theater 11 1% 3 “ I’ve been seated at a concert for the past two hours. – P5 Playing a musical instrument 8 1% 2 “ I am sitting at my piano, playing the piano. – P10 Resting Nothing/waiting 54 4% 12 “ For the last two hours, I’ve been sitting, getting my car serviced. – P9 Napping 19 2% 7 “ Since the last ping I took about half an hour nap. ” – P7 Social Face-to-faceinteraction 39 3% 9 “ I just sat down on my front porch swing and I’m talking to a friend. ” – P3 Voice call 36 3% 8 “ I just completed a telephone call, regarding a personal business. ” – P6",0.089859477124183,0.14376625441919202,0.914577097711111,0.6701223657828284,,
131,81,9,Paragraph,"[(724, 734), (734, 742), (742, 751), (751, 761), (761, 771), (771, 782), (782, 791), (791, 807), (807, 819), (819, 830), (830, 842), (842, 853)]","Reports containing a single activity were more likely to include Complete time cues than reports containing multiple activities: 64.87% (493/760) of Singleton activities were mapped with Complete time cues, compared with 20.11% (152/756) for Sequential and 47.17% (125/265) for Multitasking . Of the 319 activities from Sequential activities with time cues, about a half (167) were mapped with Incomplete time cues because participants often specified the start and end time of the entire sequence ( i.e. , the start time of the first activity and the end time of the last activity). However, this pattern was not consistent across all participants, mainly due to the high individual variance in the number of total reports (See Table 2) and in the portions of Singleton , Multitasking , and Compound activities.",0.08736437908496732,0.6951562767676768,0.4826731056104576,0.858707202020202,,
132,81,9,Paragraph,"[(853, 862), (862, 871)]",Voluntary reports were more likely to include Complete time cues than prompted reports: 45.60% (409/897) of activities from,0.08790522875816993,0.8612004686868686,0.4804691647581699,0.8863588020202019,,
133,81,9,Paragraph,"[(871, 880), (880, 888), (888, 899), (899, 906), (906, 915), (915, 926), (926, 935), (935, 939)]","voluntary reports were mapped with Complete time cues, whereas 40.84% (361/884) from prompted reports. Participants were more likely to omit time cues in prompted reports, especially when re- porting simultaneous activities: 61.36% (108/176) of Multitasking activities from prompted reports contained Incomplete or No time cues, in comparison with 35.94% (32/89) of those from voluntary re- ports. Again, these patterns were not consistent across participants with high individual variance.",0.5189918300653594,0.6951562767676768,0.9145648887843139,0.8033373373737374,,
134,81,9,Paragraph,"[(939, 949), (949, 959), (959, 969), (969, 978), (978, 988), (988, 996)]","Regarding the reports with Complete time cues , we investigated how time segments from verbal reports are aligned with those detected by activPAL. Figure 2 shows the excerpts of timelines with self-report time segments of selected activities, along with the inferred activities and step counts from activPAL. Time seg- ments from verbal reports for locomotion-based cardio exercises",0.5189918300653594,0.8058532464646465,0.9145640097254905,0.8863588020202019,,
135,82,10,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
136,83,10,Caption,"[(24, 45)]","Table 4: Number of activities in Singleton , Sequential , and Multitasking reports by reporting method and the time cue category.",0.08742156862745099,0.10664373535353525,0.9121002666666664,0.11799891111111104,Table,7.0
137,84,10,Table,"[(45, 48), (48, 50), (50, 52), (52, 55), (55, 58), (58, 61), (61, 62), (62, 66), (66, 70), (70, 74), (74, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 88), (88, 89), (89, 90), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 102), (102, 103), (103, 104)]",Singleton reports (=activities) Sequential activities Multitasking activities With time cue With time cue With time cue Method Complete Incomplete No cues Complete Incomplete No cues Complete Incomplete No cues Prompted 226 14 131 67 59 211 68 4 104 Voluntary 267 27 95 85 108 226 57 3 29 Total 493 41 226 152 167 437 125 7 133,0.16450653594771242,0.13167917095959591,0.832238297875817,0.22592792133838388,,
138,85,10,Paragraph,"[(104, 115), (115, 126), (126, 138), (138, 149), (149, 159), (159, 170), (170, 180), (180, 189), (189, 200), (200, 212), (212, 215)]","such as biking and taking a walk generally corresponded with the bands with an equivalent activPAL class and clusters of peaks in step counts. For example, the red segments in Figure 2a and or- ange segments in Figure 2b illustrate how they are aligned with activPAL’s Biking and Stepping bands. Other kinds of walking ac- tivities from verbal reports, such as walking a dog and moving in a store also corresponded with the activPAL activity patterns, but participants’ movement was more fragmented with the Stand- ing and Stepping classes compared to a pure walking exercise (see the orange segments in Figure 2c and 2d which also overlap with activPAL’s Standing band).",0.08790522875816993,0.25171562020202015,0.4829432573803921,0.4014294242424242,,
139,85,10,Paragraph,"[(215, 224), (224, 233), (233, 245), (245, 253), (253, 265), (265, 271)]","Activities performed while sitting often did not correspond with the momentary changes in the activPAL activities. For example, blue segments in Figure 2e and 2f indicate screen time and desk work activities that participants reported performing while sitting. In all cases, the bands of activPAL’s Sitting class cover a wider region than the self-report time segments.",0.08736437908496732,0.4039226909090909,0.482727238379085,0.4844295090909091,,
140,86,10,Section,"(271, 277)",5.4 Reporting Patterns for Effort Level,0.08790522875816993,0.5092296743686868,0.41024061323529415,0.523003790530303,,
141,87,10,Paragraph,"[(277, 289), (289, 301)]","About a half of reports (644 out of 1,224 reports, 52.61%) contained cues on the effort level (see Table 5), with high variance among",0.0873921568627451,0.5284340545454546,0.4804691647581699,0.5535923878787878,,
142,87,10,Paragraph,"[(301, 314), (314, 325), (325, 340), (340, 349), (349, 359), (359, 368), (368, 378), (378, 389), (389, 397), (397, 405), (405, 414), (414, 427), (427, 437), (437, 451), (451, 463)]","participants ( 𝑆𝐷 = 31.19%; 𝑚𝑖𝑛 = 5.26% [P8], 𝑚𝑎𝑥 = 98.04% [P9]). We grouped the effort level cues into seven orderly categories on a spectrum of No effort – Low – Moderate – Strenuous , and two addi- tional categories— Relaxed and Uncategorizable (see Table 6). The most common effort level reported were Low activities (276 reports by 12 participants), followed by Moderate activities (132 reports by 11 participants). The majority of Low activities were stationary activities such as screen time, eating, driving, or desk work, and the Moderate activities included exercises, gardening, or thorough cleaning activities. Strenuous activities were rarely captured (20 reports by 5 participants). The Relaxed category includes responses such as “ I’m sitting totally relaxed , reading my phone and watch- ing TV ”, and the Uncategorizable category covers responses that conveyed ambiguous level of effort ( e.g. , “ Stretches for my back, knee bends. Nothing too strenuous but just to break up the sitting. ”).",0.5188316993464052,0.24920231111111113,0.9145766595555557,0.4567779090909091,,
143,87,10,Paragraph,"[(463, 472), (472, 480), (480, 488), (488, 498), (498, 508), (508, 518)]","To examine how self-report effort level categories are related with device-based intensity measures, we compared intensity mea- surements across the effort level categories using mixed-effects models because these models can handle unbalanced data with re- peated measured from the same participant [95]. For this analysis, we included 480 activities that contained both Complete time cues",0.5189918300653594,0.45927117575757576,0.9145769478901961,0.5397767313131313,,
144,88,10,Figure,"[(518, 519), (519, 521), (521, 522), (522, 523), (523, 524), (524, 526), (526, 527), (527, 528), (528, 530), (530, 532), (532, 533), (533, 535), (535, 536), (536, 537), (537, 541), (541, 545), (545, 549), (549, 554), (554, 557), (557, 560), (560, 564), (564, 565), (565, 566), (566, 567), (567, 568), (568, 569), (569, 570), (570, 571), (571, 572), (572, 575), (575, 576), (576, 577), (577, 578), (578, 580), (580, 581), (581, 582), (582, 583), (583, 584), (584, 586), (586, 588), (588, 589)]",Sitting StandingSteppingBikingIn Vehicle Self-report Steps Sitting StandingSteppingBikingIn Vehicle Self-report Steps 11am 9am P13 P4 P2 P11 P3 P11 P3 P9 P13 P7 P5 (b) Taking a walk (c) Walking a dog (d) Walking in a store (e) Screen time (f) Desk work 9am 12pm 9am 6pm 6pm 3pm 1pm 4pm 7am 6am 3pm 8pm P12 P7 P3 A B C (a) Biking Sitting Standing Stepping Biking In Vehicle 5pm 9pm 10am,0.09398193481862745,0.5725621516073233,0.9042821098104575,0.8321010180429292,,
145,89,10,Caption,"[(589, 609), (609, 634), (634, 643)]","Figure 2: The excerpts of self-report time segments ( B ○ ) of selected activities, along the timeline with automatically-inferred activities ( A ○ ) and step counts ( C ○ ) from activPAL. The colors denote the types of activPAL’s activity classes. The self-report time segments are color-coded as the equivalent activPAL classes.",0.08790522875816993,0.852165507070707,0.9121018901960788,0.8915798464646465,Figure,3.0
146,90,11,Header,"[(0, 10), (10, 27)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.08790522875816993,0.0783156446969697,0.912089879738563,0.0871209477272727,,
147,91,11,Caption,"[(27, 41)]",Table 5: Number of reports with and without effort level cues by each participant.,0.229859477124183,0.10664373535353525,0.7696601986928107,0.1179649474747474,,
148,92,11,Table,"[(41, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 51), (51, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 68), (68, 69), (69, 70), (70, 71), (71, 72), (72, 73), (73, 75), (75, 76), (76, 77), (77, 78), (78, 79), (79, 80), (80, 81), (81, 82), (82, 83), (83, 84), (84, 85), (85, 86), (86, 87), (87, 88), (88, 89), (89, 90), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97), (97, 98), (98, 99), (99, 100), (100, 101), (101, 102), (102, 103), (103, 104), (104, 105), (105, 106), (106, 107), (107, 108), (108, 109)]",Effort level cue Total P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 Included 644 25 75 20 30 30 175 42 5 50 84 27 55 25 Not included 580 44 11 111 38 82 75 30 90 1 5 11 14 69 Total 1224 69 86 131 68 112 250 72 95 51 89 38 69 94 0% 100% 75% 25% 50%,0.1362908496732026,0.12799904414772725,0.8639799020522875,0.1950342752588384,,
149,93,11,Paragraph,"[(109, 120), (120, 129), (129, 144), (144, 161), (161, 168), (168, 179), (179, 190), (190, 201), (201, 213), (213, 221), (221, 230), (230, 241), (241, 250), (250, 263), (263, 272), (272, 281)]","and Effort level cues; we counted two or more activities included in Multitasking reports as one activity because multiple activities ( e.g. , “ watching TV while eating dinner ”) were mapped to one effort level ( e.g. , “ it was very low effort ”). In this analysis, we excluded the Uncateogrizable category. We employed two common indicators of intensity in physical activity research— the percentage of HR 𝑚𝑎𝑥 (the average heart rate during the period expressed as a percent- age of age-adjusted maximum heart rate 3 ) and walking cadence (steps/min) [2, 118]. We generated a model for each of the three measurements—the percentage of HR 𝑚𝑎𝑥 from smartwatch, walk- ing cadence from activPAL, and walking cadence from smartwatch. We used intercept (participant) as a random effect and effort level category as a fixed effect. From Maximum-likelihood tests with other variables, we found that age , elapsed days , and activity types did not have significant effects on the measurements. Therefore, we excluded them from fixed effects in the models.",0.08720261437908497,0.22114870101010106,0.4829437022745099,0.4400254686868687,,
150,93,11,Paragraph,"[(281, 290), (290, 301), (301, 319), (319, 333), (333, 343)]","We found significant differences among the effort level categories in their intensity measurements across all three metrics: 𝐹 (7, 407.69) = 7.32, 𝑝 < .001 for the percentage of HR 𝑚𝑎𝑥 ; 𝐹 (7, 446.69) = 12.00, 𝑝 < .001 for walking cadence from activPAL; and 𝐹 (7, 369.96) = 6.19, 𝑝 < .001 for walking cadence from the smartwatch. We conducted",0.08629411764705881,0.44254011515151515,0.48208378882091507,0.5092098121212121,,
151,94,11,Footnote,"[(343, 374)]",3 We used Nes and colleagues’ formula ( 211 − 0 . 64 ∗ 𝑎𝑔𝑒 ) [87] as an estimate of age- adjusted maximum heart rate to reflect the age-related changes.,0.08790522875816993,0.5305546713383839,0.48239333860000005,0.5521337063131314,,
152,95,11,Paragraph,"[(374, 383), (383, 392), (392, 403), (403, 412), (412, 421), (421, 431), (431, 447), (447, 456), (456, 466), (466, 482), (482, 490), (490, 502), (502, 517), (517, 527)]","post-hoc pairwise comparisons of the least-squared means of in- tensity measurements among 8 effort level categories using Tukey adjustment in emmeans [69] package in R. Figure 3 visualizes the significance over the 95% confidence intervals of measurements in each category. Across all three metrics, the intensity measurements of the activities specified as Moderate were significantly higher than those of No effort ( 𝑝 < .001) and Low ( 𝑝 < .001). The percentage of HR 𝑚𝑎𝑥 and activPAL-measured walking cadence for Low-to- Moderate activities were also significantly higher than those of No effort activities ( 𝑝 = .005 for the percentage of HR 𝑚𝑎𝑥 and 𝑝 = .004 for walking cadence). For Moderate-to-Strenuous activities, only the percentage of HR 𝑚𝑎𝑥 was significantly higher than that of No effort ( 𝑝 = .003) and Low ( 𝑝 = .036) activities. The activities specified as No effort and Low did not differ across all metrics.",0.51909477124183,0.22114870101010106,0.9145640097254906,0.41237386868686865,,
153,95,11,Paragraph,"[(527, 536), (536, 545), (545, 556), (556, 565), (565, 575), (575, 584), (584, 593), (593, 605), (605, 614)]","Participants’ subjective evaluation of the effort level did not match the standard intensity level of physical activity, especially for the activities that are Moderate or above (26.67%; 128/480). Of the 119 Moderate , Moderate-to-Strenuous , and Strenuous activities with the percentage of HR 𝑚𝑎𝑥 measurements, only one activity exceeded the lower bound of standard moderate intensity (64%–76% for moderate-intensity physical activity [2]). Similarly, five (out of 128) and three (out of 113) activities in the same categories exceeded the threshold of moderate intensity walking cadence (100 steps/min",0.5189918300653594,0.4148671353535353,0.9125691322352942,0.5368840545454545,,
154,96,11,Caption,"[(614, 633), (633, 643)]","Table 6: Categories of verbalized effort level cues with the number of reports and participants (Ps), and example phrasings from utterances. The effort level cues are highlighted in bold.",0.08742156862745099,0.5699303515151515,0.912095952941177,0.5950886848484849,Table,3.0
155,97,11,Table,"[(643, 646), (646, 650), (650, 651), (651, 652), (652, 668), (668, 687), (687, 689), (689, 690), (690, 705), (705, 706), (706, 707), (707, 723), (723, 724), (724, 725), (725, 744), (744, 761), (761, 774), (774, 775), (775, 776), (776, 793), (793, 794), (794, 795), (795, 815), (815, 835), (835, 853), (853, 854), (854, 855), (855, 874), (874, 875), (875, 876), (876, 906), (906, 907), (907, 908), (908, 926)]","Effort level category Reports Ps Example phrasings Relaxed 43 9 “ Lying in bed, watching a retirement seminar life. Super relaxed . ” – P4 “ I’m sitting down and the salesperson is helping me try on shoes. Pretty leisurely . ” – P3 No effort 87 8 “ Trying to research something on my computer. No effort . ” – P2 No-to-Low 5 3 “ Standing in the kitchen, preparing lunch. Little to no effort . ” – P1 Low 276 12 “ I’ve been eating for probably about 20 minutes. And effort level is low . ” – P10 “ Had a 15 minute walk with the dog. It was light exertion . ” – P9 “ I’ve been in the kitchen, cooking. Minimal effort . ” – P7 Low-to-Moderate 37 5 “ Cutting material for large raised bed garden. Light to moderate activity . ” – P6 Moderate 132 11 “ In the garden again and bending down, digging holes in the ground. Moderate exertion . ” – P2 “ Thoroughly wiped down stainless refrigerator and cleaned inner seal of doors, 25 minutes. Medium exertion . ” – P6 “ Preparing lunch, heating a bowl of soup up. My activity level is average . ” – P10 Moderate-to-Strenuous 10 2 “ Walking through the airport for about a half hour, medium to heavy intensity . ” – P9 Strenuous 20 5 “ I moved boxes and canned goods and so on into the storage area. Expended a great deal of energy doing that. Was tired afterwards . ” – P12 Uncategorizable 44 8 “ Dressing and cleaning up for about 15 minutes total. Not much effort . ” – P5",0.09191176470588236,0.6096930220959595,0.8181355462418302,0.8897145375,,
156,98,12,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
157,99,12,Paragraph,"[(24, 36)]",*** 𝑝 < .001; ** 𝑝 < .01; * 𝑝 <. 05,0.08790522875816993,0.10539179255050513,0.22723457614379086,0.11656034166666668,,
158,99,12,Paragraph,"[(36, 72), (72, 73), (73, 85), (85, 87), (87, 88), (88, 89), (89, 90), (90, 91), (91, 92), (92, 93), (93, 94), (94, 95), (95, 96), (96, 97)]",R e l a x e d N o e ﬀ o r t N - t o - L L o w L - t o - M M o d e r a t e M - t o - S S t r e nu o u s 36 38 40 42 44 46 48 50 52 54,0.1055970547140523,0.1280403840265151,0.3561983893998464,0.2771069791845152,,
159,99,12,Paragraph,"[(97, 103), (103, 108), (108, 110)]",P e r c e n t a g e ( % ),0.09027842584640523,0.16088853610606055,0.10024607290522876,0.21414975201515155,,
160,99,12,Paragraph,"[(110, 146), (146, 147), (147, 159), (159, 161), (161, 162), (162, 163), (163, 164), (164, 165), (165, 166), (166, 167), (167, 168), (168, 169), (169, 175), (175, 178), (178, 214), (214, 215), (215, 227), (227, 229), (229, 230), (230, 231), (231, 232), (232, 233), (233, 234), (234, 235), (235, 236), (236, 237), (237, 243), (243, 246)]",R e l a x e d N o e ﬀ o r t N - t o - L L o w L - t o - M M o d e r a t e M - t o - S S t r e nu o u s −20 −10 0 10 20 30 40 50 S t e p s / m i n R e l a x e d N o e ﬀ o r t N - t o - L L o w L - t o - M M o d e r a t e M - t o - S S t r e nu o u s −20 −10 0 10 20 30 40 50 S t e p s / m i n,0.36675020506699346,0.12762068718560607,0.9103805775027876,0.2769516628549698,,
161,100,12,Figure,"[(246, 253), (253, 257), (257, 261), (261, 262), (262, 263), (263, 264), (264, 265), (265, 266), (266, 268), (268, 269)]",(a) % of Age-related Maximum HR (Smartwatch) (b) Walking Cadence (activPAL) (c) Walking Cadence (Smartwatch) ** ** *** *** * **** ****** ***,0.09771812809640523,0.11954531636742434,0.8685497664493463,0.2944720944128788,,
162,101,12,Paragraph,"[(269, 270)]",***,0.7562087025375817,0.13326310260606075,0.7758100804787582,0.1448165116969697,,
163,102,12,Caption,"[(270, 286), (286, 305), (305, 321), (321, 332)]",Figure 3: Distributions of device-based intensity measurements during the same time segments for each effort-level category. The colored rectangles denote 95% confidence intervals estimated by the mixed-effects model with a center bar as the least squared mean after controlling the individual differences. The asterisks with arms indicate significance between the connected categories. (We did not mark the pairs that are not significant.),0.08742156862745099,0.3060540888888888,0.9146014483660132,0.3588854020202019,Figure,3.0
164,103,12,Paragraph,"[(332, 342), (342, 347)]","or higher for moderate activity [118]) with the measurements from activPAL and the smartwatch, respectively.",0.08790522875816993,0.38318657979797976,0.4804630221176471,0.40834491313131316,,
165,103,12,Paragraph,"[(347, 355), (355, 367), (367, 378), (378, 380), (380, 393), (393, 401), (401, 412), (412, 425), (425, 440), (440, 451), (451, 461), (461, 476), (476, 488), (488, 497), (497, 508), (508, 522), (522, 534), (534, 546), (546, 554)]","To examine how predictive the device-based intensity measure- ments are for the effort level, we conducted a multiple linear regres- sion analysis using MASS [123] package in R. This method initially addsallpredictors—thethreedevice-basedintensity measurements— to a model and iteratively excludes the predictors that do not make a significant contribution to the prediction, reassessing the contribu- tions of the remaining predictors at each step. We first transformed the seven ordinal categories ( No effort – Strenuous ) into a continuous effort level scale (1–7, with Low as 3 and Moderate as 5) and used it as a dependent variable. For this analysis, we included 349 activities which contain the values of all three measurements. A significant regression equation (see Table 7) was found ( 𝐹 (3, 345) = 15.25, 𝑝 < .0001), with an adjusted 𝑅 2 of .11. Although all three measurements collectively contributed to the prediction and were thus included to the final model, only walking cadence from activPAL was statis- tically significant ( 𝑝 = .0004). The 𝑅 2 value denotes that the model explains only 11% of the variance of the effort level scores. This implies that it may not be feasible to accurately predict the exact effort level score using only the device-based measurements.",0.08736437908496732,0.4108595595959596,0.4847592648366015,0.6712476909090909,,
166,104,12,Caption,"[(554, 564), (564, 573), (573, 585), (585, 594), (594, 598)]","Table 7: Regression model for the effort level score, fitted from the device-based intensity measurements, 𝐹 (3, 345) = 15.25, 𝑝 < .0001, adjusted 𝑅 2 = .11. The positive coefficient denotes that the given parameter is positively correlated to the effort level score.",0.08742156862745099,0.7112687353535353,0.48046359738562094,0.7779371696969696,Table,4.0
167,105,12,Table,"[(598, 599), (599, 601), (601, 605), (605, 606), (606, 607), (607, 609), (609, 611), (611, 614), (614, 615), (615, 617), (617, 619), (619, 623), (623, 625), (625, 626), (626, 631), (631, 632), (632, 634), (634, 635), (635, 647)]",Parameter Coef. 𝑺𝑬 𝒕 -statistic 𝒑 -value Constant -2.00 0.75 -2.67 < .01** Walking cadence (activPAL) -0.02 0.01 -3.55 < .001*** Walking cadence (Smartwatch) -0.01 0.01 -1.43 .15 % of age-related maximum HR -0.03 0.02 -1.71 .09 *** 𝑝 < .001; ** 𝑝 < .01; * 𝑝 < .05,0.09610294117647059,0.7925327063131313,0.4788228114379086,0.8906373618686868,,
168,106,12,Section,"(647, 652)",5.5 Quality of Voice Recording,0.5195343137254903,0.38127765416666665,0.7772528885620917,0.3950517703282828,,
169,107,12,Paragraph,"[(652, 662), (662, 671), (671, 678), (678, 687), (687, 698), (698, 710), (710, 720), (720, 729), (729, 739), (739, 751), (751, 762), (762, 770), (770, 779), (779, 788), (788, 800), (800, 814), (814, 825), (825, 840), (840, 852), (852, 856)]","To investigate the potentials of activity labeling with speech in- put, we assessed how accurately the existing automatic speech recognition (ASR) technologies can recognize participants’ speech inputs, especially since there is prior evidence on disproportionate ASR word error rates for older adults’ voices [19, 124]. Considering the transcribed text of verbal reports by our research team as the ground-truth, we compared it with the output from two commercial ASR services, Microsoft Cognitive Speech [81] and Google Cloud Speech [38]. Using their REST APIs, we retrieved the recognized text from the audio files for each verbal report. We then calculated Word Error Rate (WER) of the recognized text using the human- transcribed text. When calculating WER, we removed punctuation and fixed contractions using NLTK (Natural Language Toolkit) [7] and Contractions Python Library [61]. On average, the Microsoft API recognized reports with an word error rate of 4.93% per report per participant ( 𝑁 = 13, 𝑆𝐷 = 2.12%). This is slightly lower than 5.10% that Microsoft had reported in 2018 [133]. The Google API yielded an error rate of 8.50% per report per participant ( 𝑁 = 13, 𝑆𝐷 = 2.97%). This is 3.60% higher than 4.90% that Google had officially announced in 2017 [100].",0.5188316993464052,0.4004820343434344,0.9145727872418302,0.6747060242424243,,
170,107,12,Paragraph,"[(856, 867), (867, 878), (878, 888), (888, 900), (900, 910), (910, 922), (922, 935), (935, 945), (945, 956), (956, 964), (964, 972), (972, 986), (986, 1002), (1002, 1014), (1014, 1022)]","We performed an error analysis to gain insights into the potential effect these errors may have in automating the retrieval of activity labels from free form verbal reports. Specifically, we manually in- spected a total of 651 verbal reports where there was a disagreement between our ground truth and the best performing ASR service. Many of the errors (70.97%; 462/651) did not affect the words cap- turing activity type, time, or effort level, i.e. , with the local context of the verbal report someone could correctly infer this information if it was reported. Typically, errors in these reports involved filler words, conjunctions, or other details that participants provided along their activity. For example, misrecognized conjunction in the ASR output of P1’s report, “ Eating lunch, Ann [should be and ] about to get on a zoom call, seated, viewing on a laptop for an hour ,” does not affect the coding of activity type (eating food and screen time). Interestingly, some (9.74%; 45/462) disagreements in these",0.5189918300653594,0.6772219333333332,0.9145761260862745,0.8822615797979798,,
171,108,13,Header,"[(0, 10), (10, 27)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.08790522875816993,0.0783156446969697,0.912089879738563,0.0871209477272727,,
172,109,13,Paragraph,"[(27, 37), (37, 49), (49, 63), (63, 72), (72, 83), (83, 86)]","reports were due to background or irrelevant speech being perhaps correctly captured by ASR but being omitted in the ground truth by our team as they were not intended to be part of the verbal report. For example, this would occur when participants were capturing sedentary activities like watching TV and the voice from the TV was also captured.",0.08736437908496732,0.10956031717171716,0.4827247417934641,0.1900658727272727,,
173,109,13,Paragraph,"[(86, 96), (96, 107), (107, 123), (123, 137), (137, 152), (152, 163), (163, 178), (178, 190), (190, 201), (201, 213), (213, 221)]","Even some of the errors involving words that captured activity type could be recoverable. These cases include errors in the verb tenses ( e.g. , “ Just came downstairs and fix [should be fixed ] me some coffee ...” [P8]) or compound words ( e.g. , “ Walked up stairs [should be upstairs ] to second floor ...” [P6]). This was also the case for time and effort level. For example, the ASR service often made formatting errors in recognizing time ( e.g. , “ Read a book from 6:15 until 647 [pronounced ‘six forty-seven’; should be 6:47 ]. ” [P13]), which can be fixed referring to the local context. A disagreement in P6’s report, “... Standing, minimal [should be minimum ] level of exertion ,” does not affect the coding of effort level category.",0.08625,0.19258178181818186,0.4820821204915033,0.34227294343434345,,
174,109,13,Paragraph,"[(221, 233), (233, 245), (245, 257), (257, 270), (270, 283), (283, 294), (294, 308), (308, 321), (321, 331), (331, 335)]","If we had relied solely on the ASR output for their corresponding reports, 82 (out of 651; 12.60%) would have affected our coding of activity type, time, or effort level. For example, it is challenging to extract time from the ASR output of P11’s report, “ Since about 132 frozen 245 [should be 1:30 to present, 2:45 ] ...,” without listening to the audio record. In addition, verbs were sometimes recognized as a totally different one, changing the original meanings in text ( e.g. , “ I am just resting [should be dressing ] after taking a shower ...” [P5]). We anticipate that automated solutions may be more susceptible to some of these errors.",0.08720261437908497,0.34478885252525254,0.4827161854640523,0.4806428929292929,,
175,110,13,Section,"(335, 340)",5.6 Participants’ Experience with MyMove,0.08790522875816993,0.4963445733585858,0.4408823500000001,0.510118689520202,,
176,111,13,Paragraph,"[(340, 349), (349, 359), (359, 367), (367, 377), (377, 385), (385, 393), (393, 403), (403, 415), (415, 425), (425, 436), (436, 442)]","Following the week-long data collection period, we conducted de- briefing interviews and guided participants to reflect on their ex- periences. Their responses helped us understand both strengths and challenges in using MyMove to create verbal activity reports. Participants provided feedback on their experience using MyMove interface and the smartwatch device, specifying information com- ponents for reporting, when they used prompted or voluntary meth- ods, and preferences in using virtual vs. physical buttons. At the end of the debriefing interview, all participants agreed to be contacted for a future follow up session in the project, acknowledging their interest in contributing to this project.",0.08790522875816993,0.5155502161616161,0.4829386754091503,0.6652413777777777,,
177,111,13,Paragraph,"[(442, 451), (451, 461), (461, 471), (471, 480), (480, 490), (490, 503), (503, 516), (516, 527), (527, 542), (542, 553), (553, 564), (564, 577), (577, 587), (587, 597), (597, 607), (607, 622)]","5.6.1 Reactions to the MyMove interface and smartwatch. Partici- pants seemed to have a generally positive experience with MyMove on the smartwatch. Ten participants noted that both the interface and the smartwatch contained features that made reporting easy. For instance, P1 commented the flexibility in having multiple re- porting methods (“ I think it was easy enough to report, because I was allowed to, you know, report it in various ways ”), and explained physical features of the smartwatch that were favorable (“ the size of the screen is good for my age group, and as well as the buttons were relatively, easily to access ”). P5 mentioned how the multiple modalities helped with the reporting process (“ It was very efficient watch. It was nice that you could just either touch [the screen] or the [physical] buttons. ”). Participants also appreciated the text on the screen, indicating the type of information components to in- clude when recording their activity reports (“ I’d remember what information I had to give you so that was very helpful for me. ” [P10]).",0.08790522875816993,0.6768772363636363,0.48294232192418307,0.8957753838383838,,
178,111,13,Paragraph,"[(622, 631), (631, 641), (641, 651), (651, 665), (665, 676), (676, 688), (688, 703), (703, 714), (714, 722), (722, 737), (737, 752), (752, 766), (766, 770)]","On the other hand, participants faced challenges when interact- ing with the system. At the debriefing interview, six participants mentioned that the watch occasionally did not respond to their touch and that they had to click on the Record button a couple of times to start the recording. For example, P3 mentioned, “ There were times when I thought I’d recorded something... it seemed like the watch was telling me I hadn’t recorded it. So I recorded it again. ” We reflect on this challenge and an alternative design in Section 6.6. Some participants expressed concerns with wearing the smartwatch long-term (“ I don’t know if I would want to wear this watch all the time to do it ” [P6]), and with the smartwatch’s battery life (“ I didn’t have any challenges with the watch, except for the fact that it ran out of battery. ” [P9]).",0.5195343137254902,0.10956031717171716,0.9145638381176474,0.2869483636363636,,
179,111,13,Paragraph,"[(770, 778), (778, 787), (787, 796), (796, 808), (808, 822), (822, 836), (836, 848)]","5.6.2 Reactions to specifying information components. When re- flecting on their experience with specifying the activity, timespan, and effort, many participants reacted positively. Several found that describing their activities was relatively easy: (“ It was easier than I thought it would be ” [P10]), (“ I didn’t really have any problems with it. It was pretty straightforward ” [P7]), (“ If you just wanted what I was doing at the moment, I didn’t find that difficult ” [P2]).",0.5195343137254902,0.29635955959595967,0.9145769082352941,0.39072487878787876,,
180,111,13,Paragraph,"[(848, 856), (856, 866), (866, 877), (877, 887), (887, 899), (899, 912), (912, 925), (925, 933), (933, 943), (943, 957), (957, 971), (971, 979), (979, 989), (989, 999), (999, 1008), (1008, 1018), (1018, 1031), (1031, 1045), (1045, 1060)]","Participants also expressed challenges in specifying the level of effort required and the time taken. Seven participants reported having difficulty describing their effort level since it was hard to determine, especially for activities involving multiple tasks (“ In the midst of that activity, I did something else that may have changed the amount of energy required, ...the effort was the hard one for me to actually document that piece of it ” [P1]). To help determine ef- fort, some participants would use physiological indicators such as breathing, muscle strain, tiredness, or even their exercise per- formance (“ I can just look at my Strava recording and give you a time and a speed, which sort of gives you an intensity ” [P4]). Six participants found specifying time components to be challenging, including recalling the amount of time taken or just remembering to add time components to the activity description. Eight partic- ipants utilized different strategies to assist with tracking activity timespan. Methods included using their memory, a device, or even writing the time down in order to remember the time (“ Whenever I started an activity, I would just look at the watch before I started, and then try to record it right afterwards, so I had it right there ” [P13]).",0.5189918300653594,0.3932181454545454,0.9145742557908497,0.6536289191919191,,
181,111,13,Paragraph,"[(1060, 1070), (1070, 1079), (1079, 1088), (1088, 1097), (1097, 1107), (1107, 1115), (1115, 1127), (1127, 1140), (1140, 1152), (1152, 1168), (1168, 1180), (1180, 1190), (1190, 1199), (1199, 1211), (1211, 1224), (1224, 1233), (1233, 1246)]","5.6.3 Situations for using voluntary and prompted methods. In ad- dition, participants described situations in which they would use voluntary and prompted reporting. We found that each method had unique advantages, including having the freedom to report voluntarily at any time and being aware about reporting activities due to prompted notifications. Some participants appreciated the pings because they served as a reminder to record the activity right away or after finishing the activity. P8 commented how “ it was good to have it, because it reminded you that maybe you hadn’t recorded what you were doing, ” and P1 stated, “ Had it not been for the watch’s alert, I’m not quite sure that I would have captured that information as well. ” When asked what participants disliked about receiving ping notifications, six participants said that pings were delivered during inopportune moments (“ There were a couple of times when I just couldn’t answer the ping. That was in a meeting or something ” [P7]). Participants also reported that pings seemed unnecessary for redundant activities (“ I was... reporting the same thing all the time ”",0.5191683006535948,0.6630401151515152,0.9145680758169934,0.8957753838383838,,
182,112,14,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
183,113,14,Paragraph,"[(24, 35), (35, 49), (49, 53)]","[P7]), or too frequent for longer activities which they had already reported earlier (“ I’m just doing the same thing I recorded that I was doing before ” [P8]).",0.08790522875816993,0.10956031717171716,0.48084235453071905,0.1485784141414142,,
184,113,14,Paragraph,"[(53, 62), (62, 70), (70, 81), (81, 91), (91, 103), (103, 117), (117, 133), (133, 143), (143, 155), (155, 170), (170, 180), (180, 190), (190, 198)]","5.6.4 Preferences between virtual vs. physical buttons. Eight partic- ipants preferred using virtual buttons, whereas two participants stated a preference in using the physical buttons. Those who pre- ferred using the virtual explained how virtual buttons were more familiar and convenient (“ I’m very used to using touch screens all the time. My instinct was just natural to go there ” [P5]), and easier to use and understand (“ It was just easier for me to tap to screen ” [P1]). Some even expressed confusion in how the physical buttons would work (“ I wasn’t always sure what [the physical button] was going to do, or... how it was going to respond ” [P7]). As we mentioned in Section 5.6.1, some participants had trouble getting the virtual buttons to respond accordingly due to having difficulty with the wake-up functionality associated with using the virtual buttons.",0.08736437908496732,0.15861839797979801,0.48294503393464056,0.335983802020202,,
185,114,14,Section,"(198, 200)",6 DISCUSSION,0.08790522875816993,0.35011351275252517,0.21984829771241832,0.3638876289141414,,
186,115,14,Paragraph,"[(200, 213), (213, 223), (223, 234), (234, 244), (244, 253), (253, 264), (264, 273), (273, 281), (281, 292), (292, 304), (304, 306)]","In this section, we first reflect on several aspects that are related to the feasibility of MyMove in facilitating data collection with older adults, such as engaging older adults in activity labeling, using the verbal reports as an information source for activity labeling, and capturing older adults’ activities in a comprehensive manner. We also discuss how our findings and the data older adults collected using MyMove can be used toward creating personalized activity trackers that attune to idiosyncratic characteristics of individual users and their unique needs. We then discuss limitations in our study that may affect the generalizability of our findings as well as future work.",0.08753921568627451,0.3693178929292929,0.48084170988758185,0.5190090545454545,,
187,116,14,Section,"(306, 313)",6.1 Engaging Older Adults in Activity Labeling,0.08790522875816993,0.5331387652777777,0.4758021651960784,0.5469128814393939,,
188,117,14,Paragraph,"[(313, 323), (323, 334), (334, 345), (345, 354), (354, 364), (364, 377), (377, 389), (389, 400), (400, 408), (408, 419), (419, 429), (429, 438), (438, 450), (450, 460), (460, 466)]","We were pleasantly surprised by the high adherence and engage- ment of our participants in the one-week data collection. On av- erage, they wore the smartwatch for 11.6 hours and activPAL for nearly 24 hours every day. Furthermore, our participants submitted 13.45 reports per day on average even though the compensation was not tied to the number of reports. Given that the most chal- lenging aspect of an EMA study is its high data capture burden and frequent interruptions [120], we believe that this is a promis- ing outcome. Participants’ positive feedback on MyMove indicates that the system itself may have contributed to this high adherence; it provided flexible ways to capture data using speech, including voluntary and prompted reporting methods as well as simplified data capture flow and UI. Even though all but one experienced the smartwatch for the first time through our study, all participants could use MyMove without much trouble.",0.08720261437908497,0.5523444080808081,0.48294786426143804,0.757382791919192,,
189,117,14,Paragraph,"[(466, 474), (474, 485), (485, 494), (494, 507), (507, 517), (517, 527), (527, 539), (539, 550), (550, 560), (560, 570)]","In debriefing, however, most participants stated that collecting data in this manner would not be sustainable, and the one-week duration would probably be the maximum they could continuously engage at this level. As is common with other ESM studies, our study imposed a high burden on the participants, for example, having them consciously think of activity type, start/end time, and effort level when they receive hourly notifications. In our study, we did not limit the scope of what activities are report-worthy because our goal was to examine the feasibility of collecting in-situ activity labels with older adults using speech on a smartwatch. Going forward,",0.08736437908496732,0.7598987010101009,0.48208025108496755,0.8957527414141414,,
190,117,14,Paragraph,"[(570, 580), (580, 591), (591, 605), (605, 614), (614, 624), (624, 638)]","such comprehensive data capture may not be necessary; we expect that the burden of capturing activity labels would be reduced when we have a fixed set of targeted activities ( e.g. , walk, gardening, golf, yoga) that require labeling and better mechanisms for estimating activity timespan ( e.g. , by automatically detecting abrupt changes in sensor data) and effort level ( e.g. , by leveraging heart rate data).",0.5189918300653594,0.10956031717171716,0.9141941695477125,0.1900885151515152,,
191,118,14,Section,"(638, 649)",6.2 Leveraging Verbal Reports as an Information Source for Activity Labeling,0.5195343137254902,0.2054531592171716,0.8904802218954249,0.23558081073232323,,
192,119,14,Paragraph,"[(649, 663), (663, 673), (673, 681), (681, 691), (691, 704), (704, 713), (713, 722), (722, 731), (731, 740), (740, 748), (748, 757), (757, 766), (766, 778), (778, 787), (787, 797), (797, 807), (807, 820), (820, 830), (830, 842), (842, 853), (853, 866), (866, 876), (876, 886), (886, 898), (898, 909), (909, 916)]","It was encouraging to see that all of the 1,224 verbal reports are valid and that researchers could transcribe and understand all of them. Although some participants, in the debriefing interview, mentioned that they accidentally triggered the recording, it seemed that they were able to cancel it or the recording was timed out and thus erased. This demonstrates that, despite the challenges older adult participants faced with the unfamiliar technologies, they still could successfully submit valid reports using our novel data collection approach. Furthermore, the word error rates by two state-of-the-art automatic speech recognition systems were relatively low: 4.93% with Microsoft Cognitive Speech and 8.50% with Google Cloud Speech. We were reassured that Microsoft Cognitive Speech’s error rate on our older adult participants is lower than what Microsoft had reported in 2018 (5.0%). Nonetheless, these numbers serve merely as anecdotal evidence. Our small sample is not representative of older adults; all participants were native English speakers in the US and none of them identified as disabled. Of course, speech as an input modality, can be advantageous for many disabled people such as blind individuals [4, 47, 136] and those with upper limb motor impairments [50, 76]. However, it is still limited for dysarthric [24, 76], deaf [35], and accented [99] speech as well as for low resource languages and noisy environments, all being active areas of research. With advances in speech recognition, we believe that we could leverage for many older adults their verbal reports as a reliable data source in an automated manner. This opens up an opportunity for automatically extracting user-generated activity labels (type and",0.5188316993464052,0.24101107474747477,0.9143525960784316,0.5982577919191919,,
193,119,14,Paragraph,"[(916, 924), (924, 939), (939, 953), (953, 967), (967, 977), (977, 988), (988, 992)]","semantics).Thatsaid, inferring quantitative or ordinal data from free-form text is not a trivial problem, as in the case of the effort level coding. As such, data such as effort level would be better off if they are collected in a structured way ( e.g. , have people select from a scale or predefined categories). This would require new UI & interaction designs e.g. , leveraging a simple touch interaction on a smartwatch or predefined voice commands.",0.5190212418300654,0.600773701010101,0.9143495193725493,0.708953498989899,,
194,120,14,Section,"(992, 999)",6.3 Comprehensive Capturing of Older Adults’ Activities,0.5195343137254902,0.724340785479798,0.9075560274509804,0.7544684369949495,,
195,121,14,Paragraph,"[(999, 1009), (1009, 1022), (1022, 1032), (1032, 1042), (1042, 1051)]","Even though understanding daily activities of older adults was not the main goal of our study, the collected data seem to cover more classical types of activities older adults perform while reflecting re- cent trends. We consider this as additional evidence to demonstrate the feasibility of in-situ data collection with older adults.",0.5195343137254902,0.7598987010101009,0.9145653869176472,0.826568397979798,,
196,121,14,Paragraph,"[(1051, 1060), (1060, 1070), (1070, 1080), (1080, 1090), (1090, 1101)]","The types of activities emerging from our participants’ reports overlap with most of the activities reported in prior literature, though their naming/grouping may not be fully aligned. For exam- ple, an interview study with 516 German older adults conducted in 1996 identified 44 types of activities and grouped them into",0.5195343137254902,0.829084307070707,0.9145709835921569,0.8957527414141414,,
197,122,15,Header,"[(0, 10), (10, 27)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.08790522875816993,0.0783156446969697,0.912089879738563,0.0871209477272727,,
198,123,15,Paragraph,"[(27, 36), (36, 51), (51, 62), (62, 72), (72, 81), (81, 93), (93, 105), (105, 112)]","eight categories— Personal Maintenance , Instrumental ADLs , Read- ing , Television , Other Leisure , Social Activities , Paid Work , and Rest- ing [49]. While these activities appeared in our dataset, we catego- rized them differently, for example, the Reading activity type under Hobby/leisure and many of the Instrumental ADL activities under Housekeeping. We note that there is not a clear consensus on how to group activities, so it will be important to preserve both raw labels and coded categories for future reference.",0.08790522875816993,0.10956031717171716,0.48294238421960795,0.21774011515151515,,
199,123,15,Paragraph,"[(112, 121), (121, 131), (131, 140), (140, 153), (153, 163), (163, 172), (172, 183), (183, 195), (195, 205), (205, 221), (221, 231), (231, 241), (241, 250), (250, 260), (260, 269), (269, 274)]","New activities have also emerged as digital technologies have advanced over time. For example, our Screen time category includes “Computer,” “Mobile device,” and “TV,” whereas prior studies con- ducted in 1982 [83] and 1998 [49] had the “Watching TV” as the top-level category (without the notion of screen time). Screen time has absorbed other activities that would have been categorized differently in the past. For example, many of our participants read news on the internet, rather than a printed paper. Screen time also commonly appeared with other activity types, such as social activity ( e.g. , a video call) and exercise ( e.g. , online yoga session), which may have transitioned from in person to remote during the pandemic. Labeling systems are bound to evolve as technology changes the way people achieve different functions. Reflecting these changes in designing activity labeling systems, there may be value in having multiple dimensions, such as device type, posture, semantics, to better characterize the captured activities.",0.08625,0.2202560242424243,0.48294082415686285,0.43913279191919197,,
200,124,15,Section,"(274, 285)",6.4 Capturing Non-Exercise Stepping as a Meaningful Activity for Older Adults,0.08790522875816993,0.4614380077020202,0.43295007957516335,0.4915656592171717,,
201,125,15,Paragraph,"[(285, 293), (293, 302), (302, 311), (311, 323), (323, 331), (331, 340), (340, 350), (350, 359), (359, 367), (367, 377), (377, 385), (385, 394), (394, 404), (404, 416), (416, 423), (423, 432), (432, 444), (444, 451), (451, 456)]","Researchers have advocated the importance of promoting non- exercise physical activities (free living activities that involve light and moderate physical activities, such as gardening, laundry, clean- ing, or casual walking) for older adults [107, 111, 112]. Recent ev- idence shows that non-exercise physical activities are positively associated with longevity [26, 41] and cardiovascular health [26], suggesting that any activity is better than no activity. However, it is challenging to capture non-exercise physical activities using recall-based methods or sensor-only methods [111]. For example, the interview study by Horgas and colleagues identified a generic “Walking” category [49]. Studies that leveraged accelerometer sen- sors often use statistically determined thresholds for sensor move- ments and bout duration to categorize activities by intensity level, treating sporadic and light activities as non-exercise ( e.g. , [14, 66]). However, applying uniform thresholds may ignore individualized characteristics [101, 109] and there are no standardized thresholds validated for older adults [39]. In addition, most of such studies do not capture user-generated context, potentially important details to understand what people did.",0.08625,0.4969959232323232,0.4829470118169935,0.757382791919192,,
202,125,15,Paragraph,"[(456, 466), (466, 477), (477, 486), (486, 497), (497, 506), (506, 519), (519, 527), (527, 537), (537, 546), (546, 552)]","In our study, 14% of verbal reports (171/1224) contained non- exercise stepping with context. We assert that our in-situ data col- lection method enabled participants to capture more subtle, light- intensity lifestyle activities ( e.g. , walking around home) that would have not been captured otherwise. Older adults’ non-exercise step- ping activities, which tend to be in slow gaits, are difficult to detect accurately with current waist and wrist-worn accelerometers. The context information captured in verbal reports may be a valuable supplement for device-based monitoring to support the training of person-specific classifiers for non-exercise physical activities.",0.08790522875816993,0.7598987010101009,0.4829418986248367,0.8957527414141414,,
203,126,15,Section,"(552, 556)",6.5 Personalizing Activity Trackers,0.5195343137254902,0.10765139154040401,0.8149178040849673,0.12142550770202028,,
204,127,15,Paragraph,"[(556, 564), (564, 574), (574, 585), (585, 598), (598, 608), (608, 616), (616, 626), (626, 635), (635, 646), (646, 656), (656, 668), (668, 677), (677, 687), (687, 698), (698, 710), (710, 722), (722, 729), (729, 739), (739, 750), (750, 759), (759, 769), (769, 781), (781, 789), (789, 797), (797, 808), (808, 814)]","Even though our participant sample was relatively homogeneous and geographically constrained to the same area, we observed high variation in the types of activities captured depending both on the participant and on other factors, such as the time of the year. We identified many implications from our findings for the design of personalized activity tracking systems with older adults. When automating the tracking of these activities from the sensor data, researchers can potentially leverage some of the existing datasets from younger adults ( e.g. , WISDM [129, 130], UCI-HHAR [115], and ExtraSensory [119]) to pre-train models for higher level activi- ties, such as sitting, standing, and walking that tend to be common among people regardless of their age. However, preliminary re- sults indicate that model adaptation is necessary as models trained on younger adults’ sensor data tend to perform worse on older adults [28]. In addition, the diversity in the activity types and se- mantics among the older adults in our study calls for model per- sonalization beyond adaptation, as one-to-one mapping between older adults’ activities and those available in the datasets (from younger adults) may not be possible. We could employ novel model personalization methods like teachable machines [25, 46, 54, 68], which leverage advances in transfer learning [90] and meta learn- ing [30, 70]. Systems like MyMove could play a critical role in facilitating this personalization process by supporting older adults and other underrepresented populations in fine-tuning the models in activity tracking applications with their own data, so that the applications can reflect their idiosyncratic characteristics.",0.5189918300653594,0.1268557717171717,0.9145750777098037,0.4841024888888889,,
205,128,15,Section,"(814, 821)",6.6 Usability Challenges with Smartwatch’s Low-power Mode,0.5195343137254902,0.4990175531565656,0.8815667009803921,0.5291452046717171,,
206,129,15,Paragraph,"[(821, 829), (829, 837), (837, 847), (847, 858), (858, 872), (872, 882), (882, 894), (894, 905), (905, 914), (914, 926), (926, 937), (937, 945)]","While our participants had generally positive experiences with MyMove and the smartwatch, six participants occasionally experi- enced the watch being unresponsive (Section 5.6.1). We suspect that this was caused during Wear OS’s low-power mode (also known as ambient mode ). When a user is not interacting with their watch for a while, Wear OS automatically enters into the low-power mode, dim- ming the watch display to save the battery. In this low-power mode, MyMove’s button icons and labels were still visible in low con- trast. To make the virtual buttons interactive, participants needed to “wake up” the screen by tapping anywhere on the watch dis- play. Alternatively, they could push the physical button to start the recording without needing to wake up the screen.",0.5188316993464052,0.5345767313131313,0.9145680076339872,0.6981037515151515,,
207,129,15,Paragraph,"[(945, 956), (956, 967), (967, 976), (976, 985), (985, 996), (996, 1007), (1007, 1018), (1018, 1029), (1029, 1039), (1039, 1041)]","We showed the buttons and icons in low contrast during the lower-power mode because we wanted to use them as a visual reminder to encourage data capture. However, the low-power mode was, in hindsight, an unfamiliar concept to participants, especially those who are new to a smartwatch: some participants thought that they could interact with the visible button in the low-power mode. Instead of showing the button icons and labels in low contrast, hiding them completely might have been a better design to avoid confusion, which is an interesting design tradeoff we learned from this study.",0.5189918300653594,0.7006196606060606,0.914346003137255,0.8364749636363636,,
208,130,15,Section,"(1041, 1046)",6.7 Limitations and Future Work,0.5195343137254902,0.8513900279040404,0.796129909640523,0.8651641440656566,,
209,131,15,Paragraph,"[(1046, 1058), (1058, 1068)]","In this section, we discuss the limitations of our study that could impact the generalizability of our findings. Although we aimed to",0.5195343137254902,0.8705956707070707,0.9121015461960783,0.8957527414141414,,
210,132,16,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
211,133,16,Paragraph,"[(24, 32), (32, 42), (42, 54), (54, 67), (67, 77), (77, 88), (88, 97), (97, 108), (108, 115), (115, 125), (125, 135), (135, 145), (145, 157), (157, 170), (170, 182), (182, 193), (193, 204), (204, 211), (211, 221), (221, 229), (229, 241), (241, 250), (250, 260), (260, 267), (267, 280), (280, 291), (291, 299), (299, 312), (312, 326), (326, 335), (335, 345), (345, 356), (356, 364), (364, 367)]","recruit participants with diverse backgrounds, our participants are not representative samples of older adults. They were all highly educated ( e.g. , having a college degree or above), had high-baseline technical proficiency ( e.g. , being able to use a Zoom video call), and did not have speech, hearing, motor, movement, or cognitive impairments. While this work is just a first step toward designing and developing inclusive activity tracking systems, we believe it is important to conduct a follow-up study with older adults with different educational backgrounds, health conditions, and technical proficiencies. This would help us extend our understanding of the strengths and limitations of in-situ data collection with speech on a smartwatch. As discussed above, we anticipate that this modality can be advantageous for people that were not captured by our small sample such as those who are blind or have low vision, as speech input can be more efficient for this user group [4, 136]. However, it may not be inclusive of dysarthric, deaf, and accented speech, especially if the goal is automatic extraction of the activity labels. Recent speech recognition personalization efforts like Google’s Project Euphonia [40, 74] are promising. Similarly, efforts that at- tempt cross-lingual knowledge transfer in speech recognition from high- to low-resource languages ( e.g. , [58, 116]) can make speech input more inclusive. Even then, the challenge of automatically extracting activity labels, timing, and effort levels from verbal re- ports remains. Information extraction from unstructured reports is an active area of research in natural language processing ( e.g. , processing medical verbal or written reports [57, 85]). Similar to the healthcare context, we could leverage transfer- and meta-learning techniques to deal with the lack of training data. More so, in con- trast to healthcare, we also have an opportunity to shape ( i.e. , via design and personalization) the user interactions with the activity trackers. Thus, we can influence the structure and vocabulary in the reports to meet the algorithmic capabilities halfway e.g. , by optimizing across flexibility, efficiency, and effectiveness for both users and algorithms.",0.08790522875816993,0.10956031717171716,0.48294875528366044,0.5775027414141415,,
212,133,16,Paragraph,"[(367, 379), (379, 388), (388, 398), (398, 409), (409, 418), (418, 426), (426, 437), (437, 447), (447, 455), (455, 467), (467, 480), (480, 489), (489, 498), (498, 514), (514, 528), (528, 542), (542, 555), (555, 566), (566, 575), (575, 584), (584, 595), (595, 603), (603, 607)]","Our study preparation ( e.g. , dropping off & picking up study equipment) and study design provided more face-to-face time with the participants than a typical remote deployment study. This pro- vided a chance for older adult participants to ask questions and troubleshoot issues. Thus, these repeated interactions may have con- tributed to forming rapport between participants and researchers, which in turn, could have contributed to the high engagement. We had two onboarding sessions with the 4-day adaptation period in between. During the 4-day adaptation period, participants became used to wearing and maintaining ( e.g. , charging) the devices, and were ready to collect data on Day 5 of the study. Some participants explicitly mentioned that the tutorial and the adaptation period were critical for their engagement. For example, P5 commented, “ Giving me a few days to get used to the equipment and how it worked... Making sure I had plugged in and make sure that I was charging, how to record, and I thought that was really good. And just how to give the reports, I think the orientation was very helpful as well. ” While we believe that giving a good tutorial before the actual experiment is important, we acknowledge that our particular approach may not scale. In addition, the study compensation and participants’ interest in contributing to a research project may also have affected participants’ engagement, although it is common to incentivise participants in ESM studies.",0.08625,0.5800186505050505,0.48293857285228753,0.8957527414141414,,
213,133,16,Paragraph,"[(607, 619), (619, 628), (628, 639), (639, 650), (650, 659), (659, 670), (670, 683), (683, 691), (691, 699), (699, 705)]","We note that we did not collect information on medication use of participants. Medications, such as 𝛽 -blockers, can influence heart rate and may blunt the response to higher intensity exercise, result- ing in lower heart rate measurements. As such, the intensities we recorded during Strenuous activities may be not accurately reflect the degree of vigor with which the participant was being active, resulting in the percentage of HR 𝑚𝑎𝑥 that were closer to the low intensity activities. Future work should consider the incorporation of participants’ medication information to further validate heart rate intensities, especially for high-intensity activities.",0.5195343137254902,0.10956031717171716,0.9145648887843141,0.24541435757575758,,
214,133,16,Paragraph,"[(705, 717), (717, 728), (728, 737), (737, 750), (750, 761), (761, 773), (773, 784), (784, 794), (794, 805), (805, 815), (815, 826), (826, 836), (836, 849), (849, 854)]","We chose a smartwatch as an only means to collect verbal activity reports and to deliver notifications. In the future, we can leverage other “smart” devices for more comprehensive and accurate data collection. For example, when the TV is on and the person is nearby (without much movement), we can infer that the person is watching TV. In addition, we can leverage the speech input capability of other devices. For example, a person can report their activities using a smart speaker that is becoming more prevalent (the speaker can even play the recording back to the person). Similarly, a person can record their activities from their smartphone, tablet, laptop, or desktop as all these devices are equipped with a microphone. Since these devices have larger display than a smartwatch, people can view or edit data they captured elsewhere ( e.g. , a smartwatch or smart speaker) from these devices.",0.51909477124183,0.24793026666666673,0.9124804687058825,0.43913279191919197,,
215,133,16,Paragraph,"[(854, 866), (866, 877), (877, 886), (886, 897), (897, 910), (910, 919), (919, 930), (930, 941), (941, 948), (948, 958), (958, 968), (968, 974)]","In our study, we did not provide feedback other than the number of reports participants submitted on a given day. However, in the debriefing interview, half of our participants reported that they became more aware of the activities they performed and how they spent the time. Also known as the “reactivity effect,” this is a well- known phenomenon in behavioral psychology [86]. We believe that our approach to collecting in-situ data can serve a dual purpose of activity labeling and self-monitoring ; the latter can be further augmented through providing informative and engaging feedback— for example, showing how much they have been sitting, working out, gardening—and people may be more motivated to engage in desirable activities while capturing data (labels).",0.5195343137254902,0.441648701010101,0.9145667641098041,0.6051757212121212,,
216,134,16,Section,"(974, 976)",7 CONCLUSION,0.5195343137254902,0.6299973511363636,0.6629568931372549,0.6437714672979797,,
217,135,16,Paragraph,"[(976, 987), (987, 998), (998, 1006), (1006, 1016), (1016, 1027), (1027, 1037), (1037, 1047), (1047, 1058), (1058, 1068), (1068, 1077), (1077, 1085), (1085, 1096), (1096, 1106), (1106, 1114), (1114, 1124), (1124, 1136), (1136, 1145), (1145, 1152)]","In this work, we examined the feasibility of collecting in-situ activ- ity reports with older adults, with the ultimate goal of developing personalized activity tracking technologies that better match their preferences and patterns. We built MyMove, an Android Wear re- porting app. Considering older adults as the main user group, we streamlined the data capture flow and leveraged the flexible speech input on a smartwatch. Through a 7-day deployment study with 13 older adults, we collected a rich dataset including older adult participants’ verbal reports, the sensor data from a smartwatch and a thigh-worn activity monitor, and participants’ feedback from the debriefing interviews. Our results showed that participants were highly engaged in the data collection. They submitted a total of 1,224 verbal reports. Additionally, the wear time of the smartwatch (11.6 hours/day) and thigh-worn activity monitor (23.3 hours/day) was very high. Examining the verbal reports further, we found that all of them were valid, that is, a researcher could understand and transcribe them. Moreover, verbal reports could be transcribed with state-of-the-art automatic speech recognition systems with",0.5189918300653594,0.649202993939394,0.9145761260862747,0.8957527414141414,,
218,136,17,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.3363069709150327,0.0871209477272727,,
219,137,17,Bibliography,"[(10, 27)]","Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K.",0.46807988464052286,0.0783156446969697,0.912089879738563,0.0871209477272727,,
220,138,17,Paragraph,"[(27, 38), (38, 48), (48, 58), (58, 68), (68, 76), (76, 84), (84, 86)]","acceptable error rates ( e.g. , 4.93% with Microsoft Cognitive Speech). These results, taken together, indicate that our novel data collec- tion approach, realized in MyMove, can facilitate older adults to collect useful in-situ activity labels. Going forward, we are excited to continue our endeavors towards building personalized activity tracking technologies that further capture meaningful activities for older adults.",0.0874656862745098,0.10956031717171716,0.48294370227451,0.203902993939394,,
221,139,17,Section,"(86, 87)",ACKNOWLEDGMENTS,0.08790522875816993,0.21715265416666665,0.27999095032679744,0.23092677032828293,,
222,140,17,Paragraph,"[(87, 98), (98, 108), (108, 119), (119, 129), (129, 138), (138, 144)]","We thank our study participants for their time, efforts, and feedback. We also thank Catherine Plaisant, Yuhan Luo, and Rachael Zehrung for helping us improve the tutorial protocol. We are also grateful for Bonnie McClellan and Explorations On Aging for helping us recruit study participants. This work was supported by National Science Foundation awards #1955568 and #1955590.",0.08720261437908497,0.23635703434343433,0.48272136010457534,0.3168638525252525,,
223,141,17,Section,"(144, 145)",REFERENCES,0.08790522875816993,0.33011225012626255,0.20132778316993463,0.34388636628787883,,
224,142,17,Bibliography,"[(145, 170), (170, 193), (193, 196), (196, 217), (217, 224), (224, 237), (237, 265), (265, 275), (275, 305), (305, 311), (311, 324), (324, 346), (346, 358), (358, 378), (378, 401), (401, 424), (424, 444), (444, 466), (466, 473), (473, 496), (496, 510), (510, 538), (538, 559), (559, 587), (587, 612), (612, 632), (632, 647), (647, 670), (670, 693), (693, 713), (713, 737), (737, 750), (750, 776), (776, 794), (794, 820), (820, 841), (841, 863), (863, 885), (885, 911), (911, 934), (934, 948), (948, 969), (969, 988), (988, 1011), (1011, 1026), (1026, 1051), (1051, 1072), (1072, 1094), (1094, 1117), (1117, 1131), (1131, 1153), (1153, 1178), (1178, 1190), (1190, 1213), (1213, 1223), (1223, 1246), (1246, 1264), (1264, 1287), (1287, 1297)]","[1] Barbara E. Ainsworth, William L. Haskell, Arthur S. Leon, David R. Jacobs, Henry J. Montoye, James F. Sallis, and Ralph S. Paffenbarger. 1993. Com- pendium of Physical Activities: Classification of Energy Costs of Human Physical Activities. Medicine & Science in Sports & Exercise 25, 1 (1993), 71– 80. https://journals.lww.com/acsm-msse/Fulltext/1993/01000/Compendium_ of_Physical_Activities__classification.11.aspx [2] American College of Sports Medicine. 2021. ACSM’s Guidelines for Exercise Testing and Prescription (11 ed.). Lippincott Williams & Wilkins, Philadelphia, PA, USA. 548 pages. [3] MichaelJ.Annear,GrantCushman,andBobGidlow.2009. Leisuretimephysical activity differences among older adults from diverse socioeconomic neighbor- hoods. Health&Place 15,2(2009),482–490. https://doi.org/10.1016/j.healthplace. 2008.09.005 [4] Shiri Azenkot and Nicole B. Lee. 2013. Exploring the Use of Speech Input by Blind People on Mobile Devices. In Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility (Bellevue, Washington) (ASSETS’13) .AssociationforComputingMachinery,NewYork,NY,USA,Article 11, 8 pages. https://doi.org/10.1145/2513383.2513440 [5] Maxim Bakaev. 2008. Fitts’ Law for Older Adults: Considering a Factor of Age. In Proceedings of the VIII Brazilian Symposium on Human Factors in Computing Systems (PortoAlegre,RS,Brazil) (IHC’08) .SociedadeBrasileiradeComputação, BRA, 260–263. [6] PaulB.BaltesandMargretM.Baltes.1990. SuccessfulAging:Perspectivesfromthe Behavioral Sciences . Cambridge University Press, Cambridge, United Kingdom. https://doi.org/10.1017/CBO9780511665684 [7] Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python (1st ed.). O’Reilly Media, Inc., Sebastopol, CA, USA. [8] NiallBolger,AngelinaDavis,andEshkolRafaeli.2003.DiaryMethods:CapturingLifeasItIsLived. Annual Review of Psychology 54, 1 (2003), 579–616. https: //doi.org/10.1146/annurev.psych.54.101601.145030 [9] Marc Bonnefoy, Sylvie Normand, Christiane Pachiaudi, Jean RenÃ© Lacour, Martine Laville, and Tomasz Kostka. 2001. Simultaneous Validation of Ten Physical Activity Questionnaires in Older Men: A Doubly Labeled Water Study. Journal of the American Geriatrics Society 49, 1 (Jan. 2001), 28–35. https://doi. org/10.1046/j.1532-5415.2001.49006.x [10] Gunnar Borg. 1990. Psychophysical Scaling with Applications in Physical Work and the Perception of Exertion. Scandinavian Journal of Work, Environment and Health 16, SUPPL. 1 (1990), 55–58. https://doi.org/10.5271/sjweh.1815 [11] Gunnar Borg. 1998. Borg’s Perceived Exertion and Pain Scales . Human Kinetics, USA. [12] Gerry R. Boss and J. Edwin Seegmiller. 1981. Age-Related Physiological Changes and Their Clinical Significance. The Western Journal of Medicine 135, 6 (Dec. 1981), 434–40. http://www.ncbi.nlm.nih.gov/pubmed/7336713http: //www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1273316 [13] Lawrence R. Brawley, W. Jack Rejeski, and Abby C. King. 2003. Promoting Physical Activity for Older Adults: the Challenges for Changing Behavior. American Journal of Preventive Medicine 25, 3 Suppl 2 (Oct. 2003), 172–83. https: //doi.org/10.1016/s0749-3797(03)00182-x [14] Matthew P. Buman, Eric B. Hekler, William L. Haskell, Leslie Pruitt, Terry L. Conway, Kelli L. Cain, James F. Sallis, Brian E. Saelens, Lawrence D. Frank, and Abby C. King. 2010. Objective Light-Intensity Physical Activity Associations With Rated Health in Older Adults. American Journal of Epidemiology 172, 10 (Nov. 2010), 1155–1165. https://doi.org/10.1093/aje/kwq249 [15] Eli Carmeli, Hagar Patish, and Raymond Coleman. 2003. The Aging Hand. The Journals of Gerontology: Series A 58, 2 (Feb. 2003), M146–M152. https: //doi.org/10.1093/gerona/58.2.M146 [16] Marta E. Cecchinato, Anna L. Cox, and Jon Bird. 2017. Always On(Line)? User Experience of Smartwatches and Their Role within Multi-Device Ecologies. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems . ACM, New York, NY, USA, 3557–3568. https://doi.org/10.1145/3025453.3025538 [17] Barbara L. Chalfonte, Robert S. Fish, and Robert E. Kraut. 1991. Expressive Richness:aComparisonofSpeechandTextasMediaforRevision.In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’91) . Association for Computing Machinery, New York, NY, USA, 21–26. https: //doi.org/10.1145/108844.108848 [18] Ranganathan Chandrasekaran, Vipanchi Katthula, and Evangelos Moustakas. 2020. Patterns of Use and Key Predictors for the Use of Wearable Health Care Devices by US Adults: Insights from a National Survey. Journal of Medical Internet Research 22, 10 (Oct. 2020), e22443. https://doi.org/10.2196/22443 [19] Liu Chen and Meysam Asgari. 2021. Refining Automatic Speech Recognition System for Older Adults. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, Washington, DC, USA, 7003–7007. https://doi.org/10.1109/ICASSP39728.2021.9414207 [20] Wojtek J. Chodzko-Zajko, David N. Proctor, Maria A. Fiatarone Singh, Christo- pher T. Minson, Claudio R. Nigg, George J. Salem, and James S. Skinner. 2009. Exercise and Physical Activity for Older Adults. Medicine & science in sports & exercise 41, 7 (2009), 1510–1530. [21] Pak-Kwong Chung, Yanan Zhao, Jing-Dong Liu, and Binh Quach. 2015. A Brief Note on the Validity and Reliability of the Rating of Perceived Exertion Scale in Monitoring Exercise Intensity among Chinese Older Adults in Hong Kong. Perceptual and Motor Skills 121, 3 (Dec. 2015), 805–809. https://doi.org/10.2466/ 29.PMS.121c24x8 [22] Leigh Clark, Philip Doyle, Diego Garaialde, Emer Gilmartin, Stephan Schlögl, Jens Edlund, Matthew Aylett, João Cabral, Cosmin Munteanu, Justin Edwards, et al. 2019. The State of Speech in HCI: Trends, Themes and Challenges. Inter- acting with Computers 31, 4 (2019), 349–371. https://doi.org/10.1093/iwc/iwz016 [23] Mark G. Davis, Kenneth R. Fox, Melvyn Hillsdon, Debbie J. Sharp, Jo C. Coulson, and Janice L. Thompson. 2011. Objectively Measured Physical Activity in a DiverseSampleofOlderUrbanUKAdults. Medicine&ScienceinSports&Exercise 43, 4 (April 2011), 647–654. https://doi.org/10.1249/MSS.0b013e3181f36196 [24] Luigi De Russis and Fulvio Corno. 2019. On the impact of dysarthric speech on contemporary ASR cloud platforms. Journal of Reliable Intelligent Environments 5, 3 (2019), 163–172. https://doi.org/10.1007/s40860-019-00085-y [25] Utkarsh Dwivedi, Jaina Gandhi, Raj Parikh, Merijke Coenraad, Elizabeth Bon- signore, and Hernisa Kacorri. 2021. Exploring Machine Teaching with Children. In 2021 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC) . IEEE, IEEE, Washington, DC, USA, 11 pages. [26] Elin Ekblom-Bak, Björn Ekblom, Max Vikström, Ulf de Faire, and Mai-Lis Hel- lénius. 2014. The Importance of Non-Exercise Physical Activity for Cardiovas- cular Health and Longevity. British Journal of Sports Medicine 48, 3 (Feb. 2014), 233–238. https://doi.org/10.1136/bjsports-2012-092038 [27] Chloe Fan, Jodi Forlizzi, and Anind Dey. 2012. Considerations for Technology that Support Physical Activity by Older Adults. In Proceedings of the 14th inter- national ACM SIGACCESS conference on Computers and accessibility - ASSETS ’12 . ACM Press, New York, New York, USA, 33. https://doi.org/10.1145/2384916. 2384923 [28] Sabahat Fatima. 2021. Activity Recognition in Older Adults with Training Data from Younger Adults: Preliminary Results on in VivoSmartwatch Sensor Data. In Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’21) . ACM, New York, NY, USA, 26:1–26:8. https://doi.org/10.1145/3441852.3476475 [29] MireiaFernández-ArdèvolandAndreaRosales.2017. MyInterests,MyActivities: Learning from an Intergenerational Comparison of Smartwatch Use. In Human Aspects of IT for the Aged Population. Applications, Services and Contexts , Jia Zhou and Gavriel Salvendy (Eds.). Springer International Publishing, Cham, 114–129. https://doi.org/10.1007/978-3-319-58536-9_10 [30] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-Agnostic Meta- Learning for Fast Adaptation of Deep Networks. In Proceedings of the 34th InternationalConferenceonMachineLearning(ICML’17) (ProceedingsofMachine Learning Research, Vol. 70) . PMLR, USA, 1126–1135. https://proceedings.mlr. press/v70/finn17a.html [31] F. Foerster, M. Smeja, and J. Fahrenberg. 1999. Detection of Posture and Motion by Accelerometry: a Validation study in Ambulatory Monitoring. Computers in Human Behavior 15, 5 (Sept. 1999), 571–583. https://doi.org/10.1016/S0747- 5632(99)00037-0 [32] David P. French, Ellinor K. Olander, Anna Chisholm, and Jennifer Mc Sharry. 2014. Which Behaviour Change Techniques are Most Effective at Increasing Older Adults’ Self-Efficacy and Physical Activity Behaviour? A Systematic Review. Annals of behavioral medicine 48, 2 (2014), 225–234. [33] Paul A. Gardiner, Bronwyn K. Clark, Genevieve N. Healy, Elizabeth G. Eakin, Elisabeth A.H. Winkler, and Neville Owen. 2011. Measuring Older Adults’ Sedentary Time: Reliability, Validity, and Responsiveness. Medicine & Science in",0.09320424836601307,0.11149001944444449,0.9140245360202613,0.8980313674242425,,
225,143,18,Header,"[(0, 14), (14, 24)]","MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.0871209477272727,,
226,144,18,Bibliography,"[(24, 34), (34, 37), (37, 49), (49, 61), (61, 78), (78, 82), (82, 95), (95, 107), (107, 123), (123, 142), (142, 164), (164, 190), (190, 201), (201, 211), (211, 232), (232, 243), (243, 245), (245, 253), (253, 275), (275, 287), (287, 310), (310, 328), (328, 351), (351, 372), (372, 382), (382, 407), (407, 425), (425, 448), (448, 452), (452, 472), (472, 497), (497, 515), (515, 538), (538, 557), (557, 575), (575, 596), (596, 609), (609, 633), (633, 647), (647, 661), (661, 677), (677, 699), (699, 723), (723, 752), (752, 770), (770, 794), (794, 815), (815, 841), (841, 854), (854, 877), (877, 897), (897, 922), (922, 938), (938, 948), (948, 962), (962, 972), (972, 985), (985, 1008), (1008, 1023), (1023, 1038), (1038, 1060), (1060, 1081), (1081, 1101), (1101, 1127), (1127, 1142), (1142, 1165), (1165, 1182), (1182, 1204), (1204, 1223), (1223, 1244), (1244, 1262), (1262, 1281), (1281, 1296)]","Sports & Exercise 43, 11 (Nov. 2011), 2127–2133. https://doi.org/10.1249/MSS. 0b013e31821b94f7 [34] KathrinGerling,MoRay,VeroVandenAbeele,andAdamB.Evans.2020. Critical ReflectionsonTechnologytoSupportPhysicalActivityamongOlderAdults:AnExplorationofLeadingHCIVenues. ACM Transactions on Accessible Computing 13, 1 (April 2020), 1–23. https://doi.org/10.1145/3374660 [35] Abraham T. Glasser, Kesavan R. Kushalnagar, and Raja S. Kushalnagar. 2017. FeasibilityofUsingAutomaticSpeechRecognitionwithVoicesofDeafandHard-of-HearingIndividuals.In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (Baltimore, Maryland, USA) (ASSETS ’17) .AssociationforComputingMachinery,NewYork,NY,USA,373–374. https: //doi.org/10.1145/3132525.3134819 [36] Google. 2021. Activity Recognition API. Retrieved Sep 09, 2021 from https: //developers.google.com/location-context/activity-recognition [37] Google. 2021. Protocol Buffers. Retrieved Sep 09, 2021 from https://developers. google.com/protocol-buffers [38] Google. 2021. Speech-to-Text: Automatic Speech Recognition | Google Cloud. Retrieved Sep 09, 2021 from https://cloud.google.com/speech-to-text [39] E. Gorman, H. M. Hanson, P. H. Yang, K. M. Khan, T. Liu-Ambrose, and M. C. Ashe.2014. AccelerometryAnalysisofPhysicalActivityandSedentaryBehavior in Older Adults: a Systematic Review and Data Analysis. European Review of Aging and Physical Activity 11, 1 (April 2014), 35–49. https://doi.org/10.1007/ s11556-013-0132-x [40] Jordan R. Green, Robert L. MacDonald, Pan-Pan Jiang, Julie Cattiau, Rus Hey- wood, Richard Cave, Katie Seaver, Marilyn A. Ladewig, Jimmy Tobin, Michael P. Brenner, Philip C. Nelson, and Katrin Tomanek. 2021. Automatic Speech Recog- nitionofDisorderedSpeech:PersonalizedModelsOutperformingHumanListen-ersonShortPhrases.In Interspeech 2021 . ISCA, Winona, MN, USA, 4778–4782. https://doi.org/10.21437/Interspeech.2021-1384 [41] Mark Hamer, Cesar de Oliveira, and Panayotes Demakakos. 2014. Non-Exercise Physical Activity and Survival: English Longitudinal Study of Ageing. American Journal of Preventive Medicine 47, 4 (Oct. 2014), 452–460. https://doi.org/10. 1016/j.amepre.2014.05.044 [42] GabriellaM.Harari,NicholasD.Lane,RuiWang,BenjaminS.Crosier,AndrewT.Campbell,andSamuelD.Gosling.2016.UsingSmartphonestoCollectBehav-ioralDatainPsychologicalScience. Perspectives on Psychological Science 11, 6 (Nov.2016),838–854. https://doi.org/10.1177/1745691616650285arXiv:15334406 [43] Juliet Harvey, Sebastien Chastin, and Dawn Skelton. 2013. Prevalence of Seden- tary Behavior in Older Adults: A Systematic Review. International Journal of Environmental Research and Public Health 10, 12 (Dec. 2013), 6645–6661. https://doi.org/10.3390/ijerph10126645 [44] Andrea L. Hergenroeder, Bethany Barone Gibbs, Mary P. Kotlarczyk, Robert J. Kowalsky, Subashan Perera, and Jennifer S. Brach. 2018. Accuracy of Objective Physical Activity Monitors in Measuring Steps in Older Adults. Gerontology and Geriatric Medicine 4 (Jan. 2018), 233372141878112. https://doi.org/10.1177/ 2333721418781126 [45] Javier Hernandez, Daniel McDuff, Christian Infante, Pattie Maes, Karen Quigley, and Rosalind Picard. 2016. Wearable ESM: Differences in the Experience Sam- pling Method across Wearable Devices. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services . ACM, New York, NY, USA, 195–205. https://doi.org/10.1145/2935334.2935340 [46] JonggiHong,KyungjunLee,JuneXu,andHernisaKacorri.2020. Crowdsourcing the Perception of Machine Teaching. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20) . ACM, New York, NY, USA, 1–14. https://doi.org/10.1145/3313831.3376428 [47] Jonggi Hong, Christine Vaing, Hernisa Kacorri, and Leah Findlater. 2020. Re- viewing Speech Input with Audio: Differences between Blind and Sighted Users. ACM Trans. Access. Comput. 13, 1, Article 2 (April 2020), 28 pages. https://doi.org/10.1145/3382039 [48] EnamulHoque,RobertF.Dickerson,andJohnA.Stankovic.2014. Vocal-Diary:A VoiceCommandbasedGroundTruthCollectionSystemforActivityRecognition.In Proceedings of the Wireless Health 2014 on National Institutes of Health . ACM, New York, NY, USA, 1–6. https://doi.org/10.1145/2668883.2669587 [49] Ann L. Horgas, Hans-Ulrich Wilms, and Margret M. Baltes. 1998. Daily Life in Very Old Age: Everyday Activities as Expression of Successful Living. The Gerontologist 38, 5 (Oct. 1998), 556–568. https://doi.org/10.1093/geront/38.5.556 [50] Xueliang Huo, Hangue Park, and Maysam Ghovanloo. 2012. Dual-Mode Tongue Drive System: Using Speech and Tongue Motion to Improve Computer Access for People with Disabilities. In Proceedings of the Conference on Wireless Health (San Diego, California) (WH ’12) . Association for Computing Machinery, New York, NY, USA, Article 6, 8 pages. https://doi.org/10.1145/2448096.2448102 [51] Stephen Intille, Caitlin Haynes, Dharam Maniar, Aditya Ponnada, and Justin Manjourides. 2016. 𝜇 EMA: Microinteraction-Based Ecological Momentary Assessment (EMA) Using a Smartwatch. In Proceedings of the 2016 ACM In- ternational Joint Conference on Pervasive and Ubiquitous Computing (Heidel- berg, Germany) (UbiComp ’16) . ACM, New York, NY, USA, 1124–1128. https: //doi.org/10.1145/2971648.2971717 [52] David R. Jacobs, Barbara E. Ainsworth, Terryl J. Hartman, and Arthur S. Leon. 1993. A Simultaneous Evaluation of 10 Commonly Used Physical Activity Questionnaires. Medicine & Science in Sports & Exercise 25, 1 (1993), 81– 91. https://journals.lww.com/acsm-msse/Fulltext/1993/01000/A_simultaneous_ evaluation_of_10_commonly_used.12.aspx [53] JetBrains s.r.o. 2021. Kotlin Programming Language. Retrieved Sep 09, 2021 from https://kotlinlang.org/ [54] Hernisa Kacorri. 2017. Teachable Machines for Accessibility. SIGACCESS Access. Comput. 119 (Nov. 2017), 10–18. https://doi.org/10.1145/3167902.3167904 [55] Man-Yee Kan, Muzhi Zhou, Daniela Veronica Negraia, Kamila Kolpashnikova, Ekaterina Hertog, Shohei Yoda, and Jiweon Jun. 2021. How do Older Adults Spend Their Time? Gender Gaps and Educational Gradients in Time Use in East Asian and Western Countries. Journal of Population Ageing 14 (2021), 537–562. https://doi.org/10.1007/s12062-021-09345-3 [56] Sidney Katz, Amasa B. Ford, Roland W. Moskowitz, Beverly A. Jackson, and Marjorie W. Jaffe. 1963. Studies of Illness in the Aged: The Index of ADL: A Standardized Measure of Biological and Psychosocial Function. Journal of the American Medical Association 185, 12 (1963), 914–919. https://doi.org/10.1001/ jama.1963.03060120024016 [57] Martijn G Kersloot, Florentien J P van Putten, Ameen Abu-Hanna, Ronald Cornet, and Derk L Arts. 2020. Natural language processing algorithms for mapping clinical text fragments onto ontology concepts: a systematic review and recommendations for future studies. Journal of biomedical semantics 11, 1 (11 2020), 14–14. https://doi.org/10.1186/s13326-020-00231-z [58] Shreya Khare, Ashish Mittal, Anuj Diwan, Sunita Sarawagi, Preethi Jyothi, and Samarth Bharadwaj. 2021. Low Resource ASR: The Surprising Effectiveness of High Resource Transliteration. In Interspeech 2021 . ISCA, Winona, MN, USA, 1529–1533. https://doi.org/10.21437/Interspeech.2021-2062 [59] Matin Kheirkhahan, Sanjay Nair, Anis Davoudi, Parisa Rashidi, Amal A. Wani- gatunga, Duane B. Corbett, Tonatiuh Mendoza, Todd M. Manini, and Sanjay Ranka. 2019. A Smartwatch-Based Framework for Real-Time and Online Assess- ment and Mobility Monitoring. Journal of Biomedical Informatics 89, November 2018 (2019), 29–40. https://doi.org/10.1016/j.jbi.2018.11.003 [60] Young-Ho Kim, Bongshin Lee, Arjun Srinivasan, and Eun Kyoung Choe. 2021. Data@Hand: Fostering Visual Exploration of Personal Data On Smartphones Leveraging Speech and Touch Interaction. In Proceedings of the 2021 CHI Confer- enceonHumanFactorsinComputingSystems (Yokohama,Japan) (CHI’21) .ACM, New York, NY, USA, Article 462, 17 pages. https://doi.org/10.1145/3411764. 3445421 [61] Pascal van Kooten. 2021. Contractions Python Library. Retrieved Sep 09, 2021 from https://github.com/kootenpv/contractions [62] Sarah Kozey-Keadle, Amanda Libertine, Kate Lyden, John Staudenmayer, and PattyS.Freedson.2011.ValidationofWearableMonitorsforAssessingSedentaryBehavior. Medicine and Science in Sports and Exercise 43, 8 (2011), 1561–1567. https://doi.org/10.1249/MSS.0b013e31820ce174 [63] Oscar D. Lara and Miguel A. Labrador. 2013. A Survey on Human Activity Recognition using Wearable Sensors. IEEE Communications Surveys & Tutorials 15, 3 (2013), 1192–1209. https://doi.org/10.1109/SURV.2012.110112.00192 [64] Reed Larson and Mihaly Csikszentmihalyi. 2014. The Experience Sam- pling Method. In Flow and the foundations of positive psychology . Springer, Berlin/Heidelberg, Germany, 21–34. [65] M. Powell Lawton and Elaine M. Brody. 1969. Assessment of Older People: Self-Maintaining and Instrumental Activities of Daily Living. The Gerontologist 9, 3 Part 1 (Sept. 1969), 179–186. https://doi.org/10.1093/geront/9.3_Part_1.179 [66] I-Min Lee and Eric J. Shiroma. 2014. Using Accelerometers to Measure Physical Activity in Large- Scale Epidemiologic Studies: Issues and Challenges. British Journal of Sports Medicine 48, 3 (Feb. 2014), 197–201. https://doi.org/10.1136/ bjsports-2013-093154 [67] I-Min Lee, Eric J. Shiroma, Masamitsu Kamada, David R. Bassett, Charles E. Matthews, and Julie E. Buring. 2019. Association of Step Volume and Intensity With All-Cause Mortality in Older Women. JAMA Internal Medicine 179, 8 (Aug. 2019), 1105. https://doi.org/10.1001/jamainternmed.2019.0899 [68] Kyungjun Lee, Jonggi Hong, Simone Pimento, Ebrima Jarjue, and Hernisa Ka- corri. 2019. Revisiting Blind Photography in the Context of Teachable Object Recognizers.In The21stInternationalACMSIGACCESSConferenceonComputers and Accessibility (Pittsburgh, PA, USA) (ASSETS ’19) . ACM, New York, NY, USA, 83–95. https://doi.org/10.1145/3308561.3353799 [69] Russell V. Lenth, Paul Buerkner, Maxime Herve, Jonathon Love, Hannes Riebl, and Henrik Singmann. 2021. emmeans: Estimated Marginal Means, aka Least- Squares Means . CRAN. https://CRAN.R-project.org/package=emmeans [70] Chenglin Li, Di Niu, Bei Jiang, Xiao Zuo, and Jianming Yang. 2021. Meta- HAR: Federated Representation Learning for Human Activity Recognition. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW ’21) . ACM, New York, NY, USA, 912–922. https://doi.org/10.1145/3442381.3450006 [71] Yuhan Luo, Young-Ho Kim, Bongshin Lee, Naeemul Hassan, and Eun Kyoung Choe. 2021. FoodScrap: Promoting Rich Data Capture and Reflective Food Journaling Through Speech Input. In Designing Interactive Systems Conference 2021 (Virtual Event, USA) (DIS ’21) . ACM, New York, NY, USA, 606–618. https: //doi.org/10.1145/3461778.3462074",0.09320424836601307,0.11149001944444449,0.9140232369790847,0.9031526457070708,,
227,145,19,Bibliography,"[(0, 10), (10, 27), (27, 50), (50, 77), (77, 92), (92, 104), (104, 126), (126, 148), (148, 160), (160, 182), (182, 203), (203, 224), (224, 236), (236, 237), (237, 256), (256, 278), (278, 303), (303, 321), (321, 343), (343, 366), (366, 371), (371, 398), (398, 419), (419, 433), (433, 460), (460, 482), (482, 507), (507, 522), (522, 547), (547, 570), (570, 587), (587, 609), (609, 625), (625, 638), (638, 655), (655, 678), (678, 701), (701, 727), (727, 752), (752, 778), (778, 793), (793, 814), (814, 837), (837, 850), (850, 867), (867, 890), (890, 896), (896, 918), (918, 939), (939, 947), (947, 960), (960, 961), (961, 962), (962, 979), (979, 1002), (1002, 1030), (1030, 1046), (1046, 1068), (1068, 1089), (1089, 1116), (1116, 1136), (1136, 1165), (1165, 1171), (1171, 1194), (1194, 1204), (1204, 1230), (1230, 1244), (1244, 1245), (1245, 1246), (1246, 1263), (1263, 1277), (1277, 1287)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Kim, Y.-H., Chou, D., Lee, B., Danilovich, M., Lazar, A., Conroy, D.E., Kacorri, H., and Choe, E.K. [72] Yuhan Luo, Bongshin Lee, and Eun Kyoung Choe. 2020. TandemTrack: Shaping Consistent Exercise Experience by Complementing a Mobile App with a Smart Speaker. In Proceedings of the 2020 CHI Conference on Human Factors in Com- puting Systems (Honolulu, HI, USA) (CHI ’20) . ACM, New York, NY, USA, 1–13. https://doi.org/10.1145/3313831.3376616 [73] KateLyden,SarahKeadle,JohnStaudenmayer,andPattyFreedson.2012. Validity of Two Wearable Monitors to Estimate Breaks from Sedentary Time. Medicine and science in sports and exercise 44 (05 2012), 2243–52. https://doi.org/10.1249/ MSS.0b013e318260c477 [74] Robert L. MacDonald, Pan-Pan Jiang, Julie Cattiau, Rus Heywood, Richard Cave, Katie Seaver, Marilyn A. Ladewig, Jimmy Tobin, Michael P. Brenner, Philip C. Nelson, Jordan R. Green, and Katrin Tomanek. 2021. Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia. In Interspeech 2021 . ISCA, Winona, MN, USA, 4833–4837. https: //doi.org/10.21437/Interspeech.2021-697 [75] Soultana Macridis, Nora Johnston, Steven Johnson, and Jeff K Vallance. 2018. Consumer Physical Activity Tracking Device Ownership and Use Among a Population-Based Sample of Adults. PloS one 13, 1 (2018), e0189298. [76] Meethu Malu, Pramod Chundury, and Leah Findlater. 2018. Exploring Ac- cessible Smartwatch Interactions for People with Upper Body Motor Impair- ments . Association for Computing Machinery, New York, NY, USA, 1–12. https://doi.org/10.1145/3173574.3174062 [77] Todd Matthew Manini, Tonatiuh Mendoza, Manoj Battula, Anis Davoudi, Matin Kheirkhahan,MaryEllenYoung,EricWeber,RogerBentonFillingim,andParisaRashidi.2019.PerceptionofOlderAdultsTowardSmartwatchTechnology for Assessing Pain and Related Patient-Reported Outcomes: Pilot Study. JMIR mHealth and uHealth 7, 3 (March 2019), e10044. https://doi.org/10.2196/10044 [78] Justin Mccarthy. 2021. One in Five U.S. Adults Use Health Apps, Wearable Trackers | Gallup. Retrieved Sep 09, 2021 from https://news.gallup.com/poll/ 269096/one-five-adults-health-apps-wearable-trackers.aspx [79] Kryss McKenna, Kieran Broome, and Jacki Liddle. 2007. What Older People Do: Time Use and Exploring the Link Between Role Participation and Life Satisfaction in People Aged 65 Years and Over. Australian Occupational Therapy Journal 54, 4 (March 2007), 273–284. https://doi.org/10.1111/j.1440-1630.2007. 00642.x [80] Kathryn Mercer, Melissa Li, Lora Giangregorio, Catherine Burns, and Kelly Grindrod. 2016. Behavior change techniques present in wearable activity track- ers: a critical analysis. JMIR mHealth and uHealth 4, 2 (2016), e4461. [81] Microsoft. 2021. Cognitive Speech Services | Microsoft Azure. Retrieved Sep 09, 2021 from https://azure.microsoft.com/en-us/services/cognitive-services/ speech-services/ [82] Md Abu Sayeed Mondol, Ifat A. Emi, Sirat Samyoun, M. Arif Imtiazur Rahman, and John A. Stankovic. 2018. WaDa: An Android Smart Watch App for Sensor Data Collection. In Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers (Singapore, Singapore) (UbiComp ’18) . ACM, New York, NY, USA, 404–407. https://doi.org/10.1145/3267305.3267660 [83] Miriam S. Moss and M. Powell Lawton. 1982. Time Budgets of Older People: a Window on Four Lifestyles. Journal of Gerontology 37, 1 (Jan. 1982), 115–123. https://doi.org/10.1093/geronj/37.1.115 [84] Lilian Genaro Motti, Nadine Vigouroux, and Philippe Gorce. 2013. Interaction Techniques for Older Adults using Touchscreen Devices: a Literature Review. In Proceedings of the 25th ICME conference francophone on l’Interaction Homme- Machine - IHM ’13 . ACM Press, New York, New York, USA, 125–134. https: //doi.org/10.1145/2534903.2534920 [85] Akram Mustafa and Mostafa Rahimi Azghadi. 2021. Automated Machine Learn- ingforHealthcareandClinicalNotesAnalysis. Computers 10,24(2021),31pages. Issue 2. https://doi.org/10.3390/computers10020024 [86] Rosemery O. Nelson and Steven C. Hayes. 1981. Theoretical Explanations for Reactivity in Self-Monitoring. Behavior Modification 5, 1 (1981), 3–14. https: //doi.org/10.1177/014544558151001 [87] Bijarne Martens Nes, Imre Janszky, Ulrik Wisløff, Asbjørn Støylen, and Trine Karlsen. 2013. Age-predicted Maximal Heart Rate in Healthy Subjects: The HUNT Fitness Study. Scandinavian Journal of Medicine & Science in Sports 23, 6 (Dec. 2013), 697–704. https://doi.org/10.1111/j.1600-0838.2012.01445.x [88] Carley O’Neill and Shilpa Dogra. 2016. Different Types of Sedentary Activities and Their Association with Perceived Health and Wellness among Middle-Aged and Older Adults: A Cross-Sectional Analysis. American Journal of Health Promotion 30, 5 (2016), 314–322. https://doi.org/10.1177/0890117116646334 [89] PAL Technologies Ltd. 2021. activPAL. Retrieved Sep 09, 2021 from https: //www.palt.com/ [90] Sinno Jialin Pan and Qiang Yang. 2010. A Survey on Transfer Learning. IEEE TransactionsonKnowledgeandDataEngineering 22,10(2010),1345–1359. https: //doi.org/10.1109/TKDE.2009.191 [91] Carolyn Pang, Zhiqin Collin Wang, Joanna McGrenere, Rock Leung, Jiamin Dai, and Karyn Moffatt. 2021. Technology Adoption and Learning Preferences for Older Adults:. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . ACM, New York, NY, USA, 1–13. https://doi.org/10.1145/ 3411764.3445702 [92] Cassandra Phoenix and Meridith Griffin. 2013. Narratives at work: what can stories of older athletes do? Ageing and Society 33, 2 (2013), 243–266. https: //doi.org/10.1017/S0144686X11001103 [93] Cassandra Phoenix and Noreen Orr. 2014. Pleasure: A forgotten dimension of physical activity in older age. Social Science & Medicine 115 (2014), 94–102. https://doi.org/10.1016/j.socscimed.2014.06.013 [94] Katrina L. Piercy, Richard P. Troiano, Rachel M. Ballard, Susan A. Carlson, Janet E. Fulton, Deborah A. Galuska, Stephanie M. George, and Richard D. Olson. 2018. The Physical Activity Guidelines for Americans. JAMA 320, 19 (11 2018), 2020–2028. https://doi.org/10.1001/jama.2018.14854 [95] José Pinheiro and Douglas Bates. 2000. Mixed-Effects Models in S and S-PLUS (1 ed.). Springer-Verlag, New York. 528 pages. https://doi.org/10.1007/b98882 [96] Stefania Pizza, Barry Brown, Donald McMillan, and Airi Lampinen. 2016. Smartwatch In Vivo. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM, New York, NY, USA, 5456–5469. https: //doi.org/10.1145/2858036.2858522 [97] Aditya Ponnada, Caitlin Haynes, Dharam Maniar, Justin Manjourides, and Stephen Intille. 2017. Microinteraction Ecological Momentary Assessment Response Rates: Effect of Microinteractions or the Smartwatch? Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 92 (Sept. 2017), 16 pages. https://doi.org/10.1145/3130957 [98] AlishaPradhan,AmandaLazar,andLeahFindlater.2020. UseofIntelligentVoice AssistantsbyOlderAdultswithLowTechnologyUse. ACMTrans.Comput.-Hum. Interact. 27, 4, Article 31 (Sept. 2020), 27 pages. https://doi.org/10.1145/3373759 [99] Archiki Prasad and Preethi Jyothi. 2020. How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics . Association for Computational Linguistics, Online, 3739–3753. https://doi.org/ 10.18653/v1/2020.acl-main.345 [100] EmilProtalinski.2017. Google’sSpeechRecognitionTechnologyNowHasa4.9% Word Error Rate. Retrieved Sep 09, 2021 from https://venturebeat.com/2017/05/ 17/googles-speech-recognition-technology-now-has-a-4-9-word-error-rate/ [101] W.JackRejeski,AnthonyP.Marsh,PeterH.Brubaker,MatthewBuman,RogerA.Fielding,DonHire,ToddManini,AlvitoRego,andMichaelE.Miller.2016.AnalysisandInterpretationofAccelerometryDatainOlderAdults:TheLIFEStudy. The Journals of Gerontology Series A: Biological Sciences and Medical Sciences 71, 4 (April 2016), 521–528. https://doi.org/10.1093/gerona/glv204 [102] AARP Research. 2016. Project Catalyst and HomeLab. Building a Better Tracker: Older Consumers Weigh In on Activity and Sleep Monitoring Devices. https: //doi.org/10.26419/res.00294.001 [103] Melanie Revilla, Mick P Couper, Oriol J Bosch, and Marc Asensio. 2020. Testing the use of voice input in a smartphone web survey. Social Science Computer Review 38, 2 (2020), 207–224. https://doi.org/10.1177/0894439318810715 [104] Dori Rosenberg, Rod Walker, Mikael Anne Greenwood-Hickman, John Bellet- tiere, Yunhua Xiang, KatieRose Richmire, Michael Higgins, David Wing, Eric B Larson, Paul K Crane, and Andrea Z LaCroix. 2020. Device-assessed physical activity and sedentary behavior in a community-based cohort of older adults. BMC Public Health 20 (8 2020), 1256. Issue 1. https://doi.org/10.1186/s12889- 020-09330-z [105] Sherry Ruan, Jacob O Wobbrock, Kenny Liou, Andrew Ng, and James A Landay. 2018. Comparing Speech and Keyboard Text Entry for Short Messages in Two Languages on Touchscreen Phones. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 4 (2018), 1–23. https://doi.org/ 10.1145/3161187 [106] Cormac G. Ryan, Paul M. Grant, William W. Tigbe, and Malcolm H. Granat. 2006. The Validity and Reliability of a Novel Activity Monitor as a Measure of Walking. BritishJournalofSportsMedicine 40,9(2006),779–784. https://doi.org/ 10.1136/bjsm.2006.027276 arXiv:https://bjsm.bmj.com/content/40/9/779.full.pdf [107] James F. Sallis and Brian E. Saelens. 2000. Assessment of Physical Activity by Self-Report: Status, Limitations, and Future Directions. Research Quarterly for Exercise and Sport 71, sup2 (June 2000), 1–14. https://doi.org/10.1080/02701367. 2000.11082780 [108] SiratSamyounandJohnStankovic.2021. VoiSense:HarnessingVoiceInteraction on a Smartwatch to Collect Sensor Data: Demo Abstract. In Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co- Located with CPS-IoT Week 2021) . ACM, New York, NY, USA, 388–389. https: //doi.org/10.1145/3412382.3458777 [109] JenniferA.Schrack,RachelCooper,AnnemarieKoster,EricJ.Shiroma,JoanneM.Murabito,W.JackRejeski,LuigiFerrucci,andTamaraB.Harris.2016.AssessingDailyPhysicalActivityinOlderAdults:UnravelingtheComplexityofMonitors,Measures,andMethods. The Journals of Gerontology Series A: Biological Sciences and Medical Sciences 71, 8 (Aug. 2016), 1039–1048. https://doi.org/10.1093/ gerona/glw026 [110] SedentaryBehaviourResearchNetwork.2012. LettertotheEditor:Standardized Use of the Terms “Sedentary” and “Sedentary Behaviours”. Applied Physiology, Nutrition, and Metabolism 37, 3 (June 2012), 540–542. https://doi.org/10.1139/ h2012-024",0.08790522875816993,0.0783156446969697,0.914020638896732,0.8951841452020202,,
228,146,20,Header,"[(0, 14)]",MyMove: Facilitating Older Adults to Collect In-Situ Activity Labels on a Smartwatch with Speech,0.08790522875816993,0.0783156446969697,0.5500562199346404,0.0871209477272727,,
229,147,20,Bibliography,"[(14, 24), (24, 48), (48, 67), (67, 83), (83, 97), (97, 122), (122, 129), (129, 142), (142, 164), (164, 185), (185, 200), (200, 222), (222, 243), (243, 257)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA [111] Lee Smith, Ulf Ekelund, and Mark Hamer. 2015. The Potential Yield of Non- Exercise Physical Activity Energy Expenditure in Public Health. Sports Medicine 45, 4 (April 2015), 449–452. https://doi.org/10.1007/s40279-015-0310-2 [112] Phillip B. Sparling, Bethany J. Howard, David W. Dunstan, and Neville Owen. 2015. Recommendations for Physical Activity in Older Adults. BMJ 350, jan20 6 (Jan. 2015), h100–h100. https://doi.org/10.1136/bmj.h100 [113] Arjun Srinivasan, Bongshin Lee, Nathalie Henry Riche, Steven M. Drucker, and KenHinckley.2020. InChorus:DesigningConsistentMultimodalInteractionsfor Data Visualization on Tablet Devices. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20) . ACM, New York, NY, USA, 1–13. https://doi.org/10.1145/3313831.3376782 [114] AnitaL.Stewart,KristinM.Mills,AbbyC.King,WilliamL.Haskell,DawnGillis,andPhilipL.Ritter.2001.CHAMPSPhysicalActivityQuestionnaireforOlderAdults:OutcomesforInterventions. Medicine & Science in Sports & Exercise 33, 7 (2001), 1126–1141. https://journals.lww.com/acsm-msse/Fulltext/2001/07000/ CHAMPS_Physical_Activity_Questionnaire_for_Older.10.aspx [115] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen. 2015. Smart Devices are Different: Assessing and Mitigating Mobile Sensing Heterogeneities for Activity Recognition. In Proceedings of the 13th ACM Con- ference on Embedded Networked Sensor Systems (SenSys ’15) . ACM, New York, NY, USA, 127–140. [116] Ali Raza Syed, Andrew Rosenberg, and Michael Mandel. 2017. Active Learning for Low-Resource Speech Recognition: Impact of Selection Size and Language Modeling Data. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, Washington, DC, USA, 5315–5319. https: //doi.org/10.1109/ICASSP.2017.7953171 [117] Catrine Tudor-Locke, Cora L Craig, Yukitoshi Aoyagi, Rhonda C Bell, Karen A",0.08790522875816993,0.0783156446969697,0.9120898797385619,0.3920036558080809,,
230,148,20,Paragraph,"[(257, 258)]","Croteau,IlseDeBourdeaudhuij,BenEwald,AndrewWGardner,YoshiroHatano,LesleyDLutes,SandraMMatsudo,FarahARamirez-Marrero,LauraQRogers,DavidARowe,MichaelDSchmidt,MarkATully,andStevenNBlair.2011.HowManySteps/dayAreEnough?ForOlderAdultsAndSpecialPopulations.",0.1184264705882353,0.39326148409090905,0.4822149754901961,0.43225618106060604,,
231,149,20,Bibliography,"[(258, 273), (273, 296), (296, 320), (320, 345), (345, 363), (363, 380), (380, 403), (403, 421), (421, 443), (443, 458), (458, 479), (479, 504), (504, 530), (530, 540), (540, 558), (558, 564), (564, 587), (587, 604), (604, 625), (625, 629), (629, 645), (645, 668), (668, 684), (684, 702), (702, 727), (727, 749), (749, 770), (770, 773), (773, 789), (789, 815), (815, 835), (835, 861), (861, 882), (882, 888)]","International Journal of Behavioral Nutrition and Physical Activity 8 (7 2011), 80. Issue 1. https://doi.org/10.1186/1479-5868-8-80 [118] Catrine Tudor-Locke and David A Rowe. 2012. Using Cadence to Study Free- Living Ambulatory Behaviour. Sports Medicine 42, 5 (May 2012), 381–398. https://doi.org/10.2165/11599170-000000000-00000 [119] Yonatan Vaizman, Katherine Ellis, Gert Lanckriet, and Nadir Weibel. 2018. Ex- traSensory App: Data Collection In-the-Wild with Rich User Interface to Self- Report Behavior. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI ’18) . ACM, New York, NY, USA, Article 554, 12 pages. https://doi.org/10.1145/3173574.3174128 [120] Niels van Berkel, Denzil Ferreira, and Vassilis Kostakos. 2017. The Experience Sampling Method on Mobile Devices. ACM Comput. Surv. 50, 6, Article 93 (Dec. 2017), 40 pages. https://doi.org/10.1145/3123988 [121] Dimitri Vargemidis, Kathrin Gerling, Vero Vanden Abeele, Luc Geurts, and Katta Spiel. 2021. Irrelevant Gadgets or a Source of Worry: Exploring Wearable Activity Trackers with Older Adults. ACM Trans. Access. Comput. 14, 3, Article 16 (Aug. 2021), 28 pages. https://doi.org/10.1145/3473463 [122] Dimitri Vargemidis, Kathrin Gerling, Katta Spiel, Vero Vanden Abeele, and Luc Geurts. 2020. Wearable Physical Activity Tracking Systems for Older Adults—A Systematic Review. ACM Trans. Comput. Healthcare 1, 4, Article 25 (Sept. 2020), 37 pages. https://doi.org/10.1145/3402523 [123] William N. Venables and Brian D. Ripley. 2002. Modern Applied Statistics with S (4th ed.). Springer, New York. https://www.stats.ox.ac.uk/pub/MASS4/ ISBN 0-387-95457-0. [124] Ravichander Vipperla, Steve Renals, and Joe Frankel. 2008. Longitudinal Study of ASR Performance on Ageing Voices. In Interspeech 2008 . ISCA, Winona, MN, USA, 2550–2553. [125] Marjolein Visser and Annemarie Koster. 2013. Development of a Questionnaire to Assess Sedentary Time in Older Persons – a Comparative Study using Ac- celerometry. BMC Geriatrics 13, 1 (Dec. 2013), 80. https://doi.org/10.1186/1471- 2318-13-80 [126] Emily A. Vogels. 2020. About One-in-Five Americans Use a Smartwatch or Fitness Tracker | Pew Research Center. Retrieved Sep 09, 2021 from https://www.pewresearch.org/fact-tank/2020/01/09/about-one-in-five- americans-use-a-smart-watch-or-fitness-tracker/ [127] Darren E.R. Warburton, Crystal Whitney Nicol, and Shannon S.D. Bredin. 2006. Health Benefits of Physical Activity: The Evidence. CMAJ: Canadian Medical Association Journal 174, 6 (2006), 801–809. [128] Alanna Weisberg, Alexandre Monte Campelo, Tanzeel Bhaidani, and Larry Katz. 2020. Physical Activity Tracking Wristbands for Use in Research With Older Adults: An Overview and Recommendations. Journal for the Measurement of PhysicalBehaviour 3,4(Dec.2020),265–273. https://doi.org/10.1123/jmpb.2019- 0050 [129] Gary M. Weiss. 2019. WISDM: Smartphone and Smartwatch Activity and Biometrics Dataset Data Set. https://archive.ics.uci.edu/ml/datasets/WISDM+ Smartphone+and+Smartwatch+Activity+and+Biometrics+Dataset+ [130] Gary M. Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and Smartwatch-Based Biometrics using Activities of Daily Living. IEEE Access 7 (2019), 133190–133202. [131] Douglas J. Wiebe, Bernadette A. D’Alonzo, Robin Harris, Margot Putukian, and CarolynCampbell-McGovern.2018. JointPrevalenceofSittingTimeandLeisure- Time Physical Activity Among US Adults, 2015-2016. Journal of American Medical Association 320, 19 (Nov. 2018), 2035. https://doi.org/10.1001/jama.2018. 14165 [132] Christopher K. Wong, Helena M. Mentis, and Ravi Kuber. 2018. The Bit Doesn’t Fit: Evaluation of a Commercial Activity-Tracker at Slower Walking Speeds. Gait & posture 59 (2018), 177–181. [133] W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke. 2018. The Microsoft 2017 Conversational Speech Recognition System. In 2018 IEEE Inter- national Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, Washington,DC,USA,5934–5938. https://doi.org/10.1109/ICASSP.2018.8461870 [134] XinghuiYan,ShritiRaj,BingjianHuang,SunYoungPark,andMarkW.Newman.2020.TowardLightweightIn-situSelf-reporting:AnExploratoryStudyofAlternativeSmartwatchInterfaceDesignsinContext. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 4 (Dec. 2020), 1–22. https://doi.org/10.1145/3432212 [135] Lin Yang, Chao Cao, Elizabeth D. Kantor, Long H. Nguyen, Xiaobin Zheng, Yikyung Park, Edward L. Giovannucci, Charles E. Matthews, Graham A. Colditz, and Yin Cao. 2019. Trends in Sedentary Behavior Among the US Population, 2001-2016. JAMA 321, 16 (04 2019), 1587–1597. https://doi.org/10.1001/jama. 2019.3636 [136] Hanlu Ye, Meethu Malu, Uran Oh, and Leah Findlater. 2014. Current and Future Mobile and Wearable Device Use by People with Visual Impairments. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Toronto, Ontario, Canada) (CHI ’14) . Association for Computing Machinery, New York, NY, USA, 3123–3132. https://doi.org/10.1145/2556288.2557085",0.08790522875816993,0.11149001944444449,0.9140181775555555,0.6616337063131313,,
