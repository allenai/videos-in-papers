,index,page,type,intervals,text,x1,y1,x2,y2,block_type,block_id
0,1,0,Title,"[(0, 4), (4, 12), (12, 17)]",When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models,0.11796895424836602,0.10411012500000007,0.8820289130718953,0.17616366035353534,,
1,2,0,Author,"[(17, 19), (19, 21), (21, 25), (25, 26), (26, 28), (28, 30), (30, 34), (34, 35)]","Amy Rechkemmer Purdue University West Lafayette, Indiana, USA arechke@purdue.edu Ming Yin Purdue University West Lafayette, Indiana, USA mingyin@purdue.edu",0.20258823529411762,0.18954969898989893,0.7988275999999999,0.24934253257575795,,
2,3,0,Abstract,"[(35, 36), (36, 46), (46, 56), (56, 67), (67, 77), (77, 86), (86, 95), (95, 108), (108, 116), (116, 126), (126, 136), (136, 148), (148, 157), (157, 168), (168, 178), (178, 186), (186, 187)]","ABSTRACT Previous research shows that laypeople’s trust in a machine learn- ing model can be affected by both performance measurements of the model on the aggregate level and performance estimates on in- dividual predictions. However, it is unclear how people would trust the model when multiple performance indicators are presented at the same time. We conduct an exploratory human-subject experi- ment to answer this question. We find that while the level of model confidence significantly affects people’s belief in model accuracy, both the model’s stated and observed accuracy generally have a larger impact on people’s willingness to follow the model’s pre- dictions as well as their self-reported levels of trust in the model, especially after observing the model’s performance in practice. We hope the empirical evidence reported in this work could open doors to further studies to advance understanding of how people perceive, process, and react to performance-related information of machine learning.",0.08790522875816985,0.2610529066919195,0.4829470118169935,0.49913531717171716,,
3,4,0,Paragraph,"[(187, 189)]",CCS CONCEPTS,0.08790522875816993,0.5132271491161616,0.22029393088235297,0.5270012652777778,,
4,4,0,Paragraph,"[(189, 199), (199, 205)]",• Human-centered computing → Empirical studies in HCI ; • Computing methodologies → Machine learning .,0.08790522875816993,0.531514511111111,0.4804660235294118,0.5575898626262626,,
5,5,0,Keywords,"[(205, 206), (206, 213), (213, 214)]","KEYWORDS Machine learning, confidence, accuracy, trust, human-subject ex- periments",0.08790522875816993,0.571681694570707,0.48293592102483673,0.6160456707070707,,
6,6,0,Paragraph,"[(214, 227), (227, 249), (249, 268)]","ACM Reference Format: Amy Rechkemmer and Ming Yin. 2022. When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Ma- chine Learning Models. In CHI Conference on Human Factors in Computing Systems (CHI ’22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3491102.3501967",0.08742320261437908,0.6242285266414141,0.48266210691062067,0.6972083753787879,,
7,7,0,Section,"(268, 270)",1 INTRODUCTION,0.08790522875816993,0.7125857349747475,0.2557663312091503,0.7263598511363636,,
8,8,0,Paragraph,"[(270, 278), (278, 289), (289, 299)]","Today, numerous innovative machine learning (ML) models have been rapidly developed and applied to a wide range of application scenarios to assist people, from decision-making in everyday life to",0.0874656862745098,0.7317913777777778,0.4804687984836601,0.7707868323232324,,
9,8,0,Paragraph,"[(299, 330)]",Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed,0.08790522875816993,0.7837160295454545,0.48068126775816994,0.8025844638888888,,
10,9,0,Footnote,"[(330, 360), (360, 374), (374, 391), (391, 406)]","for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,topostonserversortoredistributetolists,requirespriorspecificpermission and/or a fee. Request permissions from permissions@acm.org. CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-9157-3/22/04...$15.00",0.08711928104575163,0.8038422921717172,0.48067784922875817,0.8851034032828282,,
11,10,0,Footer,"[(406, 407)]",https://doi.org/10.1145/3491102.3501967,0.08790522875816993,0.8863612315656566,0.2727792993464053,0.8951665345959596,,
12,11,0,Paragraph,"[(407, 415), (415, 425), (425, 433), (433, 443), (443, 453), (453, 463), (463, 473), (473, 483)]","problem-solving in critical societal challenges. For example, neural networks have been used to forecast traffic speed in large-scale transportation networks and recommend optimal routes [16, 40]. Researchers have trained ML models to predict poverty in develop- ing countries and help local governments better allocate their scarce resources [27]. ML has also shown potential in accurately predict- ing the household re-entry to homeless system, which can further inform the design of more effective intervention strategies [23, 32].",0.5195343137254902,0.26296183232323234,0.9145751097934643,0.3711428929292929,,
13,11,0,Paragraph,"[(483, 494), (494, 505), (505, 515), (515, 526), (526, 536), (536, 547), (547, 558), (558, 571), (571, 583), (583, 593), (593, 603), (603, 612)]","With the rapid growth of user-facing systems that are built on top of ML models, a growing line of research on understanding whether and how do end-users trust these models has recently emerged [52, 57, 58]. Many different factors have been identified as influencing people’s trust in ML, such as people’s understanding of how the model works [19, 37] and people’s perceptions on whether the model is biased [9, 62]. Perhaps more intuitively, people’s trust in an ML model is also highly dependent on how well the model can perform. For example, it was found that people’s trust in an ML model is significantly affected by the model’s performance, as measured by both the model’s stated accuracy on some held-out data and its observed accuracy in practice [35, 58].",0.5189918300653594,0.373658802020202,0.9124822940653596,0.5371870848484849,,
14,11,0,Paragraph,"[(612, 621), (621, 630), (630, 643), (643, 653), (653, 666), (666, 675), (675, 684), (684, 694), (694, 704), (704, 716), (716, 724)]","While a performance metric like accuracy may provide useful summary information for people to evaluate the overall reliability of an ML model across a set of predictions, it contains little insight into how likely each individual prediction that the model makes is correct. On the other hand, an ML model can often quantify its uncertainty on each individual prediction using a confidence score, which represents the model’s accuracy estimate on that particular prediction [25, 43, 46, 60]. More recently, researchers have found that such model confidence also influences how much people would be willing to trust the model [61]—people trust the ML model more in cases when the model has higher confidence.",0.5189918300653594,0.5397029939393939,0.913707857840523,0.6893941555555556,,
15,11,0,Paragraph,"[(724, 733), (733, 746), (746, 755), (755, 765), (765, 778), (778, 787), (787, 797), (797, 809), (809, 817), (817, 829), (829, 840), (840, 849), (849, 858), (858, 862)]","Our knowledge of how different kinds of performance indicators of an ML model alone affects people’s trust in the model continues to grow. However, there remains a key, but currently under-explored, aspect in further advancing our understanding of people’s trust in ML—that is, how would people trust an ML model in the presence of multiple performance indicators of it. For example, when informa- tion on both the model’s accuracy measurements (on some held-out data and/or on a number of real-world trials) and the model’s con- fidence estimates on individual predictions are provided, which one(s) would people choose to rely upon in deciding how much to trust the model? And what’s the role of one performance indicator— say model confidence—in moderating or even changing the effects of other performance indicators (e.g., model accuracy) on people’s trust in the model?",0.5195343137254902,0.691908802020202,0.9145722728784311,0.8831113272727273,,
16,12,1,Header,"[(0, 10), (10, 15)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Amy Rechkemmer and Ming Yin",0.08790522875816993,0.0783156446969697,0.9120931493464052,0.0871209477272727,,
17,13,1,Paragraph,"[(15, 26), (26, 40), (40, 49), (49, 57), (57, 67), (67, 79), (79, 89), (89, 99), (99, 110), (110, 120), (120, 131), (131, 142), (142, 153), (153, 163), (163, 174), (174, 181), (181, 190), (190, 201), (201, 211), (211, 224), (224, 233)]","Specifically, we are interested in finding out the answers to these questions for those users of ML models who are laypeople . This is be- cause laypeople’s interactions with ML models are becoming highly prevalent today, especially in various low-stakes decision making settings such as optimal route planning [16, 40] and entertainment selection [6]. This implies that, due to the legal requirement of trans- parency and design guidelines of best practices [1], laypeople are increasingly exposed to a variety of performance information of an ML model during their usage of the model. However, our empirical understanding of how laypeople would interpret and act upon this rich set of performance information of the ML model and whether their reactions to this information leads to appropriate trust in the model is still largely lacking. In fact, prior research has indicated that laypeople may experience difficulty in making use of numerical information [44, 48], and they may follow an incorrect ML model recommendation even when performance information about the recommendation suggesting it as less trustworthy is given [51]. Thus, such understanding is critical for us to analyze how effective the current ways of ML model performance communication are for a target user population of laypeople, and it can also inform us of how to make such communication truly useful to laypeople.",0.0874656862745098,0.10956031717171716,0.4829477664836601,0.3976214282828283,,
18,13,1,Paragraph,"[(233, 243), (243, 250), (250, 262), (262, 268)]","Therefore, in this paper, we conduct an exploratory study to experimentally examine how multiple performance indicators of an ML model, together, affect laypeople’s trust in the model in a low-stakes decision-making task. Specifically, we ask:",0.08790522875816993,0.40013733737373736,0.4804724612287582,0.4529699131313132,,
19,14,1,List,"[(268, 278), (278, 288), (288, 297), (297, 307), (307, 314), (314, 325), (325, 332), (332, 342), (342, 352), (352, 362), (362, 371)]","• When people receive information on both the model’s stated accuracy on held-out data and the model’s confidence on indi- vidual predictions, but have not observed the model’s accuracy in practice yet, does the stated accuracy (alternatively, the model confidence) affect people’s trust in the model? • Does the answer for the question above change after people have observed the model’s accuracy in practice? • Does a model’s observed accuracy in practice affect people’s trust in the model, in the presence of model confidence? • How do model confidence and model accuracy interact with each other to influence people’s trust in the model?",0.09168954248366014,0.4580009737373737,0.48293450997124177,0.6133524888888888,,
20,15,1,Paragraph,"[(371, 382), (382, 394), (394, 404), (404, 414), (414, 424), (424, 434), (434, 445), (445, 457), (457, 468), (468, 480), (480, 493), (493, 503), (503, 516), (516, 525), (525, 538), (538, 549), (549, 558), (558, 571), (571, 582), (582, 589)]","Conjecturing the answer to any of these questions turns out to be quite challenging. On the one hand, compared to a model’s stated accuracy, model confidence is provided on the level of individual prediction and appears more directly relevant for people to evaluate whether each of the model’s predictions is trustworthy. After all, the model’s stated accuracy is obtained on held-out data which may be fundamentally different from the current use cases at hand. Following this line of thinking, one may expect to see a significant impact of model confidence on people’s trust in the model, and perhaps the effect of the model’s stated accuracy on trust is minimal, if any. On the other hand, a model’s confidence on a prediction is only the model’s “estimate” on how likely the prediction would be correct, and it has been shown in many studies in the machine learning community that the raw confidence scores produced by an ML model can be poorly calibrated [25, 33, 42]. That is, the confidence estimate an ML model associates to a prediction does not necessarily reflect the true correctness likelihood of that prediction. In this case, it is reasonable to hypothesize that people’s trust in an ML model would still be affected by the model’s stated accuracy, but not so much by model confidence.",0.08736437908496732,0.6183848121212121,0.4827208739346407,0.892608802020202,,
21,15,1,Paragraph,"[(589, 599), (599, 611), (611, 620), (620, 631), (631, 642), (642, 652), (652, 664), (664, 675), (675, 684), (684, 695), (695, 698)]","Even more complicated, after people get the chance to interact with the ML model and observe its accuracy in practice, people may make additional inference on how calibrated the model’s confidence and how reliable the model’s stated accuracy is through their limited interactions with the model. Thus, in deciding how much to trust the model after observing its performance in practice, people may need to consider both relevance and reliability for each of the three pieces of information that can be used to gauge the trustworthiness of the ML model—model confidence, stated accuracy, and observed accuracy. It is unclear how these three factors would affect trust, separately and collectively.",0.5189918300653594,0.10956031717171716,0.9137122017254903,0.25925147878787885,,
22,15,1,Paragraph,"[(698, 708), (708, 717), (717, 727), (727, 739), (739, 750), (750, 766), (766, 776), (776, 787), (787, 799), (799, 810), (810, 820), (820, 829), (829, 838), (838, 843)]","To answer these questions, we designed and conducted a random- ized behavioral experiment in which we recruited human subjects from Amazon Mechancial Turk to complete a sequence of decision making tasks (i.e., predict speed dating outcome) with the help of an ML model. Our experiment consisted of a total of eight treatments, arranged in a 2 × 2 × 2 design, and ML models used in different treat- ments differed along three factors—the ML model’s confidence level , stated accuracy , and observed accuracy . Due to the multidimensional nature of trust, we used a variety of measures to quantify subjects’ trust in the ML model in our experiment, including subjective mea- sures focusing on trust perceptions (e.g., subject’s belief in model competence and self-reported trust level) and more objective mea- sures characterizing trusting behavior (e.g., frequency for a subject to “follow” a model’s prediction).",0.5195343137254902,0.2617673878787879,0.9145767309803923,0.4529699131313132,,
23,15,1,Paragraph,"[(843, 851), (851, 861), (861, 873), (873, 883), (883, 891), (891, 901), (901, 909), (909, 920), (920, 930), (930, 939), (939, 950), (950, 962), (962, 971), (971, 980), (980, 990)]","Our experimental results show that, overall, model confidence affects people’s belief in model competence but has no reliable impact on their self-reported levels of trust in the model or their willingness to follow the model’s predictions, both before and after they have observed the model’s performance through real-world trials. In contrast, the model’s accuracy, including both the stated accuracy and observed accuracy, consistently and significantly af- fects people’s trust in the model in all dimensions. Comparing the magnitude of the effects of different performance indicators on var- ious measures of trust, we found that model accuracy—especially the model’s observed accuracy after it has been obtained from real- world trials—has a larger effect on all measures of trust except for people’s belief in model competence. Further analyses also reveal that there exist some interactions between model confidence and model accuracy in influencing people’s trust in an ML model.",0.5189918300653594,0.45548455959595957,0.9145660315607843,0.6605242060606061,,
24,15,1,Paragraph,"[(990, 998), (998, 1008), (1008, 1019), (1019, 1029), (1029, 1040), (1040, 1050), (1050, 1060), (1060, 1070), (1070, 1081), (1081, 1091), (1091, 1100), (1100, 1111), (1111, 1121), (1121, 1130), (1130, 1139), (1139, 1142)]","Taken together, our results provide exploratory evidence that model confidence and model accuracy play different roles in influ- encing people’s trust in an ML model. They also highlight behavior of people when they react to multiple performance indicators that can potentially be irrational, such as their over-reliance on a model’s observed accuracy despite it having been obtained through a small number of real-world trials. We conclude by discussing the design implications and limitations of our work. In particular, proper cau- tions should be used when generalizing our results to other settings. More confirmatory studies should be conducted to examine to what extent our exploratory findings still hold for different populations, on different types of tasks, and using different ways to communi- cate model performance. We hope these findings will inspire more theoretical investigations into the mechanisms of how people make sense of and make use of various performance-related information of machine learning.",0.5195343137254902,0.6630401151515152,0.914569005709804,0.8819168828282828,,
25,16,2,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636911764705882,0.0783156446969697,0.9120929186274509,0.0871209477272727,,
26,17,2,Section,"(10, 13)",2 RELATED WORK,0.08790522875816993,0.10765139154040401,0.2574953879084968,0.12142550770202028,,
27,18,2,Paragraph,"[(13, 23), (23, 34), (34, 46), (46, 54), (54, 67), (67, 79), (79, 90), (90, 102), (102, 113), (113, 125)]","Research on understanding whether, when, and how do people trust the outputs of an ML system has received increased attention over the years. It was shown that, for example, people accept ML model recommendations over human suggestions or their own judgment in an unfamiliar domain [11, 38], but they quickly lose trust in an algorithm after seeing it err [17]. Such decrease in trust is reduced if people are given the control to adjust the algorithm predictions rather than simply accepting them as is [18]. However, loss of trust due to incorrect ML system decisions cannot be fully gained back by the same amount of correct decisions made by the system [59].",0.08790522875816993,0.1268557717171717,0.48085089605228765,0.2627110747474747,,
28,18,2,Paragraph,"[(125, 134), (134, 144), (144, 156), (156, 166), (166, 177), (177, 188), (188, 197), (197, 209), (209, 222), (222, 234), (234, 244), (244, 254), (254, 265), (265, 276), (276, 284), (284, 295), (295, 306), (306, 317), (317, 329), (329, 332)]","More recently, a growing number of experimental studies have been conducted to examine what the key influencing factors are in determining people’s trust in an ML model and how. One such factor is the ML model’s intelligibility [19], though different stud- ies showed inconsistent results to this end. For instance, Lai and Tan [35] found that in the context of deception detection, show- ing explanations of an ML model significantly increases people’s trust in the model. In contrast, the transparency of an ML model, in terms of whether the model is a clear or black-box model, was shown to not affect people’s trust in the model [47], while provid- ing explanations for a text emotion predicting system only affected trust before people experienced the system [50]. Moreover, it was found that the type of explanation presented to users impacted their ability to appropriately calibrate trust in a model [55]. Other than the model’s intelligibility, researchers have also demonstrated that the level of agreement between humans and the ML model [39], a human’s mental model of the ML model’s error boundary [4], and the compatibility of an ML model update with humans’ prior experience of the model [5], can all affect how much people will trust the model.",0.0874656862745098,0.26522698383838383,0.4829427331346405,0.5394509737373737,,
29,18,2,Paragraph,"[(332, 343), (343, 355), (355, 364), (364, 375), (375, 385), (385, 397), (397, 410), (410, 420), (420, 429), (429, 439), (439, 450), (450, 457), (457, 468), (468, 479), (479, 491), (491, 501), (501, 512), (512, 523), (523, 533), (533, 545), (545, 557), (557, 569), (569, 581), (581, 591), (591, 599)]","Another natural influencing factor of trust in an ML model is the model’s performance. Indeed, Yin et al. [58] ran a sequence of human-subject experiments and found that both an ML model’s stated accuracy on held-out data and its observed accuracy in prac- tice significantly affect laypeople’s trust in the model. Yet, even for models with the same level of accuracy, the ways that expecta- tions on model performance are set prior to the use of them were also found to affect people’s perceptions and acceptance of the model [31]. Beyond the accuracy measurements, a model’s confi- dence on each individual prediction is also relevant in influencing end-user’s trust. For example, a few previous studies have found that for context-aware systems and autonomous systems, displaying system confidence on the quality of its decision aids (e.g., memory aids or location predictions) would both improve user’s trust in the system [2, 54] and increase user performance in the task [3, 15]. Lim and Dey [36], however, showed that displaying low confidence levels of a context-aware system allows users to realize the limited capabilities of the system and thus leads to decreased trust. Most recently, Zhang et al. [61] conducted an experimental study and showed that confidence scores of an ML model help people to cali- brate their trust with confidence levels, such that they trust an ML model more when its confidence is high. Suresh et al. [51], however, found that a person’s capability of calibrating trust in an ML model based on its confidence may vary with the person’s characteristics, such as the person’s math and logic skills.",0.08790522875816993,0.5419668828282829,0.48294701181699357,0.8853752161616161,,
30,18,2,Paragraph,"[(599, 609), (609, 620), (620, 629), (629, 639), (639, 647), (647, 658), (658, 666), (666, 678), (678, 688), (688, 697), (697, 706), (706, 716), (716, 729), (729, 739), (739, 745), (745, 754), (754, 764), (764, 770)]","Differing from previous research, we look into how people trust in an ML model when they have multiple types of performance information that they can leverage to infer the trustworthiness of the model. Earlier literature has explored how humans would utilize multiple, possibly probabilistic, pieces of information dur- ing their decision-making when machines are not in the loop. For example, in processing multiple pieces of performance informa- tion of stocks in a sequence, a recency effect was observed among investors showing that they heavily relied upon the most recent information in their final decisions [45]. Various mechanisms have also been studied on how decision-makers aggregate a probabilis- tic forecast of multiple experts with different levels of confidence and bias [8, 53, 56]. However, to the best of our knowledge, there are no experimental studies or theoretical models on how humans interpret multiple performance-related information of ML-based decision aids, especially when different pieces of information are of differing nature and provided at different granularity (i.e., local accuracy estimate vs. aggregate accuracy measurements).",0.5195343137254902,0.10956031717171716,0.9145708224313727,0.3561100646464647,,
31,19,2,Section,"(770, 773)",3 EXPERIMENTAL DESIGN,0.5195343137254902,0.3821892703282828,0.7563972562091503,0.39596338648989904,,
32,20,2,Paragraph,"[(773, 784), (784, 794), (794, 803), (803, 811), (811, 813)]","To understand how laypeople’s trust in an ML model for making low-stakes decisions is affected by both model confidence and model accuracy, we designed and conducted a randomized behavioral ex- periment with human subjects recruited from Amazon Mechanical Turk (MTurk).",0.51909477124183,0.4013949131313131,0.9145767853803923,0.4680646101010101,,
33,21,2,Section,"(813, 820)",3.1 Experimental Tasks: Predicting Speed Dating Outcome,0.5195343137254902,0.49414381578282823,0.8650069723856209,0.5242702046717171,,
34,22,2,Paragraph,"[(820, 832), (832, 844), (844, 854), (854, 866), (866, 878), (878, 880)]","Each subject in our experiment was asked to complete a sequence of 40 tasks on predicting the outcome of speed dating events with the help from a pre-trained ML model. Specifically, in each prediction task the subject was presented with a profile of one participant in a speed dating event along with some information about his or her date, including:",0.5191977124183007,0.5297017313131314,0.9123785229803922,0.6102085494949495,,
35,23,2,List,"[(880, 892), (892, 898), (898, 909), (909, 919), (919, 927), (927, 939), (939, 950), (950, 957), (957, 971), (971, 983), (983, 993), (993, 1005), (1005, 1012)]","• Basic demographics of the participant and the date , e.g., gender, age, field of study, and race. • The participant’s dating preferences , which consisted of the par- ticipant’s allocation of 100 points to six attributes (e.g., attrac- tiveness, sincerity, intelligence) to show the relative importance of them in relation to one another when it comes to romantic attraction, as well as the level of importance for the participant to date someone of the same race. • The participant’s rating of the date , with respect to the six at- tributes using a scale from one to ten, and two additional self- reported scores on how happy the participant expected to be with the date and how much the participant liked the date, again in the range of one to ten.",0.5233169934640522,0.61523961010101,0.9145772864836601,0.7963790040404041,,
36,24,2,Paragraph,"[(1012, 1024), (1024, 1034), (1034, 1045), (1045, 1057), (1057, 1071), (1071, 1084), (1084, 1095)]","Figure 1 shows an example of the task interface. Profiles that we showed to subjects (e.g., Figure 1A) were taken from participants of real-life speed dating events in an experimental study [22]. At the end of each speed dating event, the participant was asked to indicate if he/she would want to see the date again in the future, and this is what we asked the subject to predict. In particular, the subject followed a 4-step procedure in each task to make the prediction:",0.5195343137254902,0.8014100646464647,0.91209824972549,0.8957527414141414,,
37,25,3,Header,"[(0, 10), (10, 15)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Amy Rechkemmer and Ming Yin",0.08790522875816993,0.0783156446969697,0.9120931493464052,0.0871209477272727,,
38,26,3,Caption,"[(15, 26), (26, 38), (38, 49), (49, 61), (61, 71), (71, 80), (80, 92), (92, 102), (102, 112), (112, 120), (120, 130), (130, 139), (139, 149), (149, 150)]","Figure 1: Interface of the experimental task in Phase 1. A: subjects are shown a profile of one participant in a speed dat- ing event along with some information about his or her date; B: based on this profile, subjects are asked to make an initial binary prediction of the speed dating event outcome; C: the ML model’s binary prediction and confidence score are then shown to subjects; D: subjects are asked to make a final pre- diction of the speed dating event outcome; E: subjects are asked to indicate their belief in the model’s prediction be- ing correct; depending on whether subjects’ final prediction agreed with the model’s prediction or not, we rephrased this question as equivalent to reporting subjects’ belief in their own final prediction being correct (if agreed) or wrong (if disagreed).",0.08790522875816993,0.5538684828282828,0.4830861228758171,0.7450697454545455,Figure,4.0
39,27,3,List,"[(150, 163), (163, 174), (174, 187), (187, 201), (201, 213), (213, 223), (223, 233), (233, 244), (244, 257), (257, 268), (268, 280), (280, 284), (284, 298), (298, 309), (309, 320), (320, 329), (329, 340), (340, 350), (350, 360), (360, 371)]","• Step 1 : the subject needed to carefully review the profile and make her own binary prediction about whether or not the partic- ipant in the profile would want to see the date again (Figure 1B). • Step 2 : then, the subject was shown an ML model’s binary pre- diction on the current case as well as a confidence score between 0 and 1 representing how confidently the model believed its prediction was correct; the higher the score, the more certain the model was that it made a correct prediction (Figure 1C). • Step 3 : after considering both her own prediction and the model’s prediction, the subject was asked to make a final binary predic- tion on whether or not the participant would want to see the date again (Figure 1D). • Step 4 : finally, before moving on to the next task, the subject needed to indicate her belief in the ML model’s prediction be- ing correct as a number between 0 (“the model’s prediction is absolutely wrong”) and 1 (“the model’s prediction is absolutely correct”) 1 . To help the subject contextualize this question, de- pending on whether the subject’s final prediction in the task agreed with the model’s prediction, we further indicated to sub- jects that this question is effectively the same as reporting how",0.09168954248366012,0.10956031717171716,0.9145762094326798,0.8957527414141414,,
40,28,3,Paragraph,"[(371, 381), (381, 388)]",much they believe their final prediction was “also correct” (if agreed) or “wrong” (if disagreed) (Figure 1E).,0.5371160130718954,0.2774908727272727,0.9120908610718955,0.30264920606060614,,
41,28,3,Paragraph,"[(388, 398), (398, 410), (410, 419), (419, 431), (431, 439), (439, 447), (447, 458), (458, 468), (468, 476), (476, 486), (486, 498), (498, 506)]","The task of predicting romantic relationship is suitable for our study for two main reasons. First, such a task does not require special expertise or domain knowledge and involves relatively lim- ited risks, so it can be easily understood by our laypeople subjects (i.e., MTurk workers) while still representing realistic low-stakes decision-making tasks that laypeople undertake in their day-to-day life well. Second, ML models have been developed to make predic- tions in romantic attraction and compatibility [24, 28, 41], which makes the experimental setting sufficiently credible and ensures the ecological validity of our study. Note that similar prediction tasks were previously used in [58] to explore the effects of model accuracy alone on people’s trust in ML models.",0.51909477124183,0.30579390303030307,0.9145667348078433,0.469333507070707,,
42,29,3,Section,"(506, 509)",3.2 Experimental Procedure,0.5195343137254902,0.4839026541666666,0.7571459199346405,0.49767677032828284,,
43,30,3,Paragraph,"[(509, 521), (521, 532), (532, 543), (543, 553), (553, 564), (564, 573), (573, 583), (583, 595), (595, 607), (607, 618), (618, 630), (630, 642), (642, 655), (655, 666), (666, 680), (680, 692), (692, 701), (701, 714), (714, 724), (724, 734), (734, 747), (747, 763), (763, 768)]","Figure 2 shows a flowchart of our experiment, in which each subject completed a sequence of 40 prediction tasks that were divided into two phases. Specifically, before a subject started to work on any prediction tasks, we explained the prediction tasks to the subject and walked through an example of the task interface with step- by-step instructions detailing each component of the speed dating profile (Figure 1A). Subjects were also given basic written instruc- tions on how to complete the task and explaining the meaning of a confidence score assigned to a prediction by the model. We also revealed the ML model’s stated accuracy to the subject by stating “We previously evaluated this model on a large data set of speed dating participants and its accuracy was x %, i.e., the model’s pre- dictions were correct on x % of the speed dating participants in this data set.” After reading the instruction, the subject started Phase 1 of the experiment to work on a set of 20 prediction tasks in succes- sion, and in each task, the subject followed the 4-step procedure as described above. In particular, when the model’s binary prediction was shown to the subject in Step 2 of each task, we communicated the model’s confidence about this prediction to the subject through the sentence “The model makes this prediction with a confidence score of y , (i.e., the model believes the chance for this prediction to be correct is 100 × y %),” and we also reminded the subject of the model’s stated accuracy (Figure 1C).",0.5178790849673203,0.5031082969696969,0.9145770197960784,0.8188423878787878,,
44,30,3,Paragraph,"[(768, 778), (778, 789), (789, 802)]","The subject received no feedback on whether her prediction or the model’s prediction was correct on each of the individual tasks in Phase 1. However, after all 20 tasks in Phase 1 were completed,",0.5195343137254902,0.821358296969697,0.913704568564706,0.8603537515151516,,
45,31,3,Footnote,"[(802, 832)]","1 Both a slider and a text box were provided for subjects to indicate their belief in the model’s correctness, and the number could be reported to the hundredths place.",0.519328431372549,0.8740949791666667,0.9120898806986927,0.8951665345959596,,
46,32,4,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636911764705882,0.0783156446969697,0.9120929186274509,0.0871209477272727,,
47,33,4,Paragraph,"[(10, 13), (13, 18), (18, 20), (20, 26), (26, 32), (32, 33), (33, 39), (39, 41), (41, 43), (43, 44), (44, 48), (48, 51), (51, 53), (53, 57), (57, 59)]",• Interface tutorial • Reveal model’s stated accuracy ✕ 20 • Reveal model’s Phase 1 accuracy • Reveal subject’s own Phase 1 accuracy • Subject’s self- reported trust in both phases • Demographics Introduction Phase 1 (20 tasks) Phase 1 feedback ✕ 20 Phase 2 (20 tasks) Exit Survey,0.17835211272156862,0.11480587625958137,0.800878463372549,0.21276832125215162,,
48,34,4,Caption,"[(59, 65)]",Figure 2: Flowchart of the experiment.,0.37277941176470586,0.24304651313131312,0.6272229882352941,0.25436772525252527,Figure,8.0
49,35,4,Paragraph,"[(65, 76), (76, 89), (89, 99), (99, 109), (109, 120), (120, 132), (132, 142), (142, 155), (155, 164), (164, 176), (176, 184)]","we provided a brief summary to the subject reporting the model’s overall accuracy on the 20 tasks of Phase 1 (i.e., the model’s “ob- served accuracy”) along with the subject’s own accuracy on those 20 tasks before seeing the model predictions (accuracy feedback on individual tasks was not given though). Then, the subject went on to complete another set of 20 prediction tasks in Phase 2 following a similar procedure. The only difference was that whenever the model prediction was shown in each of the tasks in Phase 2, in addition to displaying the model’s confidence, we reminded the subject of not only the model’s stated accuracy, but also its observed accuracy on the 20 tasks in Phase 1.",0.08736437908496729,0.2883532464646465,0.4829349247581702,0.4380444080808081,,
50,35,4,Paragraph,"[(184, 194), (194, 209), (209, 216)]","Finally, after completing all 40 prediction tasks, the subject was asked to report her level of trust in the ML model in each phase of the experiment by answering two exit-survey questions:",0.08790522875816993,0.4405603171717171,0.48046506668758165,0.47955577171717173,,
51,36,4,List,"[(216, 227), (227, 238), (238, 248), (248, 249), (249, 260), (260, 271), (271, 281), (281, 282)]","• How much did you trust our machine learning model’s predic- tions on the first twenty speed dating participants (that is, before you saw any feedback on your performance and the model’s performance)? • How much did you trust our machine learning model’s predic- tions on the last twenty speed dating participants (that is, after you saw any feedback on your performance and the model’s performance)?",0.09168954248366014,0.48270046868686867,0.4829414252339871,0.5927678929292929,,
52,37,4,Paragraph,"[(282, 294), (294, 308), (308, 317), (317, 326)]","The subject answered these questions using a scale from 1 (“I didn’t trust it at all”) to 10 (“I fully trusted it”). We also collected some basic demographic information (e.g., gender, age, and highest level of education) from the subject through the exit survey.",0.0874656862745098,0.5959125898989899,0.48047178886274516,0.6487451656565657,,
53,38,4,Section,"(326, 329)",3.3 Experimental Treatments,0.08790522875816993,0.6627031592171717,0.33599812712418303,0.676477275378788,,
54,39,4,Paragraph,"[(329, 344), (344, 348)]",We considered a total of 8 experimental treatments with a 2 × 2 × 2 design along three factors:,0.08720261437908497,0.6809917838383838,0.4804647197908497,0.7070671353535354,,
55,40,4,List,"[(348, 361), (361, 372), (372, 382), (382, 395), (395, 406), (406, 415), (415, 427), (427, 436), (436, 449), (449, 463)]","• Confidence level : the level of confidence scores that the ML model associates to its predictions, which has two levels — low and high. In the low confidence treatments, the confidence scores of the ML model on all 40 tasks were between 0.5 and 0.8, while the confidence scores of the ML model for the high confidence treatments were between 0.8 and 1 on all tasks. • Stated accuracy : the ML model’s stated accuracy on the held-out data, which has two levels — 60% and 90%. • Observed accuracy : the ML model’s accuracy on the Phase 1 tasks (i.e., the first 20 tasks), which also has two levels — 55% and 95%.",0.09168954248366014,0.712098195959596,0.4827191860915035,0.8517262262626262,,
56,41,4,Paragraph,"[(463, 472), (472, 482), (482, 493)]","Figure 3 illustrates the design of our experimental treatments. To minimize the differences across treatments as much as possible, we had subjects in different treatments see exactly the same 40",0.08736437908496732,0.8567585494949496,0.4827161605228761,0.8957640626262625,,
57,41,4,Paragraph,"[(493, 503), (503, 512), (512, 523), (523, 534), (534, 544), (544, 557), (557, 568), (568, 579), (579, 590), (590, 601), (601, 613), (613, 622), (622, 634), (634, 646), (646, 661), (661, 684)]","prediction tasks. The predictions of ML models shown to subjects in different treatments, including both the binary predicted labels and the confidence scores, were produced by real ML models that we developed prior to the experiment deployment. We trained 4 ML models for this experiment, including a neural network, a random forest, a naive Bayes classifier, and a support vector machine (SVM) 2 . For the neural network, random forest, and naive Bayes models, we directly used the conditional probability of the predicted label as the confidence score. For the SVM model, we adopted Platt scaling [46] to compute the probability of the predicted label given the binary prediction and used that as the confidence score. On the 40 tasks in our experiment, the Pearson correlation between the model’s confidence on a task and the model’s accuracy on that task was found to be positive for all 4 models, though with different levels of significance (neural network: r =0 . 643, p < 0 . 001; random forest: r =0 . 281, p =0 . 079; naive Bayes: r =0 . 175, p =0 . 281; SVM: r =0 . 254,",0.5189493464052287,0.2883532464646465,0.9143499278431373,0.5072526565656565,,
58,41,4,Paragraph,"[(684, 697), (697, 707), (707, 716), (716, 727), (727, 737), (737, 745), (745, 754), (754, 765), (765, 778), (778, 790)]","p =0 . 114).Notethat in our experiment, for any two treatments with the same level of model confidence and observed accuracy, but different stated accuracies, model predictions shown to the subject were taken from the same pre-trained ML model. For example, as shown in Figure 3, T1 (i.e., “stated-60%, observed-55%, low confidence”) and T2 (i.e., “stated-90%, observed-55%, low confidence”) shared exactly the same model predictions (including both binary predicted labels and confidence scores) on all 40 tasks, which were generated from a single SVM model 3 . As such, the only difference between these two treatments is the level of stated accuracy for the ML model.",0.5188022875816993,0.5094515717171717,0.9124741808104576,0.6594370848484848,,
59,42,4,Section,"(790, 794)",3.4 Other Experimental Control,0.5195343137254902,0.6748092198232323,0.7890354295751635,0.6885833359848486,,
60,43,4,Paragraph,"[(794, 803), (803, 813), (813, 824), (824, 835), (835, 837)]","Our experiment was implemented as a Human Intelligence Task (HIT) on Amazon Mechanical Turk (MTurk), and we limited the subjects of our experiments to be U.S. workers on MTurk only. Upon arrival, each subject was randomly assigned to one of the eight treatments.",0.51909477124183,0.6940148626262627,0.9143547937254903,0.760683296969697,,
61,43,4,Paragraph,"[(837, 848), (848, 860), (860, 870), (870, 885)]","The 20 prediction tasks in Phase 1 were carefully selected such that ML models for the 4 treatments with a 55% model’s observed accuracy (i.e., T1–T4) always had the same binary predicted labels on each of these 20 tasks, and 11 out of these 20 labels were correct",0.5195343137254902,0.7631992060606061,0.9120987227607847,0.8160317818181818,,
62,44,4,Footnote,"[(885, 910), (910, 930), (930, 963)]","2 These models were chosen because they naturally tend to produce different extremes of observed accuracies and confidence scores. For instance, neural networks can be trained to have high observed accuracy, but they are often overconfident in their incorrect predictions, leading to inflated confidence scores. 3 We note that in reality, different levels of stated accuracy can be claimed for a single ML model when the set of held-out data on which the model is evaluated is different.",0.5195343137254902,0.8329674539141414,0.9123103527254901,0.8951665345959596,,
63,45,5,Header,"[(0, 10), (10, 15)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Amy Rechkemmer and Ming Yin",0.08790522875816993,0.0783156446969697,0.9120931493464052,0.0871209477272727,,
64,46,5,Figure,"[(15, 19), (19, 23), (23, 27), (27, 31), (31, 35), (35, 39), (39, 43), (43, 47), (47, 48), (48, 49), (49, 50), (50, 51)]","T1: stated-60%, observed-55%, low T2: stated-90%, observed-55%, low T3: stated-60%, observed-55%, high T4: stated-90%, observed-55%, high T5: stated-60%, observed-95%, low T6: stated-90%, observed-95%, low T7: stated-60%, observed-95%, high T8: stated-90%, observed-95%, high … … … …",0.17835373069215685,0.11627067940223484,0.6640278860917647,0.2995467583838385,,
65,47,5,Equation,"[(51, 52)]",…,0.4686686589215686,0.16501371576587123,0.4833212042780392,0.17633613717768942,,
66,48,5,Figure,"[(52, 53)]",…,0.6502018360784314,0.16501371576587123,0.6648543814349019,0.17633613717768942,,
67,49,5,Equation,"[(53, 54)]",…,0.468668613132353,0.1882812975840531,0.48332115848882357,0.1996037189958713,,
68,50,5,Figure,"[(54, 55)]",…,0.6500731683823529,0.1881963794022348,0.6647257137388235,0.199518800814053,,
69,51,5,Equation,"[(55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61)]",… … … … … …,0.468840826372549,0.22021053394768939,0.664006823052549,0.27806811899587114,,
70,52,5,Figure,"[(61, 62), (62, 63), (63, 67), (67, 71), (71, 72), (72, 74), (74, 76), (76, 78)]",… … Phase 1 (20 tasks) Phase 2 (20 tasks) SVM Naïve Bayes Random forest Neural network,0.4055999669117647,0.1306444947474748,0.8255191272058822,0.3332592765656567,,
71,53,5,Caption,"[(78, 96), (96, 117), (117, 138), (138, 161), (161, 179), (179, 202), (202, 207)]","Figure 3: Design of experimental treatments. Model predictions presented to subjects in treatments with the same level of model confidence and observed accuracy (e.g., T1 and T2, or any two treatments shown in the same color) were taken from the same pre-trained ML model. In Phase 1, treatments with the same level of observed accuracy (i.e., T1–T4 as highlighted in the yellow dashed box, or T5–T8 as highlighted in the purple dashed box) had the same binary predicted labels on each of the 20 tasks (but model confidence differed between low confidence treatments and high confidence treatments). In Phase 2, all treatments (i.e., T1–T8 as highlighted in the blue dotted box) had the same binary predicted labels on each of the 20 tasks (but again, model confidence may differ).",0.08790522875816993,0.35101621010101003,0.9120961307189551,0.4453588868686869,Figure,5.0
72,54,5,Paragraph,"[(207, 219), (219, 230), (230, 242), (242, 256), (256, 269), (269, 279), (279, 290), (290, 300), (300, 312), (312, 321), (321, 333), (333, 344), (344, 351)]","(see the yellow dashed box in Figure 3). Similarly, ML models for the other 4 treatments with a 95% model’s observed accuracy (i.e., T5–T8) also had the same binary predicted labels on each of these 20 tasks, while 19 out of these 20 labels were correct (see the purple dashed box in Figure 3). So, within each set of 4 treatments that shared the same level of observed accuracy, the only differences among these treatments in Phase 1 were the model’s stated accuracy and its confidence score for each prediction. Importantly, since there was no overlap in the range of confidence scores between the low confidence treatments and the high confidence treatments, on any of the 20 prediction tasks in Phase 1, the confidence score produced by models of low confidence treatments was lower than the score produced by models of high confidence treatments.",0.08736437908496732,0.4793456707070707,0.48207080993464074,0.6567098121212122,,
73,54,5,Paragraph,"[(351, 362), (362, 374), (374, 386), (386, 398), (398, 408), (408, 420), (420, 431), (431, 441), (441, 455), (455, 464), (464, 468)]","Moreover, the 20 prediction tasks in Phase 2 were also carefully selected such that ML models for all 8 treatments made exactly the same binary predictions on each of them (see the blue dashed box in Figure 3). 16 out of these 20 predictions were correct, although the subject received no feedback on the model’s accuracy during Phase 2. So, the only differences across all eight treatments in Phase 2 were the model’s stated accuracy, observed accuracy in Phase 1, and confidence scores that the model associated with its predictions. Again, on any of the 20 tasks in Phase 2, models of high confidence treatments were more confident about their prediction than models of low confidence treatments.",0.0873921568627451,0.6592257212121211,0.48272043440522877,0.8089168828282828,,
74,54,5,Paragraph,"[(468, 478), (478, 486), (486, 497), (497, 507), (507, 514), (514, 525)]","The order of prediction tasks was randomized within each phase. To incentivize high-quality predictions from subjects, in addition to the $1.5 base payment that each subject was guaranteed to receive once they submitted the experiment HIT, we also pro- vided performance-contingent bonuses—we told the subject that we would randomly select one task from the 40 prediction tasks,",0.08736437908496732,0.8114327919191919,0.48293492475817,0.8919396101010101,,
75,54,5,Paragraph,"[(525, 540), (540, 552), (552, 563), (563, 573), (573, 584), (584, 592), (592, 604), (604, 607)]","and we would pay a $1 bonus to her if her final prediction on that task was correct. As the median amount of time subjects spent on our HIT was about 17 minutes, the bonus payment we provided was roughly equivalent to an additional hourly wage of $3.5/hour; this is 75% higher than workers’ median hourly wage on MTurk (about $2/hour, [26]) and likely provided considerable motivation for workers to carefully decide whether to trust the ML model in their decision making.",0.5189918300653594,0.4793456707070707,0.9131414416732027,0.5875254686868687,,
76,55,5,Section,"(607, 609)",4 DATA,0.5195343137254902,0.6025516440656565,0.5950958740196078,0.6163257602272727,,
77,56,5,Paragraph,"[(609, 618), (618, 631), (631, 642), (642, 654), (654, 665), (665, 678), (678, 688), (688, 699), (699, 711), (711, 721), (721, 734), (734, 747), (747, 759), (759, 769)]","After removing workers who had accidentally completed our HIT more than once, we were left with a total of 1,224 unique subjects who participated in our experiment 4 . Among these subjects, 42.3% of them were female, and their average age was 35. When each subject worked on a prediction task, we recorded her initial predic- tion on whether the participant in that task would want to see the date again before seeing the model’s prediction, her final prediction after seeing the model’s prediction, and her reported belief on how likely the model’s prediction would be correct. At the end of the experiment, we also recorded the subject’s responses to the survey questions, which asked her to report the level of trust she had on our model in each phase. Based on these data, we used four different measures to quantify the subject’s trust in the ML model, and we computed the values of these measures separately for each phase:",0.5189918300653594,0.6217572868686869,0.9145655480784315,0.8129585494949495,,
78,57,5,List,"[(769, 781), (781, 793), (793, 802), (802, 805)]",• Subject’s belief in model accuracy (belief) : the average value of the number that the subject gave in Step 4 of each prediction task indicating how much they believed the model’s prediction would be correct.,0.5233169934640522,0.8179908727272727,0.912098514509804,0.8708234484848484,,
79,58,5,Footnote,"[(805, 822)]","4 The number of subjects in T1–T8 were 163, 156, 138, 150, 145, 159, 154, and 159.",0.519328431372549,0.8844750296717171,0.8942246232026142,0.8951665345959596,,
80,59,6,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636911764705882,0.0783156446969697,0.9120929186274509,0.0871209477272727,,
81,60,6,List,"[(10, 21), (21, 29), (29, 36), (36, 49), (49, 59), (59, 70), (70, 81), (81, 91), (91, 103), (103, 115), (115, 126), (126, 137), (137, 148), (148, 161), (161, 173), (173, 184), (184, 194), (194, 204), (204, 214), (214, 223), (223, 235), (235, 245), (245, 249)]","• Agreement fraction (agreement) : the number of tasks that the subject’s final prediction agreed with the model’s prediction, divided by the total number of tasks. • Switch fraction (switch) : the number of tasks for which the subject initially made an opposite prediction as compared to the model, but after seeing the model’s prediction she decided to switch her final prediction to agree with the model, divided by the number of tasks that the subject initially disagreed with the model. • Self-reported trust level (self-report) : the level of trust that the subject reported to have in the ML model in the exit survey. We note that measures we adopted here were among the most common ones that have been used in previous literature to quantify trust in ML models. For example, subject’s belief in model accuracy was used in [31, 50], agreement or switch fraction was used in [35, 58, 61], and subject’s self-reported level of trust in the model was used in [10, 58]. However, subtle differences also exist between these trust measures. While subject’s belief in model accuracy and self- reported trust level tend to characterize how much subjects “think” they trust the ML model, the other two metrics—agreement fraction and switch fraction—focus more on measuring trust by examining how subjects behave (e.g., how often did a subject “follow” a model’s prediction?). Intuitively, for all 4 trust measures, larger values imply higher levels of trust.",0.08736437908496732,0.10956031717171716,0.48293545219346423,0.43158481212121214,,
82,61,6,Paragraph,"[(249, 260), (260, 269), (269, 279), (279, 289), (289, 302), (302, 311), (311, 323), (323, 331)]","Our data suggests that human subjects are, in general, not very good at making accurate predictions for speed dating outcomes by themselves. Specifically, the average accuracy of a subject’s ini- tial predictions before seeing the model’s predictions was 53.6% in Phase 1 and 62.8% in Phase 2, and it was not significantly differ- ent across treatments (in contrast, depending on the experimental treatment the subject was in, the model’s accuracy was 55% or 95% in Phase 1, and 80% in Phase 2).",0.08790522875816993,0.4341007212121212,0.48293857285228764,0.5422805191919192,,
83,62,6,Section,"(331, 333)",5 RESULTS,0.08790522875816993,0.5614417955808081,0.19143472679738563,0.5752159117424243,,
84,63,6,Paragraph,"[(333, 345), (345, 355), (355, 365)]","In this section, we report our findings on how laypeople are affected by multiple performance indicators of an ML model when this model is provided to assist them in low-stakes decision making.",0.08790522875816993,0.5806474383838384,0.4804691647581699,0.619642892929293,,
85,64,6,Section,"(365, 371)",5.1 Analyzing Trust in Phase 1,0.08790522875816993,0.6388029066919192,0.3461942140522876,0.6525770228535354,,
86,65,6,Paragraph,"[(371, 382), (382, 394), (394, 405), (405, 415), (415, 427), (427, 437), (437, 444)]","We start by analyzing the experimental data that we obtained in Phase 1 to understand overall how laypeople’s trust in an ML model is affected by both the model’s stated accuracy and its confidence before they observe the model’s performance in practice (i.e., view the model’s actual accuracy in Phase 1). Before people have had the chance to observe the ML model’s performance in practice, research questions we are interested in examining include:",0.08720261437908497,0.6580085494949495,0.4810031469281048,0.7523512262626263,,
87,66,6,List,"[(444, 458), (458, 468), (468, 482), (482, 491), (491, 495), (495, 508), (508, 516)]","• RQ1 : Does a higher level of confidence that the ML model asso- ciates with a prediction make people trust the prediction more? • RQ2 : Is people’s trust in a model prediction still affected by the model’s stated accuracy, even though the model’s confidence on that prediction is provided? • RQ3 : How does the effect of model confidence on trust vary between models with different levels of stated accuracy?",0.09168954248366012,0.7554732808080807,0.48294495524183007,0.8536138525252525,,
88,67,6,Paragraph,"[(516, 526), (526, 536), (536, 545)]","To answer these questions, we used the model’s stated accuracy and confidence level as our independent variables, and the four trust measures as described above (i.e., belief, agreement, switch,",0.08790522875816993,0.8567585494949496,0.4820699015738565,0.8957527414141414,,
89,67,6,Paragraph,"[(545, 555), (555, 569), (569, 580), (580, 592), (592, 606), (606, 619), (619, 631), (631, 638)]","and self-report) were used as the dependent variables. Recall that in Phase 1, models in T1–T4 (i.e., the 4 treatments with a 55% model accuracy in Phase 1) made different predictions on the tasks than models in T5–T8 (i.e., the 4 treatments with a 95% model accuracy in Phase 1). As a result, to separate only the effects of model’s stated accuracy and confidence on trust for RQ1 – RQ3 , we analyzed the Phase 1 data by comparing trust within each set of 4 treatments with the same Phase 1 model predictions.",0.5189918300653594,0.10956031717171716,0.9124769043137254,0.21774011515151515,,
90,67,6,Paragraph,"[(638, 649), (649, 659), (659, 668), (668, 679), (679, 692), (692, 702), (702, 712), (712, 725), (725, 737), (737, 749), (749, 759), (759, 768), (768, 779), (779, 785)]","To emphasize the exploratory nature of this study, in addition to controlling for false discovery and avoiding the issues with multiple comparisons, we conducted our analysis using the interval estimate method [14, 20]. To analyze the effects of our independent variables on the four trust measures, we plotted effect sizes for a given trust measure between subjects assigned to different levels of a given independent variable (e.g., levels 60% and 90% for stated accuracy). Effect sizes were measured using Cohen’s d 5 , and we also plotted their 95% bootstrap confidence intervals ( R = 5000). To indicate the size of an effect, we also followed Cumming (2013) in considering a confidence interval’s range in relation to zero [13]. We conducted this analysis separately for both confidence and stated accuracy (answering RQ1 and RQ2 , respectively), and this was repeated for both T1–T4 and T5–T8 6 .",0.51909477124183,0.2202560242424243,0.9143442450196079,0.41145854949494953,,
91,67,6,Paragraph,"[(785, 793), (793, 804), (804, 814), (814, 823), (823, 835), (835, 847), (847, 856), (856, 866), (866, 881), (881, 899), (899, 911), (911, 926), (926, 940), (940, 955), (955, 962)]","Meaningful interaction effects were identified by conducting a two-way ANOVA (stated accuracy × confidence) for each of the 4 dependent variables within T1–T4 (and within T5–T8), and we fol- lowed the same interval estimate method approach for interactions worth noting (answering RQ3 ) 7 . Unlike in previous analysis using the interval analysis method to illustrate the main effect of a single independent variable, we calculate it for interaction effects using a difference in difference : Consider the interaction effect between two independent variables A (with two levels A 1 and A 2 ) and B (with two levels B 1 and B 2 ) on a measure of trust Y . The difference in difference is then defined as the difference in Y between two treatments that both belong to the higher level of A (i.e., A 2 ) and differ on the level of B , minus the difference in Y between two treatments that both belong to the lower level of A (i.e., A 1 ) and differ on the level of B .",0.5189918300653594,0.4139744585858586,0.9145632967320261,0.6190241636363636,,
92,67,6,Paragraph,"[(962, 975), (975, 987), (987, 998), (998, 1008), (1008, 1022), (1022, 1040), (1040, 1049), (1049, 1059), (1059, 1070), (1070, 1081), (1081, 1091), (1091, 1102), (1102, 1116), (1116, 1127)]","5.1.1 RQ1: The Main Effect of Confidence in Phase 1. We first an- alyzed the data obtained in the 4 treatments with a 55% model’s observed accuracy (i.e., T1–T4). As shown in Figure 4a, when the ML model associates a higher confidence score to its prediction, people believe that the prediction ( belief ) is more likely to be cor- rect (Cohen’s d =0 . 72 [0 . 55, 0 . 89]). On average, subjects in the high confidence treatments considered the model predictions to be 7.2% more accurate compared to the same model predictions that are shown in the low confidence treatments (i.e., ∆ M =0.072). Beyond that, it also appears that higher model confidence on a prediction may nudge people into following the prediction more often, both in terms of how often people agree with the model’s predictions ( agreement ) and in how often they will switch to agree with the model’s predictions ( switch ). However, we find that the evidence",0.51909477124183,0.6307585494949495,0.9145763572287582,0.8219723959595959,,
93,68,6,Footnote,"[(1127, 1146), (1146, 1173), (1173, 1183)]","5 In computing Cohen’s d , we always treat the level with the lowest value(s) as the baseline group. 6 For figures of raw data distributions and a summary of all estimated effect sizes as well as their 95% bootstrap confidence intervals, see the supplementary materials. 7 For the ANOVA test results, see the supplementary materials.",0.5191127450980392,0.842472504419192,0.9120993457467321,0.8951665345959596,,
94,69,7,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.08790522875816993,0.0783156446969697,0.3363069709150327,0.0871209477272727,,
95,70,7,Figure,"[(10, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 50), (50, 53)]",Amy Rechkemmer and Ming Yin 0.2 0.0 0.2 0.4 0.6 0.8 Cohen's d self-report switch agreement belief 0.00 0.15 0.12 0.72 (a) T1–T4 (observed-55%) 0.2 0.0 0.2 0.4 0.6 0.8 Cohen's d self-report switch agreement belief 0.01 0.06 0.16 0.47 (b) T5–T8 (observed-95%),0.09519158169934641,0.0783156446969697,0.9120931493464052,0.2899129963383838,,
96,71,7,Caption,"[(53, 73), (73, 93), (93, 116), (116, 135)]","Figure 4: The difference in trust between subjects with a high confidence treatment and a low confidence treatment in Phase 1. Cohen’s d values are plotted and listed above each point, and error bars represent 95% bootstrap confidence intervals. An interval to the right (or left) of 0 represents a higher (or lower) mean for the subjects in a high confidence treatment. Measures of trust that see a difference between treatments are also shown with a red and bolded Cohen’s d value.",0.08752450980392158,0.31749979595959593,0.9120961307189546,0.3703663353535353,Figure,6.0
97,72,7,Paragraph,"[(135, 147), (147, 166), (166, 187), (187, 201), (201, 211)]","of this impact on trust is not as compelling for agreement frac- tion (Cohen’s d =0 . 12 [ − 0 . 04, 0 . 28]) or for switch fraction (Cohen’s d =0 . 15 [ − 0 . 01, 0 . 31]) as it was for belief in the model’s predictions. In terms of self-reported trust in the model ( self-report ) in Phase 1, we found no evidence that model confidence has an impact.",0.08736437908496732,0.4043178929292929,0.4829349247581702,0.4709875898989899,,
98,72,7,Paragraph,"[(211, 225), (225, 235), (235, 244), (244, 262), (262, 276), (276, 286), (286, 305), (305, 317), (317, 328), (328, 335)]","We next looked at the other 4 treatments in Phase 1 with a 95% model’s observed accuracy (i.e., T5–T8). Similar to T1–T4, we found that subjects given high confidence predictions believe that the prediction is more likely to be correct (Cohen’s d =0 . 47 [0 . 31, 0 . 64]), as seen in Figure 4b. We also continue to see that a higher model confidence may influence people to agree with the model more frequently (Cohen’s d =0 . 16 [ − 0 . 01, 0 . 32]), but we find no impact of model confidence on how often people will switch to a model’s predictions. Again, model confidence is not found to have an effect on people’s self-reported trust in Phase 1.",0.08790522875816993,0.47350349898989896,0.48207663550326796,0.6093575393939394,,
99,72,7,Paragraph,"[(335, 346), (346, 356), (356, 368), (368, 380), (380, 390), (390, 400)]","Putting it all together, to answer RQ1, our results suggest that before observing an ML model’s accuracy in practice, people believe a model with high confidence scores to be more accurate, but do not self-report to trust it more. In terms of following the model’s predictions, we find some evidence that a higher model confidence can have greater influence, but the evidence is not reliable.",0.08790522875816993,0.6118734484848485,0.4804725930875817,0.692379004040404,,
100,72,7,Paragraph,"[(400, 413), (413, 425), (425, 436), (436, 445)]","5.1.2 RQ2: The Main Effect of Stated Accuracy in Phase 1. We now look into whether the stated accuracy of an ML model can still influence people’s trust in the model in the presence of model confidence, before the model’s accuracy is observed in practice.",0.08790522875816993,0.7183886,0.4810014212810458,0.7712199131313131,,
101,72,7,Paragraph,"[(445, 457), (457, 468), (468, 479), (479, 492), (492, 509), (509, 527), (527, 546), (546, 562), (562, 574)]","A visual inspection of Figures 5a and 5b indicates a positive an- swer. For example, when focusing on T1–T4, we found that overall, claiming the model’s stated accuracy to be 90% rather than 60% led to an increase of 2.7% in the subject’s belief in model accuracy (Cohen’s d =0 . 26 [0 . 09, 0 . 42]), an increase of 2.9% in agreement fraction (Cohen’s d =0 . 17 [0 . 02, 0 . 34]), an increase of 5.5% in switch fraction (Cohen’s d =0 . 17 [0 . 01, 0 . 34]), and an increase of 7.7% in self- reported trust (Cohen’s d =0 . 37 [0 . 20, 0 . 53]). These results highlight the effect of stated accuracy alone on people’s trust given that the",0.0874656862745098,0.7737358222222221,0.48294486232679734,0.8957640626262625,,
102,72,7,Paragraph,"[(574, 583), (583, 594), (594, 603), (603, 614), (614, 624), (624, 634), (634, 643)]","model’s prediction between the two treatments (including both bi- nary predicted labels and confidence scores) on each task were kept unchanged. This indicates that despite the fact that fine-grained information of model confidence is provided at the level of individ- ual predictions, an ML model’s stated accuracy still casts consistent and significant impact on people’s trust in the model—the higher the stated accuracy, the more people trust the model.",0.5195343137254902,0.4043178929292929,0.9145767853803923,0.49866183232323236,,
103,72,7,Paragraph,"[(643, 657), (657, 670), (670, 682), (682, 692), (692, 703), (703, 714), (714, 724)]","To put the effect sizes (as measured by Cohen’s d ) of stated accu- racy on trust in Phase 1 into context, we can compare them against the effect sizes of model confidence on trust. We find that while the model confidence has a larger effect in influencing subjects’ belief in model accuracy, the model’s stated accuracy tends to have a larger and more reliable impact on subjects’ willingness to follow the model’s prediction as well as their self-reported trust levels.",0.5195343137254902,0.5011777414141414,0.9145727626666666,0.5955204181818182,,
104,72,7,Paragraph,"[(724, 733), (733, 744), (744, 753), (753, 764), (764, 776), (776, 788), (788, 799), (799, 811), (811, 832), (832, 850), (850, 861), (861, 873), (873, 883), (883, 893), (893, 903), (903, 914), (914, 925), (925, 928)]","5.1.3 RQ3: The Interaction between Confidence and Stated Accuracy in Phase 1. Lastly, we examine the interaction effect between model confidence and the model’s stated accuracy on influencing people’s trust in the model in Phase 1. The only meaningful interaction effect we found is with respect to subject’s belief in model accuracy. In particular, when an ML model has a high stated accuracy, the increase in subject’s belief in model accuracy brought up by the increase in model confidence is larger than that for an ML model with a low stated accuracy ( T1–T4 : Cohen’s d =0 . 32 [ − 0 . 01, 0 . 64], T5–T8 : Cohen’s d =0 . 38 [0 . 07, 0 . 70]). In other words, the magnitude of the effect of model confidence on how much people believe the predictions to be correct varies depending on the levels of the model’s stated accuracy. Alternatively, we can also interpret this as that model confidence moderates the effect of the model’s stated accuracy on subject’s belief in model accuracy—when the model is more confident, an increase in the model’s stated accuracy will lead to a more significant increase in subject’s belief of how accurate the model is.",0.5189918300653594,0.6492482787878788,0.914349519372549,0.8957527414141414,,
105,73,8,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636911764705882,0.0783156446969697,0.9120929186274509,0.0871209477272727,,
106,74,8,Figure,"[(10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 18), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 37), (37, 38), (38, 39), (39, 40), (40, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 48)]",0.2 0.0 0.2 0.4 0.6 0.8 Cohen's d self-report switch agreement belief 0.37 0.17 0.17 0.26 (a) T1–T4 (observed-55%) 0.2 0.0 0.2 0.4 0.6 0.8 Cohen's d self-report switch agreement belief 0.21 0.30 0.15 0.29 (b) T5–T8 (observed-95%),0.09519158169934641,0.11782739226705367,0.8674051090890522,0.2899129963383838,,
107,75,8,Caption,"[(48, 69), (69, 89), (89, 113), (113, 133)]","Figure 5: The difference in trust between subjects with a high stated accuracy treatment and a low stated accuracy treatment in Phase 1. Cohen’s d values are plotted and listed above each point, and error bars represent 95% bootstrap confidence intervals. An interval to the right (or left) of 0 represents a higher (or lower) mean for the subjects in a high stated accuracy treatment. Measures of trust that see a difference between treatments are also shown with a red and bolded Cohen’s d value.",0.08736274509803924,0.31749979595959593,0.9146011084967322,0.3703663353535353,Figure,4.0
108,76,8,Section,"(133, 139)",5.2 Analyzing Trust in Phase 2,0.08790522875816993,0.40240896729797976,0.3461942140522876,0.4161830834595959,,
109,77,8,Paragraph,"[(139, 153), (153, 165), (165, 175), (175, 186), (186, 197), (197, 210), (210, 221), (221, 233), (233, 243), (243, 247)]","We now turn our attention to the data that we obtained in Phase 2 to examine overall how laypeople’s trust in an ML model is affected by the model’s stated accuracy, observed accuracy, and model con- fidence after they learn about the model’s overall accuracy in Phase 1. Since subjects in all treatments worked on the same prediction tasks and saw the same binary prediction on each of the tasks in Phase 2, we can directly compare the trust measurements across all 8 treatments. In this set of analyses, we are interested in exploring the following research questions after people have observed an ML model’s performance in practice:",0.08720261437908497,0.4216146101010101,0.48294118072679754,0.5574686505050505,,
110,78,8,List,"[(247, 258), (258, 262), (262, 275), (275, 285), (285, 292), (292, 303), (303, 313), (313, 315)]","• RQ4 : Does a model’s confidence score still influence people’s trust in the model? • RQ5 : Do the stated accuracy and observed accuracy of the model still affect people’s trust in the model, despite model confidence still being provided for each individual prediction? • RQ6 : How do model confidence, the model’s stated accuracy, and the model’s observed accuracy interact with each other to influence trust?",0.09168954248366014,0.560590705050505,0.48208399037908506,0.6725671353535354,,
111,79,8,Paragraph,"[(315, 326), (326, 337), (337, 349), (349, 358), (358, 366), (366, 377), (377, 390), (390, 399), (399, 408), (408, 418), (418, 429), (429, 439), (439, 444)]","To answer these questions, for each of the four trust measure- ments, we repeated our Phase 1 analysis method. This time, how- ever, we included all 8 treatments in the analysis, and we included observed accuracy along with confidence and stated accuracy as independent variables to consider. Employing the interval estimate method, we answer RQ4 by examining the effect of confidence in Phase 2, and we answer RQ5 by looking into the effects of stated accuracy and observed accuracy in Phase 2. Meaningful interac- tion effects were identified by conducting a three-way ANOVA (stated accuracy × observed accuracy × confidence) for each of the four trust treatments, and again we followed the interval es- timate method by calculating a difference in difference for these interactions to answer RQ6 .",0.0874656862745098,0.6757118323232323,0.48294032052287617,0.8530772363636364,,
112,79,8,Paragraph,"[(444, 456), (456, 468)]",5.2.1 RQ4: The Main Effect of Confidence in Phase 2. Figure 6a shows the effect of model confidence on each of the four trust,0.08790522875816993,0.8705956707070707,0.48047060883660125,0.8957527414141414,,
113,79,8,Paragraph,"[(468, 479), (479, 489), (489, 501), (501, 519), (519, 529), (529, 541), (541, 552), (552, 560), (560, 574), (574, 585), (585, 597), (597, 610), (610, 618)]","measures for Phase 2. Consistent with our Phase 1 results, we found that subjects still tended to believe that models producing higher levels of confidence scores are more likely to be correct in Phase 2 (Cohen’s d =0 . 56 [0 . 44, 0 . 67]). This suggests that even after observing the model’s accuracy in practice, subjects still believe a prediction with a high confidence score as more likely to be correct. However, it does not appear that the model confidence has any impact on agreement fraction, switch fraction, or self-reported level of trust. That is to say, in Phase 2, subjects did not seem to follow a model prediction more often when a high confidence was associated with it as compared to when a low confidence was associated with it, nor did they feel that they trust a model more when the confidence scores it produced were higher.",0.5189918300653594,0.4043178929292929,0.9143495193725493,0.581683296969697,,
114,79,8,Paragraph,"[(618, 629), (629, 631), (631, 639), (639, 650), (650, 662), (662, 674), (674, 682), (682, 694), (694, 713), (713, 730), (730, 749), (749, 767), (767, 786), (786, 803), (803, 813), (813, 832), (832, 851), (851, 856)]","5.2.2 RQ5: The Main Effect of Stated and Observed Accuracy in Phase2. Figures6band6cshowthateventhoughmodelconfidence— the model’s own accuracy estimate for individual predictions—is provided on each prediction, both the stated accuracy and the ob- served accuracy of an ML model still have a significant impact on people’s trust in the model in Phase 2. After observing the model’s performance in practice, subjects not only believed predictions made by a model with higher stated or observed accuracy to be more accurate ( stated : Cohen’s d =0 . 27 [0 . 16, 0 . 38], observed : Co- hen’s d =0 . 45 [0 . 33, 0 . 57]), but also agreed with such predictions more often ( stated : Cohen’s d =0 . 15 [0 . 04, 0 . 26], observed : Cohen’s d =0 . 35 [0 . 23, 0 . 46]), and switched their answer to match such pre- dictions more often ( stated : Cohen’s d =0 . 19 [0 . 08, 0 . 31], observed : Cohen’s d =0 . 75 [0 . 63, 0 . 88]). A higher observed accuracy also led subjects to self-report that they trusted the model more (Cohen’s d =0 . 77 [0 . 64, 0 . 90]), and it also appears that a higher stated accuracy may have a similar effect (Cohen’s d =0 . 10 [ − 0 . 01, 0 . 21]), though this result is less compelling.",0.5195343137254902,0.593899793939394,0.9176551880784316,0.8404055191919191,,
115,79,8,Paragraph,"[(856, 866), (866, 878), (878, 888), (888, 900)]","When comparing the effect sizes of stated accuracy and observed accuracy on trust in Phase 2 against those of the model confidence, we found the model’s stated accuracy and observed accuracy have a larger impact on all trust measures except for subject’s belief in",0.5189918300653594,0.8429214282828282,0.9137031913725491,0.8957527414141414,,
116,80,9,Header,"[(0, 10), (10, 15)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Amy Rechkemmer and Ming Yin",0.08790522875816993,0.0783156446969697,0.9120931493464052,0.0871209477272727,,
117,81,9,Figure,"[(15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (20, 23), (23, 24), (24, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 41), (41, 42), (42, 43), (43, 44), (44, 45), (45, 46), (46, 47), (47, 48), (48, 49), (49, 52), (52, 53), (53, 54), (54, 55), (55, 56), (56, 57), (57, 60), (60, 61), (61, 62), (62, 63), (63, 64), (64, 65), (65, 66), (66, 67), (67, 68), (68, 71)]",0.2 0.0 0.2 0.4 0.6 0.8 Cohen's d self-report switch agreement belief 0.02 0.04 0.06 0.56 (a) Confidence 0.2 0.0 0.2 0.4 0.6 0.8 Cohen's d self-report switch agreement belief 0.10 0.19 0.15 0.27 (b) Stated Accuracy 0.2 0.0 0.2 0.4 0.6 0.8 Cohen's d self-report switch agreement belief 0.77 0.75 0.35 0.45 (c) Observed Accuracy,0.093248522875817,0.11458393000083822,0.8793176798815358,0.24575895593434333,,
118,82,9,Caption,"[(71, 90), (90, 110), (110, 134), (134, 148)]","Figure 6: The difference in trust between subjects with different levels of the given independent variable. Cohen’s d values are plotted and listed above each point, and error bars represent 95% bootstrap confidence intervals. An interval to the right (or left) of 0 represents a higher (or lower) mean for the subjects in the higher valued treatment. Measures of trust that see a difference between treatments are also shown with a red and bolded Cohen’s d value.",0.08745098039215687,0.27334701818181806,0.9120961307189547,0.3262122949494947,Figure,6.0
119,83,9,Paragraph,"[(148, 157), (157, 167), (167, 177), (177, 189), (189, 199), (199, 209), (209, 221), (221, 226)]","model accuracy. In particular, a model’s observed accuracy domi- nates the model’s stated accuracy and confidence on influencing the subject’s willingness to follow the model predictions or their self- reported level of trust in the model in Phase 2. These observations imply that after an ML model’s performance has been observed in practice, its stated accuracy and observed accuracy still signifi- cantly influence people’s trust in the model, and the impact of the observed accuracy is especially substantial.",0.08790522875816993,0.3601651151515151,0.482943627440523,0.4683449131313131,,
120,83,9,Paragraph,"[(226, 234), (234, 244), (244, 253), (253, 266), (266, 274), (274, 286), (286, 297), (297, 309), (309, 319)]","5.2.3 RQ6: The Interaction between Confidence, Stated Accuracy, and Observed Accuracy in Phase 2. Finally, we investigate whether different factors interact with each other in influencing people’s trust in the model in Phase 2. We first note that the three-way interaction between the model’s confidence, stated accuracy, and observed accuracy is not meaningful for any of the four trust mea- sures, implying that the strength of the interaction between any of the two factors (e.g., stated and observed accuracy) on trust is not dependent on the level of the third factor (e.g., confidence).",0.08790522875816993,0.4970412080808081,0.482944104972549,0.6190128424242424,,
121,83,9,Paragraph,"[(319, 329), (329, 342), (342, 351), (351, 362), (362, 371), (371, 383), (383, 394), (394, 410), (410, 420), (420, 430), (430, 447), (447, 463), (463, 474), (474, 485), (485, 497), (497, 511), (511, 515)]","We then move on to examine the two-way interactions between any pair of influencing factors. Similar to that in Phase 1, we again detected an interaction between model confidence and stated accu- racy on subject’s belief in model accuracy, suggesting that after a model’s performance is observed in practice, increasing the model’s confidence on a prediction still leads to a larger increase in sub- ject’s belief in the correctness of the prediction when the model’s stated accuracy is higher (Cohen’s d =0 . 23 [0 . 003, 0 . 46]). Additional meaningful interactions that we found are between a model’s stated accuracy and observed accuracy in influencing people’s trust in the model with respect to switch fraction (Cohen’s d =0 . 41 [0 . 18, 0 . 64]) and self-reported trust (Cohen’s d =0 . 43 [0 . 19, 0 . 65]), which are similar to results previously reported in [58]. That is, after subjects have had the opportunity to observe a model’s accuracy in practice, higher levels of stated accuracy only push people to follow a model more often and report a higher level of trust in the model if its observed accuracy is high.",0.08790522875816993,0.6215287515151515,0.4829353203346405,0.8542426404040404,,
122,83,9,Paragraph,"[(515, 526), (526, 536), (536, 544)]","In sum, to answer RQ6, we indeed found evidence suggesting that after people observe an ML model’s performance in practice, there are some interactions between model confidence, stated accuracy,",0.08790522875816993,0.8567585494949496,0.48207795961307215,0.8957527414141414,,
123,83,9,Paragraph,"[(544, 554), (554, 559)]","and observed accuracy that influence people’s trust in the model, as quantified by different measures.",0.5195343137254902,0.3601651151515151,0.9137131540392156,0.3853234484848485,,
124,84,9,Section,"(559, 561)",6 DISCUSSIONS,0.5195343137254902,0.40119937133838385,0.6604613473856209,0.4149734875,,
125,85,9,Paragraph,"[(561, 572), (572, 585), (585, 595), (595, 605), (605, 615), (615, 625), (625, 635), (635, 646), (646, 656), (656, 669), (669, 680), (680, 690), (690, 701), (701, 706)]","In this paper, we conduct an exploratory study to investigate how laypeople’s trust in an ML model is affected by both the model’s con- fidence and its accuracy. Our results suggest that model confidence and model accuracy play different roles in influencing people’s trust in an ML model. Model confidence mostly influences people’s belief in model accuracy, but the model’s stated accuracy and observed accuracy consistently impact how much people think they trust a model and how frequently they actually follow the model. In this section, we begin by discussing potential explanations for the lim- ited impact that model confidence had in our study, as well as why we see an inconsistency in our results across different measures of trust. Next, we provide implications for future design and caution readers on the generalizability of our results. Finally, we end with possible directions for future work.",0.5189918300653594,0.42040375151515147,0.914566764109804,0.6116062767676768,,
126,86,9,Section,"(706, 714)",6.1 Understanding the Limited Impact of Model Confidence,0.5195343137254902,0.6274821996212121,0.8608358459150327,0.6576085885101012,,
127,87,9,Paragraph,"[(714, 724), (724, 734), (734, 746), (746, 759), (759, 769), (769, 780), (780, 790), (790, 802), (802, 810), (810, 822), (822, 827)]","Compared to our observations that an ML model’s stated accuracy and observed accuracy have significant impact on people’s trust in the model, as consistently shown in all four measures of trust that we adopt, the impact of model confidence on trust seems to be more limited. The only consistent effect of model confidence on trust is with respect to people’s belief of model accuracy; regardless of whether the model’s performance is observed in practice, the more confident a model, the more accurate people believe the model is. In contrast, model confidence doesn’t influence the self-reported level of trust in the ML model, and it doesn’t reliably influence people’s willingness to follow a model.",0.5189918300653594,0.6630401151515152,0.9124699597490197,0.8127312767676769,,
128,87,9,Paragraph,"[(827, 841), (841, 850), (850, 865), (865, 875), (875, 883), (883, 894)]","As to why we see this result, it is possible that when both model accuracy and model confidence are presented, people consider accu- racy as a fact , but deem confidence as an estimate , and thus model confidence is treated as a less trustworthy type of performance information. Such perception may become particularly strong after people have a chance to observe a model’s accuracy on real-world",0.5195343137254902,0.8152471858585858,0.9145648887843139,0.8957527414141414,,
129,88,10,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636911764705882,0.0783156446969697,0.9120929186274509,0.0871209477272727,,
130,89,10,Paragraph,"[(10, 20), (20, 31), (31, 43), (43, 52), (52, 63), (63, 73), (73, 87), (87, 98), (98, 108), (108, 113)]","trials themselves. This is consistent with what we have observed in our data, which suggests that people substantially rely upon the model’s observed accuracy to decide their trust in an ML model in Phase 2, effectively leaving the model confidence almost ignored. This may also explain the difference between our findings and the findings in prior literature indicating that model confidence is di- rectly related to users’ levels of trust in the system [3, 36, 54, 61]: when a confidence score is the only performance indicator of a model shown, the score’s value may have greater influence on people’s trust in its predictions.",0.08736437908496732,0.10956031717171716,0.482943627440523,0.24541435757575758,,
131,90,10,Section,"(113, 123)",6.2 On the (Seemingly) Conflicting Results Across Different Trust Measures,0.08790522875816993,0.2677195733585858,0.44300356388888884,0.29784722487373744,,
132,91,10,Paragraph,"[(123, 135), (135, 145), (145, 154), (154, 165), (165, 175), (175, 186), (186, 197), (197, 205), (205, 210)]","In our experiment, we also find that the effects of model confidence on different trust measures are not always consistent. This raises an important question on how to appropriately operationalize peo- ple’s “trust” in ML models in experimental studies. To this end, we first note that different trust measures can capture different “types” of trust. For example, one can consider subject’s belief in model accuracy and self-reported trust as the “stated trust” or trust perceptions, while agreement and switch fractions represent the “revealed trust,” or trusting behaviors.",0.08625,0.30327748888888884,0.48293532033464065,0.42529567070707075,,
133,91,10,Paragraph,"[(210, 221), (221, 232), (232, 244), (244, 255), (255, 263), (263, 274), (274, 285), (285, 294), (294, 303), (303, 313), (313, 326), (326, 338), (338, 352), (352, 366), (366, 376), (376, 387), (387, 401), (401, 412), (412, 421), (421, 430), (430, 440)]","Our results, for example, show that the effect of model confidence on people’s tendency to follow a model (i.e., revealed trust) can be different than that on people’s belief in model accuracy or self- reported trust (i.e., stated trust). While this is consistent with vari- ous previous literature suggesting that subjective self-reported trust may not be a reliable quantification of trusting behavior [34, 49] (i.e., “saying and doing are two different things”), we conjecture that there may also exist some inherent relationships between different trust measures that could explain the inconsistency. For instance, one possible relationship between the belief in model accuracy and the decision to follow a prediction of an ML model could be that the subject would only be willing to adjust her own prediction to match with that of the model’s if her belief in the accuracy of the prediction is above a threshold . If this was true, then it’s possible to see model confidence has a meaningful impact on subject’s belief in model accuracy but not on agreement or switch fractions, just as what we have seen in Phase 2 of our experiment, and to some extent, Phase 1 as well. Exploring how various trust measures relate to one another, perhaps by understanding the reasoning process behind people’s trust decisions, can be another important direction which may significantly advance our understanding of trust in ML.",0.08736437908496732,0.4278115797979798,0.482945825087582,0.715872690909091,,
134,91,10,Paragraph,"[(440, 450), (450, 461), (461, 470), (470, 481), (481, 490), (490, 499), (499, 511), (511, 522), (522, 532), (532, 542), (542, 554), (554, 566), (566, 579)]","Interestingly, in our experiments, we also found the effects of model confidence on the two stated trust measures (i.e., belief in model accuracy and self-reported trust) can be different, which seems puzzling at first glance. One explanation for this is that subjects experienced the anchoring effect when providing belief in model accuracy. In particular, model confidence was described to subjects as the chance that the model believes for its prediction to be correct, and we solicited subject’s belief in model accuracy by asking how likely she thought the model’s individual prediction was correct. In addition, both confidence and belief in model accuracy were represented as a value between 0 and 1, perhaps making them analogous to one another. This may have made it easy for subjects to use model confidence as a reference point for their belief in the",0.08736437908496732,0.7183886,0.48084569495424845,0.8957527414141414,,
135,91,10,Paragraph,"[(579, 587), (587, 597), (597, 602)]","model’s accuracy, causing them to subconsciously weigh model confidence higher than other factors such as stated and observed accuracy in their final decision.",0.5195343137254902,0.10956031717171716,0.91209824972549,0.14855577171717174,,
136,91,10,Paragraph,"[(602, 612), (612, 624), (624, 637), (637, 646), (646, 657), (657, 665), (665, 678), (678, 684)]","On the other hand, the self-reported trust measure was asked upon completing the prediction tasks, and it was asked in the form of a Likert scale. As such, the belief in model accuracy can af- fect, but does not necessarily determine, one’s self-reported trust. Self-reported trust is a more holistic judgement of ML model’s per- formance, integrity and intention, reflecting the subject’s feeling about the model as a whole, and can also be influenced by one’s emotional state like surprise or confusion.",0.5195343137254902,0.15107168080808073,0.9145770197960784,0.25925147878787885,,
137,91,10,Paragraph,"[(684, 695), (695, 703), (703, 714), (714, 724), (724, 735), (735, 746), (746, 757), (757, 766), (766, 777)]","Finally, we acknowledge that trust is a complex concept and a multidimensional construct. Though we attempted to measure trust using a diverse set of metrics including both behavioral and self- reported methods, further studies are needed to understand how to, for example, design reliable scales to probe into various aspects of trust in ML models. In particular, there has been extensive literature in various fields that discusses how to define, model, and quantify trust in computational environments [7, 12], which can provide useful guidance in the future on how to appropriately define trust",0.5195343137254902,0.2617673878787879,0.9145640097254905,0.3837843070707071,,
138,91,10,Paragraph,"[(777, 787)]",in the context of interactions between humans and ML models.,0.5195343137254902,0.38630021616161614,0.897031474509804,0.3976214282828283,,
139,92,10,Section,"(787, 790)",6.3 Design Implications,0.5195343137254902,0.4224430582070707,0.7221548034313726,0.43621717436868684,,
140,93,10,Paragraph,"[(790, 802), (802, 813), (813, 824), (824, 833), (833, 842), (842, 853), (853, 864), (864, 875), (875, 887), (887, 897), (897, 908), (908, 918), (918, 930), (930, 942), (942, 951), (951, 962), (962, 973), (973, 986), (986, 997), (997, 1007), (1007, 1016), (1016, 1025)]","In our findings we saw that after the model’s performance is ob- served in practice, people’s trust in the model is affected dominantly by model’s observed accuracy, yet it is hardly affected by model’s confidence. This indicates the potential needs of helping laypeople to better understand the uncertainty inherent in performance calcu- lation and calibration estimation based on a small set of predictions. As we have discussed earlier, if people indeed simply ignore model confidence in deciding their trust in a model, especially after ob- serving its accuracy in practice, then it is crucial to help people recognize that doing so can be sub-optimal, since any accuracy measurements computed based on a small number of trials may not reflect the model’s overall performance accurately. Even if the accu- racy measurement is reliable, there is still great value in utilizing a calibrated confidence score to calibrate trust in an ML model. On the other hand, people might have actually adjusted their interpreta- tion of confidence scores after they have seen the model’s accuracy in practice and thus obtained an estimation of how calibrated the model’s confidence is. If this is the case, the key message that needs to be conveyed to people becomes that the degree of confidence calibration estimated based on a limited number of predictions can also be inaccurate, especially when confidence scores for these predictions all lie in a small range (e.g., 0.95–1).",0.5190212418300654,0.441648701010101,0.9145648887843139,0.7435469333333333,,
141,93,10,Paragraph,"[(1025, 1039), (1039, 1052), (1052, 1065), (1065, 1077), (1077, 1086), (1086, 1096), (1096, 1109), (1109, 1121), (1121, 1132), (1132, 1141), (1141, 1150)]","In other words, it is critical for people to see the value of contin- ued use of a calibrated confidence score to adjust their trust in an ML model, even after observing a very high or a very low accuracy of the model in practice (instead of simply always trusting or not trusting the model, regardless of the model confidence). Meanwhile, the estimated degree of calibration for a model’s confidence based on a small set of predictions may not be very accurate. To this end, a tool that assists people in updating their estimation of how calibrated a model’s confidence is as they interact with the model, and perhaps even quantifying and visualizing the uncertainty of such estimation [21, 29], can potentially be very helpful.",0.5195343137254902,0.7460615797979798,0.9145767309803924,0.8957527414141414,,
142,94,11,Header,"[(0, 10), (10, 15)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Amy Rechkemmer and Ming Yin",0.08790522875816993,0.0783156446969697,0.9120931493464052,0.0871209477272727,,
143,95,11,Section,"(15, 22)",6.4 On the Generalizability of Our Results,0.08790522875816993,0.1457031592171716,0.4384224549019607,0.1594772753787879,,
144,96,11,Paragraph,"[(22, 32), (32, 41), (41, 54), (54, 65), (65, 75), (75, 86), (86, 96), (96, 109), (109, 119), (119, 130), (130, 141), (141, 151), (151, 158), (158, 169), (169, 178), (178, 188), (188, 197), (197, 208), (208, 221), (221, 229)]","Despite our findings, proper caution should be used when general- izing our results to different experimental settings, task domains, and subject populations. In terms of settings, to keep the size of the experiment manageable, we included only two levels for each of our influencing factors. The levels we chose for each influencing factor are relatively extreme, with the hope of identifying clear effects by dichotomizing the levels. It is thus unclear what our experimental results would be if an influencing factor takes a value that is sub- stantially different from our chosen levels (e.g., after people observe an intermediate level of model accuracy, say 80%, whether and how the model confidence will affect people’s trust in the ML model?). Our task interface may have further constrained our results. Had we communicated multiple performance-related information of the ML model to people in a different way (e.g., communicate model confidence to people verbally, using terms like “virtually certain to be correct”, “likely correct”, etc., or using visualizations), we may have obtained different results. Limiting or expanding the amount of information that we showed to subjects for each speed dating task profile itself may have also had an impact on how much attention was given to the model’s performance information.",0.08736437908496732,0.16490753939393943,0.4829487552836603,0.43913279191919197,,
145,96,11,Paragraph,"[(229, 237), (237, 248), (248, 259), (259, 271), (271, 280), (280, 291), (291, 302), (302, 313), (313, 324), (324, 337), (337, 345), (345, 356), (356, 368), (368, 377), (377, 391), (391, 401), (401, 410), (410, 421), (421, 431), (431, 444), (444, 461), (461, 472), (472, 485), (485, 494), (494, 504), (504, 513), (513, 515)]","Additionally, we recognize the potential for “confirmation bias” from our subjects in determining whether to trust the ML model, given that they were always asked to make an independent predic- tion first before seeing the ML model’s prediction. In this way, sub- jects whose independent predictions often agreed with the model may have speculated that the model’s accuracy was higher than it was in practice, leading to over-trust in its predictions, and vice- versa. Indeed, prior research has shown that trust in peer assessment is lowered when prior expectations are not met [30], and within the context of trust in ML and its predictions, in the absence of information about an ML model’s performance, people’s reliance on the model is dependent on how often the model’s predictions agree with their own [39]. We note that the possible existence of this confirmation bias mainly influences the absolute magnitude of the four trust measures we used in this study, but is unlikely to be a confounding variable for our main results regarding whether and how model accuracy and model confidence together affect trust. This is because all our analyses were conducted only within the set of treatments where the corresponding ML model shared the same binary predictions on tasks (i.e., within T 1– T 4 and within T 5– T 8 separately for Phase 1 analysis, and within T 1– T 8 for Phase 2 analysis), so the average degree that subjects suffered from their biases should be similar across treatments in such a set due to the random assignment. However, we acknowledge that had our ex- perimental tasks been designed in a different way (e.g., showing the model’s prediction before eliciting the subject’s), our findings might change.",0.08673366013071895,0.441648701010101,0.4829470030849673,0.8127312767676769,,
146,96,11,Paragraph,"[(515, 526), (526, 537), (537, 548), (548, 559), (559, 569), (569, 579)]","Finally, while the type of task (i.e., predicting speed dating out- comes) and the kind of human subjects (i.e., MTurk workers) we chose suit the purpose of our study—that is, to understand how laypeople’s trust in an ML model is affected by multiple perfor- mance indicators of the model in their low-stakes decision making— we caution readers to generalize our results to other populations",0.08736437908496732,0.8152471858585858,0.4829417270169935,0.8957527414141414,,
147,96,11,Paragraph,"[(579, 591), (591, 603), (603, 612), (612, 622), (622, 632), (632, 646), (646, 653), (653, 663), (663, 674), (674, 685), (685, 696), (696, 705), (705, 717), (717, 727), (727, 737), (737, 739)]","or other tasks. For example, it is unclear whether our results will still hold when users of the ML model have better knowledge on uncertainty quantification (e.g., data scientists) or on the domain itself (e.g., experts of human behavior). Our goal of understanding how laypeople trust ML models in low-stakes decision making im- plies that the decision of how much to trust the ML model does not have particularly impactful or long-term consequences. Whether similar results on the effects of various performance indicators can be obtained for tasks with higher stakes, such as making predictions of prognosis or recidivism, is unknown. We also note that though the task of predicting speed dating outcomes is easy enough for laypeople to understand and make meaningful predictions, it turns out that our subjects are not good at making correct predictions on this task by themselves. Had the prediction task been significantly easier or more difficult for people, different conclusions may have been drawn.",0.5195343137254902,0.10956031717171716,0.9145705733647058,0.3284358222222223,,
148,97,11,Section,"(739, 742)",6.5 Future Work,0.5195343137254902,0.3578698258838384,0.6633847009803922,0.37164394204545464,,
149,98,11,Paragraph,"[(742, 752), (752, 763), (763, 772), (772, 782), (782, 793), (793, 803), (803, 811), (811, 823), (823, 832), (832, 842), (842, 856), (856, 864), (864, 869)]","With the exploratory evidence obtained in this study, we would like to conduct more confirmatory studies in the future to validate these results and examine their generalizability under different set- tings, for different populations, and on different tasks. There are also many other types of performance indicators of an ML model beyond accuracy and confidence, such as precision, recall, and F-1 score. Exploring how different combinations of performance indi- cators of a model affect people’s trust in the model differently is another future direction. Ultimately, we seek to gain understanding as to why various indicators of model performance affect people’s trust in the way that they do, so as to build a theoretical frame- work on humans’ perception, processing, and comprehension of performance information of machine learning.",0.5188316993464052,0.3770754686868687,0.9145659436549018,0.5544408727272727,,
150,99,11,Section,"(869, 871)",7 CONCLUSIONS,0.5195343137254902,0.5700377551767677,0.6719408578431373,0.5838118713383839,,
151,100,11,Paragraph,"[(871, 880), (880, 890), (890, 900), (900, 910), (910, 922), (922, 932), (932, 943), (943, 951), (951, 963), (963, 973), (973, 985), (985, 999), (999, 1009), (1009, 1019), (1019, 1029), (1029, 1040), (1040, 1046)]","As machine learning becomes more ubiquitous in everyday life, understanding how laypeople trust the predictions of ML models be- comes increasingly important. In this work, we explore the impact of multiple performance indicators of an ML model on laypeople’s trust in the model. Our results suggest that, in general, the confi- dence an ML model ties to its predictions significantly influences how much laypeople claim to believe in the model’s individual pre- dictions, but performance measurements like the model’s accuracy on a held-out set of data and observed accuracy in practice have greater influence on how often laypeople will actually follow the model as well as their self-reported overall trust in the model. We aim for this work to be a step towards further study into the process that people’s perception and confidence in the predictions of ma- chine learning models are shaped by diverse information about the model that they receive and how this process eventually impacts trust. We hope exploratory evidence we report in this work will inspire more discussions in this direction.",0.5190212418300654,0.589243397979798,0.9145778402509805,0.8219560242424242,,
152,101,11,Section,"(1046, 1047)",ACKNOWLEDGMENTS,0.5195343137254902,0.8375529066919192,0.7116200352941177,0.8513270228535355,,
153,102,11,Paragraph,"[(1047, 1059), (1059, 1070), (1070, 1080)]",We thank Siddharth R. Shah for his participation in the early design of this study. We are grateful to the anonymous reviewers who provided many helpful comments. We thank the support of the,0.5188316993464052,0.8567585494949496,0.9120982497254903,0.8957527414141414,,
154,103,12,Header,"[(0, 10)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA",0.6636911764705882,0.0783156446969697,0.9120929186274509,0.0871209477272727,,
155,104,12,Paragraph,"[(10, 19), (19, 26), (26, 33)]","National Science Foundation under grant IIS-1850335 on this work. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors alone.",0.0873921568627451,0.10956031717171716,0.48271929162875826,0.14855577171717174,,
156,105,12,Section,"(33, 34)",REFERENCES,0.08790522875816993,0.1645718460858586,0.20132778316993463,0.17834596224747484,,
157,106,12,Bibliography,"[(34, 36), (36, 51), (51, 72), (72, 92), (92, 112), (112, 133), (133, 156), (156, 184), (184, 200), (200, 223), (223, 245), (245, 267), (267, 278), (278, 290), (290, 303), (303, 326), (326, 350), (350, 369), (369, 388), (388, 405), (405, 425), (425, 432), (432, 446), (446, 469), (469, 488), (488, 508), (508, 526), (526, 544), (544, 567), (567, 584), (584, 604), (604, 622), (622, 632), (632, 638), (638, 663), (663, 682), (682, 706), (706, 726), (726, 743), (743, 759), (759, 778), (778, 801), (801, 820), (820, 839), (839, 863), (863, 869), (869, 893), (893, 913), (913, 933), (933, 957), (957, 968), (968, 990), (990, 1015), (1015, 1032), (1032, 1039), (1039, 1054), (1054, 1078), (1078, 1103), (1103, 1125), (1125, 1139), (1139, 1144), (1144, 1158), (1158, 1174), (1174, 1182), (1182, 1207), (1207, 1227), (1227, 1241), (1241, 1262), (1262, 1269), (1269, 1294), (1294, 1310), (1310, 1325)]","[1] SaleemaAmershi,DanWeld,MihaelaVorvoreanu,AdamFourney,BesmiraNushi,PennyCollisson,JinaSuh,ShamsiIqbal,PaulNBennett,KoriInkpen,etal.2019.Guidelinesforhuman-AIinteraction.In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM, 3. [2] Stavros Antifakos, Nicky Kern, Bernt Schiele, and Adrian Schwaninger. 2005. Towards improving trust in context-aware systems by displaying system con- fidence. In Proceedings of the 7th international conference on Human computer interaction with mobile devices & services . ACM, 9–14. [3] Stavros Antifakos, Adrian Schwaninger, and Bernt Schiele. 2004. Evaluating the effects of displaying uncertainty in context-aware applications. In International Conference on Ubiquitous Computing . Springer, 54–69. [4] Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S. Lasecki, Daniel S. Weld, and Eric Horvitz. 2019. Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance. In Proceedings of the Seventh AAAI Conference on Human Computation and Crowdsourcing . [5] Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S Weld, Walter S Lasecki, and Eric Horvitz. 2019. Updates in human-ai teams: Understanding and addressing the performance/compatibility tradeoff. In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 2429–2437. [6] James Bennett, Stan Lanning, et al. 2007. The netflix prize. In Proceedings of KDD cup and workshop , Vol. 2007. Citeseer, 35. [7] Diego De Siqueira Braga, Marco Niemann, Bernd Hellingrath, and Fernando Buarque De Lima Neto. 2018. Survey on Computational Trust and Reputation Models. ACM Computing Surveys (CSUR) 51, 5 (2018), 101. [8] David V Budescu and Adrian K Rantilla. 2000. Confidence in aggregation of expert opinions. Acta psychologica 104, 3 (2000), 371–398. [9] AylinCaliskan,JoannaJBryson,andArvindNarayanan.2017. Semanticsderived automatically from language corpora contain human-like biases. Science 356, 6334 (2017), 183–186. [10] Hao-Fei Cheng, Ruotong Wang, Zheng Zhang, Fiona O’Connell, Terrance Gray, FMaxwellHarper,andHaiyiZhu.2019. ExplainingDecision-MakingAlgorithms through UI: Strategies to Help Non-Expert Stakeholders. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM, 559. [11] Chun-Wei Chiang and Ming Yin. 2021. You’d Better Stop! Understanding Human Reliance on Machine Learning Models under Covariate Shift. In 13th ACM Web Science Conference 2021 . 120–129. [12] Jin-Hee Cho, Kevin Chan, and Sibel Adali. 2015. A survey on trust modeling. ACM Computing Surveys (CSUR) 48, 2 (2015), 28. [13] Geoff Cumming. 2013. Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis . Routledge. [14] Geoff Cumming. 2014. The new statistics: Why and how. Psychological science 25, 1 (2014), 7–29. [15] David Dearman, Alex Varshavsky, Eyal De Lara, and Khai N Truong. 2007. An explorationoflocationerrorestimation.In InternationalConferenceonUbiquitous Computing . Springer, 181–198. [16] AbhijitDhariaandHojjatAdeli.2003. Neuralnetworkmodelforrapidforecasting of freeway link travel time. Engineering Applications of Artificial Intelligence 16, 7-8 (2003), 607–613. [17] Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2015. Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 1 (2015), 114. [18] Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2016. Overcoming algorithmaversion:Peoplewilluseimperfectalgorithmsiftheycan(evenslightly)modifythem. Management Science 64, 3 (2016), 1155–1170. [19] Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter- pretable machine learning. arXiv preprint arXiv:1702.08608 (2017). [20] Pierre Dragicevic. 2016. Fair statistical communication in HCI. In Modern statistical methods for HCI . Springer, 291–330. [21] Michael Fernandes, Logan Walls, Sean Munson, Jessica Hullman, and Matthew Kay. 2018. Uncertainty displays using quantile dotplots or cdfs improve transit decision-making. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . ACM, 144. [22] Raymond Fisman, Sheena S Iyengar, Emir Kamenica, and Itamar Simonson. 2006. Gender differences in mate selection: Evidence from a speed dating experiment. The Quarterly Journal of Economics 121, 2 (2006), 673–697. [23] YuanGao,SanmayDas,andPatrickFowler.2017. Homelessnessserviceprovision: a data science perspective. In Workshops at the Thirty-First AAAI Conference on Artificial Intelligence . [24] Inga Großmann, André Hottung, and Artus Krohn-Grimberghe. 2019. Machine learningmeetspartnermatching:Predictingthefuturerelationshipqualitybasedonpersonalitytraits. PloS one 14, 3 (2019). [25] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 . JMLR. org, 1321–1330. [26] Kotaro Hara, Abigail Adams, Kristy Milland, Saiph Savage, Chris Callison-Burch, and Jeffrey P Bigham. 2018. A data-driven analysis of workers’ earnings on Amazon Mechanical Turk. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems . 1–14. [27] Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon. 2016. Combining satellite imagery and machine learning to predict poverty. Science 353, 6301 (2016), 790–794. [28] Samantha Joel, Paul W Eastwick, and Eli J Finkel. 2017. Is romantic desire predictable?Machinelearningappliedtoinitialromanticattraction. Psychological science 28, 10 (2017), 1478–1489. [29] Matthew Kay, Tara Kola, Jessica R Hullman, and Sean A Munson. 2016. When (ish) is my bus?: User-centered visualizations of uncertainty in everyday, mobile predictive systems. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . ACM, 5092–5103. [30] René F Kizilcec. 2016. How much information? Effects of transparency on trust in an algorithmic interface. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 2390–2395. [31] Rafal Kocielnik, Saleema Amershi, and Paul N Bennett. 2019. Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End-user Expectations of AI Systems.In Proceedingsofthe2019CHIConferenceonHumanFactorsinComputing Systems . ACM, 411. [32] Amanda Kube, Sanmay Das, and Patrick J Fowler. 2019. Allocating interven- tions based on predicted outcomes: A case study on homelessness services. In Proceedings of the AAAI Conference on Artificial Intelligence . [33] Volodymyr Kuleshov and Percy S Liang. 2015. Calibrated structured prediction. In Advances in Neural Information Processing Systems . 3474–3482. [34] Johannes Kunkel, Tim Donkers, Lisa Michael, Catalin-Mihai Barbu, and Jürgen Ziegler. 2019. Let Me Explain: Impact of Personal and Impersonal Explanations on Trust in Recommender Systems. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM, 487. [35] VivianLaiandChenhaoTan.2019. OnHumanPredictionswithExplanationsand Predictions of Machine Learning Models: A Case Study on Deception Detection. In Proceedings of the Conference on Fairness, Accountability, and Transparency . ACM, 29–38. [36] Brian Y Lim and Anind K Dey. 2011. Investigating intelligibility for uncertain context-aware applications. In Proceedings of the 13th international conference on Ubiquitous computing . ACM, 415–424. [37] Zachary C Lipton. 2018. The mythos of model interpretability. Commun. ACM 61, 10 (2018), 36–43. [38] JenniferMLogg,JuliaAMinson,andDonAMoore.2019. Algorithmappreciation: People prefer algorithmic to human judgment. Organizational Behavior and Human Decision Processes 151 (2019), 90–103. [39] Zhuoran Lu and Ming Yin. 2021. Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1–16. [40] Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and Yunpeng Wang. 2017. Learning traffic as images: a deep convolutional neural network for large-scale transportation network speed prediction. Sensors 17, 4 (2017), 818. [41] Polina Marinova. 2017. How Dating Site eHarmony Uses Machine Learning to HelpYouFindLove. https://fortune.com/2017/02/14/eharmony-dating-machine- learning/ [42] AnhNguyen,JasonYosinski,andJeffClune.2015.Deepneuralnetworksareeasilyfooled:Highconfidencepredictionsforunrecognizableimages.In Proceedings of the IEEE conference on computer vision and pattern recognition . 427–436. [43] AlexandruNiculescu-MizilandRichCaruana.2005. Predictinggoodprobabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning . ACM, 625–632. [44] EllenPeters,DanielVästfjäll,PaulSlovic,CKMertz,KettiMazzocco,andStephanDickert.2006.Numeracyanddecisionmaking. Psychological science 17, 5 (2006), 407–413. [45] Robert Pinsker. 2011. Primacy or recency? A study of order effects when nonpro- fessional investors are provided a long series of disclosures. Behavioral Research in Accounting 23, 1 (2011), 161–183. [46] John Platt et al. 1999. Probabilistic outputs for support vector machines and com- parisons to regularized likelihood methods. Advances in large margin classifiers 10, 3 (1999), 61–74. [47] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wort- man Wortman Vaughan, and Hanna Wallach. 2021. Manipulating and measuring modelinterpretability.In Proceedingsofthe2021CHIConferenceonHumanFactors in Computing Systems . 1–52. [48] Valerie F Reyna and Charles J Brainerd. 2008. Numeracy, ratio bias, and denom- inator neglect in judgments of risk and probability. Learning and individual differences 18, 1 (2008), 89–107. [49] James Schaffer, John O’Donovan, James Michaelis, Adrienne Raglin, and Tobias Höllerer. 2019. I can do better than your AI: expertise and explanations.. In IUI .",0.08790522875816989,0.11149001944444449,0.9140222086732026,0.8951753398989899,,
158,107,13,Header,"[(0, 10), (10, 15)]","CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA Amy Rechkemmer and Ming Yin",0.08790522875816993,0.0783156446969697,0.9120931493464052,0.0871209477272727,,
159,108,13,Bibliography,"[(15, 33), (33, 56), (56, 73), (73, 97), (97, 117), (117, 131), (131, 155), (155, 178), (178, 201), (201, 220), (220, 223), (223, 249), (249, 262), (262, 286), (286, 312), (312, 329), (329, 352), (352, 377), (377, 392), (392, 414), (414, 431)]","240–251. [50] Aaron Springer and Steve Whittaker. 2018. Progressive Disclosure: Designing for Effective Transparency. arXiv preprint arXiv:1811.02164 (2018). [51] Harini Suresh, Natalie Lao, and Ilaria Liccardi. 2020. Misplaced Trust: Measuring the Interference of Machine Learning in Human Decision-Making. In 12th ACM Conference on Web Science . 315–324. [52] Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gonza- lez Zelaya, and Aad Van Moorsel. 2020. The relationship between trust in AI and trustworthy machine learning technologies. In Proceedings of the 2020 conference on fairness, accountability, and transparency . 272–283. [53] Brandon M Turner, Mark Steyvers, Edgar C Merkle, David V Budescu, and Thomas S Wallsten. 2014. Forecast aggregation via recalibration. Machine learning 95, 3 (2014), 261–289. [54] Jhim Kiel M Verame, Enrico Costanza, and Sarvapali D Ramchurn. 2016. The effect of displaying system confidence information on the usage of autonomous systems for non-specialist applications: A lab study. In Proceedings of the 2016 chi conference on human factors in computing systems . ACM, 4908–4920. [55] Xinru Wang and Ming Yin. 2021. Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making. In 26th International Conference on Intelligent User Interfaces . 318–328. [56] Robert L Winkler, Yael Grushka-Cockayne, Kenneth C Lichtendahl, and Victor RichmondRJose.2018. AveragingProbabilityForecasts:BacktotheFuture .Harvard Business School. [57] Fumeng Yang, Zhuanyi Huang, Jean Scholtz, and Dustin L Arendt. 2020. How do visual explanations foster end users’ appropriate trust in machine learning?. In Proceedings of the 25th International Conference on Intelligent User Interfaces . 189–201. [58] Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. 2019. Understanding the Effect of Accuracy on Trust in Machine Learning Models. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . ACM, 279. [59] Kun Yu, Shlomo Berkovsky, Ronnie Taib, Dan Conway, Jianlong Zhou, and Fang Chen.2017. Usertrustdynamics:Aninvestigationdrivenbydifferencesinsystem performance. In Proceedings of the 22nd International Conference on Intelligent User Interfaces . ACM, 307–317. [60] Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into ac- curate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining . ACM, 694–699. [61] Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. 2020. Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted Decision Making. arXiv preprint arXiv:2001.02114 (2020). [62] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing . 2979–2989.",0.08790522875816989,0.11149001944444449,0.9140222086732026,0.3316324106060605,,
